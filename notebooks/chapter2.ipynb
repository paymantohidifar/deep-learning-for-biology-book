{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f848122c",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To run this notebook on Colab, a few setup steps are required. Follow along step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aac7bf",
   "metadata": {},
   "source": [
    "1. **Clone the `dlfb` library**  \n",
    "   First, clone the repository that contains the `dlfb` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!rm -rf ./dlfb-clone/\n",
    "!git clone \"https://github.com/deep-learning-for-biology/dlfb.git\" dlfb-clone --branch main\n",
    "%cd dlfb-clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3483b6e",
   "metadata": {},
   "source": [
    "2. **Install dependencies**  \n",
    "   Once the library is cloned, install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 1. Download and execute the 'uv' installer script.\n",
    "# -L: Follow redirects. -s: Silent mode. -S: Show errors. -f: Fail fast on server errors.\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n",
    "\n",
    "# 2. Add the 'uv' binary directory to your system PATH so you can call 'uv' directly.\n",
    "# This ensures the shell knows where the newly installed tool is located.\n",
    "export PATH=\"/root/.local/bin:${PATH}\" && \\\n",
    "\n",
    "# 3. Generate a resolved dependency list (compilation).\n",
    "# It combines multiple requirement files ({base, dlfb, proteins, gpu}.txt).\n",
    "# --color never: Disables ANSI color codes for clean logging.\n",
    "# --constraint: Forces versions to align with a specific 'constraints.txt' file.\n",
    "uv pip compile ./requirements/{base,dlfb,proteins,gpu}.txt \\\n",
    "  --color never \\\n",
    "  --constraint ./requirements/constraints.txt | \\\n",
    "\n",
    "# 4. Install the compiled dependencies.\n",
    "# -r -: Reads the requirements from 'stdin' (the output of the previous 'compile' pipe).\n",
    "# --system: Installs packages into the global system Python rather than a virtual env.\n",
    "uv pip install -r - --system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cace21",
   "metadata": {},
   "source": [
    "3. **Providion the datasets**  \n",
    "   You’ll then need to access and download the necessary datasets for this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: exclude models with '--no-models' flag\n",
    "!dlfb-provision --chapter proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af852f10",
   "metadata": {},
   "source": [
    "4. **Load the `dlfb` package**  \n",
    "   Finally, load the `dlfb` package.  \n",
    "   - ⚠️ Note: Loading can sometimes be finicky. If you encounter issues, simply **restart the runtime**. All previously downloaded data and installed packages will persist, so you can re-run the load step without repeating everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle JAX_DISABLE_JIT to True for easier debugging\n",
    "%env JAX_DISABLE_JIT=False\n",
    "\n",
    "try:\n",
    "  import dlfb\n",
    "except ImportError as exc:\n",
    "  # NOTE: Packages installed in editable mode are not immediately\n",
    "  #       recognized by Colab (https://stackoverflow.com/a/63312333).\n",
    "  import site\n",
    "  site.main()\n",
    "  import dlfb\n",
    "\n",
    "from dlfb.utils.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8d907",
   "metadata": {},
   "source": [
    "# Chapter 2. Learning the Language of Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11351d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb29426",
   "metadata": {},
   "source": [
    "## Biology Primer\n",
    "\n",
    "This section provides a biological foundation for protein science, emphasizing the deterministic relationship where Sequence $\\rightarrow$ Structure $\\rightarrow$ Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cfa2e",
   "metadata": {},
   "source": [
    "### Protein Structure\n",
    "\n",
    "Protein structure is organized into a four-level hierarchy, determined by the primary sequence of amino acids.\n",
    "\n",
    "* **Hierarchical Levels**:\n",
    "    * **Primary**: The linear chain of amino acids.\n",
    "    * **Secondary**: Localized folding patterns like -helices and -sheets.\n",
    "    * **Tertiary**: The complete 3D spatial arrangement of a single polypeptide chain.\n",
    "    * **Quaternary**: The complex assembly of multiple protein subunits (e.g., Hemoglobin).\n",
    "* **Amino Acid Properties**: There are 20 main amino acids, categorized by biochemical traits such as hydrophobicity, charge (positive/negative), and polarity.\n",
    "* **The Impact of Mutations**: Even a single amino acid substitution (point mutation) can be catastrophic.\n",
    "* *Example*: Sickle cell anemia occurs when a hydrophilic amino acid (E) is replaced by a hydrophobic one (V) in hemoglobin.\n",
    "\n",
    "<div style='display:flex; justify-content: center'>\n",
    "    <img src='images/aa.png' width='750px'>\n",
    "</div>\n",
    "\n",
    "### Protein Function\n",
    "\n",
    "Proteins are the primary \"workhorses\" of the cell. Their functions are systematically categorized using the **Gene Ontology (GO)** framework.\n",
    "* **Molecular Function**: The specific biochemical activity at the molecular level (e.g., DNA binding or enzyme catalysis).\n",
    "* **Biological Process**: The larger goal the protein contributes to (e.g., cell division or immune signaling).\n",
    "* **Cellular Component**: Where the protein is physically located (e.g., mitochondria or nucleus), which often hints at its role.\n",
    "* **Multi-functionality**: A single protein can have multiple annotations across these categories; for instance, a protein might be a kinase (molecular) that drives muscle contraction (process) in muscle fibers (component).\n",
    "\n",
    "### Predicting Protein Function\n",
    "\n",
    "Predicting function from sequence is a \"grand challenge\" because it requires a model to implicitly understand how sequences fold into 3D shapes.\n",
    "* **Biotechnology & Engineering**: Designing synthetic enzymes for industry or therapeutic proteins for medicine.\n",
    "* **Disease Analysis**: Identifying how specific mutations (variants) disrupt healthy functions to find therapeutic targets.\n",
    "* **Genome & Metagenomics**: Assigning functional hypotheses to the millions of \"unknown\" proteins discovered in new species or environmental samples (like gut bacteria).\n",
    "* **Computational Strategy**: While full folding prediction is complex, effective workflows often involve using **pretrained embeddings** to capture biological \"language\" and training lightweight classifiers on top of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcace7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e124c5",
   "metadata": {},
   "source": [
    "## Machine Learning Primer\n",
    "\n",
    "This section bridges the gap between biological sequences and modern AI, explaining how the \"language\" of proteins can be parsed using the same techniques that power modern LLMs.\n",
    "\n",
    "### Large Language Models\n",
    "\n",
    "LLMs are founded on a simple objective that leads to emergent, complex intelligence.\n",
    "\n",
    "* **Next-Token Prediction**: Models are trained to predict the next character or word (token) based on the preceding context. Variants include **Masked Language Models**, which predict hidden tokens.\n",
    "* **Emergent Capabilities**: By scaling the number of parameters and the volume of data, models \"unsupervisedly\" learn to summarize, translate, and reason.\n",
    "* **Biology as Language**: Because DNA and proteins are sequences from a discrete alphabet with a complex \"grammar,\" they are perfectly suited for LLM architectures.\n",
    "* **Biological LLMs**: Models like **ESM2** learn rich representations of biological information by being trained on massive corpora of protein sequences.\n",
    "    \n",
    "    >**ESM-2:** a state-of-the-art transformer-based protein language model developed by Meta AI (FAIR) that predicts protein structure and function directly from amino acid sequences. Utilizing a BERT-style architecture, it is trained on millions of protein sequences to understand evolutionary, structural, and functional patterns. It enables tasks like mutation effect prediction, protein engineering, and, via ESMFold, rapid 3D structure prediction.\n",
    "\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Embeddings are the bridge between raw biological strings and numerical computation.\n",
    "\n",
    "* **Numerical Vectors**: An embedding is a compact list of floating-point numbers that encodes the \"meaning\" of a protein.\n",
    "* **Latent/Semantic Space**: Similar entities (like related proteins) are positioned near each other in an abstract multi-dimensional space. This allows models to identify functional similarities that aren't obvious from the raw sequence alone.\n",
    "* **Cosine Similarity**: This is the standard metric used to compare how closely aligned two embedding vectors are. It helps rank the most similar known proteins to a new, uncharacterized query.\n",
    "    \n",
    "$$\\text{cosine similarity}(A,B) = \\frac{A.B}{\\lVert A \\lVert \\lVert B \\lVert}$$\n",
    "\n",
    "### Pretraining and Fine-tuning\n",
    "\n",
    "This two-stage process allows models to apply general knowledge to specific, high-stakes tasks.\n",
    "\n",
    "* **Pretraining**: The model gains \"broad knowledge\" by training on massive, diverse datasets (e.g., all known protein sequences).\n",
    "* **Fine-tuning**: A secondary step where the pretrained model is updated on a smaller, specialized dataset to perform a specific task (e.g., predicting toxicity).\n",
    "* **Frozen Feature Extraction**: An efficient alternative to fine-tuning where the large model remains \"frozen\" (unchanged). It acts as a feature extractor, providing embeddings that are then fed into a much smaller, custom-trained classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2cb76f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4378503",
   "metadata": {},
   "source": [
    "## Representations of Proteins and Protein LMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802440fa",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "import requests\n",
    "\n",
    "\n",
    "def fetch_protein_structure(pdb_id: str) -> str:\n",
    "  \"\"\"Grab a PDB protein structure from the RCSB Protein Data Bank.\"\"\"\n",
    "  url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "  response = requests.get(url)\n",
    "  return response.text\n",
    "\n",
    "\n",
    "# The Protein Data Bank (PDB) is the main database of protein structures.\n",
    "# Each structure has a unique 4-character PDB ID. Below are a few examples.\n",
    "protein_to_pdb = {\n",
    "  \"insulin\": \"3I40\",  # Human insulin – regulates glucose uptake.\n",
    "  \"collagen\": \"1BKV\",  # Human collagen – provides structural support.\n",
    "  \"proteasome\": \"1YAR\",  # Archaebacterial proteasome – degrades proteins.\n",
    "}\n",
    "\n",
    "protein = \"collagen\"  # @param [\"insulin\", \"collagen\", \"proteasome\"]\n",
    "pdb_structure = fetch_protein_structure(pdb_id=protein_to_pdb[protein])\n",
    "\n",
    "pdbview = py3Dmol.view(width=400, height=300)\n",
    "pdbview.addModel(pdb_structure, \"pdb\")\n",
    "pdbview.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "pdbview.zoomTo()\n",
    "pdbview.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33333e",
   "metadata": {},
   "source": [
    "### 2.3.1. Numerical Representation of a Protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precursor insulin protein sequence (processed into two protein chains).\n",
    "insulin_sequence = (\n",
    "  \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n",
    "  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n",
    ")\n",
    "print(f\"Length of the insulin protein precursor: {len(insulin_sequence)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50943897",
   "metadata": {},
   "source": [
    "### 2.3.2. One-Hot Encoding of a Protein Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64df222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfb.utils.display import print_short_dict\n",
    "\n",
    "# fmt: off\n",
    "amino_acids = [\n",
    "  \"R\", \"H\", \"K\", \"D\", \"E\", \"S\", \"T\", \"N\", \"Q\", \"G\", \"P\", \"C\", \"A\", \"V\", \"I\",\n",
    "  \"L\", \"M\", \"F\", \"Y\", \"W\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "amino_acid_to_index = {\n",
    "  amino_acid: index for index, amino_acid in enumerate(amino_acids)\n",
    "}\n",
    "\n",
    "print_short_dict(amino_acid_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methionine, alanine, leucine, tryptophan, methionine.\n",
    "tiny_protein = [\"M\", \"A\", \"L\", \"W\", \"M\"]\n",
    "\n",
    "tiny_protein_indices = [\n",
    "  amino_acid_to_index[amino_acid] for amino_acid in tiny_protein\n",
    "]\n",
    "\n",
    "tiny_protein_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "one_hot_encoded_sequence = jax.nn.one_hot(\n",
    "  x=tiny_protein_indices, num_classes=len(amino_acids)\n",
    ")\n",
    "\n",
    "print(one_hot_encoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01406aab",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "One-hot encoded representation of a toy protein sequence (`MALWM`), visualized with a heatmap. This binary matrix encodes the identity of each residue without implying any similarity between them",
      "name": "one_hot_matrix_visualized"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig = sns.heatmap(\n",
    "  one_hot_encoded_sequence, square=True, cbar=False, cmap=\"inferno\"\n",
    ")\n",
    "fig.set(xlabel=\"Amino Acid Index\", ylabel=\"Protein Sequence\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb673c",
   "metadata": {},
   "source": [
    "### 2.3.3. Learned Embeddings of Amino Acids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785b01c",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "\n",
    "# Model checkpoint name taken from this GitHub README:\n",
    "# https://github.com/facebookresearch/esm#available-models-and-datasets-\n",
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = EsmModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_index = tokenizer.get_vocab()\n",
    "print_short_dict(vocab_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tiny_protein = tokenizer(\"MALWM\")[\"input_ids\"]\n",
    "tokenized_tiny_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tiny_protein[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = model.get_input_embeddings().weight.detach().numpy()\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(token_embeddings)\n",
    "embeddings_tsne_df = pd.DataFrame(\n",
    "  embeddings_tsne, columns=[\"first_dim\", \"second_dim\"]\n",
    ")\n",
    "embeddings_tsne_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140cb43f",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "2D t-SNE projection of the learned token embeddings from the ESM2 model. Even without labels, clusters begin to emerge, hinting that the model has learned to organize tokens in a meaningful way.",
      "name": "tsne_no_chemical_properties"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "fig = sns.scatterplot(\n",
    "  data=embeddings_tsne_df, x=\"first_dim\", y=\"second_dim\", s=50\n",
    ")\n",
    "fig.set_xlabel(\"First Dimension\")\n",
    "fig.set_ylabel(\"Second Dimension\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9d5da",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Coloring the t-SNE projection by amino acid properties reveals clear clusters of amino acids with similar biochemical roles that tend to group together in embedding space, reflecting the model's ability to capture meaningful biological structure. Technical non-amino acid tokens also group together in this latent space",
      "name": "tsne_with_chemical_properties"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "\n",
    "embeddings_tsne_df[\"token\"] = list(vocab_to_index.keys())\n",
    "\n",
    "token_annotation = {\n",
    "  \"hydrophobic\": [\"A\", \"F\", \"I\", \"L\", \"M\", \"V\", \"W\", \"Y\"],\n",
    "  \"polar uncharged\": [\"N\", \"Q\", \"S\", \"T\"],\n",
    "  \"negatively charged\": [\"D\", \"E\"],\n",
    "  \"positively charged\": [\"H\", \"K\", \"R\"],\n",
    "  \"special amino acid\": [\"B\", \"C\", \"G\", \"O\", \"P\", \"U\", \"X\", \"Z\"],\n",
    "  \"special token\": [\n",
    "    \"-\",\n",
    "    \".\",\n",
    "    \"<cls>\",\n",
    "    \"<eos>\",\n",
    "    \"<mask>\",\n",
    "    \"<null_1>\",\n",
    "    \"<pad>\",\n",
    "    \"<unk>\",\n",
    "  ],\n",
    "}\n",
    "\n",
    "embeddings_tsne_df[\"label\"] = embeddings_tsne_df[\"token\"].map(\n",
    "  {t: label for label, tokens in token_annotation.items() for t in tokens}\n",
    ")\n",
    "\n",
    "fig = sns.scatterplot(\n",
    "  data=embeddings_tsne_df,\n",
    "  x=\"first_dim\",\n",
    "  y=\"second_dim\",\n",
    "  hue=\"label\",\n",
    "  style=\"label\",\n",
    "  s=50,\n",
    ")\n",
    "fig.set_xlabel(\"First Dimension\")\n",
    "fig.set_ylabel(\"Second Dimension\")\n",
    "texts = [\n",
    "  fig.text(point[\"first_dim\"], point[\"second_dim\"], point[\"token\"])\n",
    "  for _, point in embeddings_tsne_df.iterrows()\n",
    "]\n",
    "adjust_text(\n",
    "  texts, expand=(1.5, 1.5), arrowprops=dict(arrowstyle=\"->\", color=\"grey\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83d250",
   "metadata": {},
   "source": [
    "### 2.3.4. The ESM2 Protein Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_sequence = (\n",
    "  \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n",
    "  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n",
    ")\n",
    "\n",
    "masked_insulin_sequence = (\n",
    "  # Let's mask the `L` amino acid in the 29th position (0-based indexing):\n",
    "  #       ...LALLALWGPDPAAAFVNQH  L   CGSHLVEALYLVCGERGFF...\n",
    "  \"MALWMRLLPLLALLALWGPDPAAAFVNQH<mask>CGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGG\"\n",
    "  \"GPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n",
    ")\n",
    "\n",
    "# Tokenize the masked insulin sequence.\n",
    "masked_inputs = tokenizer(masked_insulin_sequence)[\"input_ids\"]\n",
    "\n",
    "# Check that we indeed have a <mask> token in the place that we expect it. Note\n",
    "# that the tokenizer adds a <cls> token to the start of the sequence, so we in\n",
    "# fact expect the <mask> token at position 30 (not 29).\n",
    "assert masked_inputs[30] == vocab_to_index[\"<mask>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmForMaskedLM\n",
    "\n",
    "# Model checkpoint name taken from this GitHub README:\n",
    "# https://github.com/facebookresearch/esm#available-models-and-datasets-\n",
    "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "masked_lm_model = EsmForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dca718",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Model prediction for a masked leucine (`L`) in the insulin sequence. The model confidently predicts the correct amino acid (`L`) with high probability, showing that it has learned common sequence patterns in proteins",
      "name": "predict_missing_aa"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_outputs = masked_lm_model(\n",
    "  **tokenizer(text=masked_insulin_sequence, return_tensors=\"pt\")\n",
    ")\n",
    "model_preds = model_outputs.logits\n",
    "\n",
    "# Index into the predictions at the <mask> position.\n",
    "mask_preds = model_preds[0, 30].detach().numpy()\n",
    "\n",
    "# Apply softmax to convert the model's predicted logits to probabilities.\n",
    "mask_probs = jax.nn.softmax(mask_preds)\n",
    "\n",
    "# Visualize the predicted probability of each token.\n",
    "letters = list(vocab_to_index.keys())\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "plt.bar(letters, mask_probs, color=\"grey\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Model Probabilities for the Masked Amino Acid.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd73a7c",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.inspect import MaskPredictor\n",
    "\n",
    "display([MaskPredictor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e05c5",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Model prediction for a masked asparagine (`N`) in the insulin sequence. Here, the model is more uncertain, it assigns moderate probability to several possible amino acids, indicating that this position is harder to predict based on surrounding context",
      "name": "predict_missing_aa_less_clear"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "MaskPredictor(tokenizer, model=masked_lm_model).plot_predictions(\n",
    "  sequence=insulin_sequence, mask_index=26\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d70a3",
   "metadata": {},
   "source": [
    "### 2.3.5. Strategies for Extracting an Embedding for an Entire Protein\n",
    "### 2.3.6. Extracellular Versus Membrane Protein Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c35f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from dlfb.utils.context import assets\n",
    "\n",
    "protein_df = pd.read_csv(assets(\"proteins/datasets/sequence_df_cco.csv\"))\n",
    "protein_df = protein_df[~protein_df[\"term\"].isin([\"GO:0005575\", \"GO:0110165\"])]\n",
    "num_proteins = protein_df[\"EntryID\"].nunique()\n",
    "print(protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter protein dataframe to proteins with a single location.\n",
    "num_locations = protein_df.groupby(\"EntryID\")[\"term\"].nunique()\n",
    "proteins_one_location = num_locations[num_locations == 1].index\n",
    "protein_df = protein_df[protein_df[\"EntryID\"].isin(proteins_one_location)]\n",
    "\n",
    "go_function_examples = {\n",
    "  \"extracellular\": \"GO:0005576\",\n",
    "  \"membrane\": \"GO:0016020\",\n",
    "}\n",
    "\n",
    "sequences_by_function = {}\n",
    "\n",
    "min_length = 100\n",
    "max_length = 500  # Cap sequence length for speed and memory.\n",
    "num_samples = 20\n",
    "\n",
    "for function, go_term in go_function_examples.items():\n",
    "  proteins_with_function = protein_df[\n",
    "    (protein_df[\"term\"] == go_term)\n",
    "    & (protein_df[\"Length\"] >= min_length)\n",
    "    & (protein_df[\"Length\"] <= max_length)\n",
    "  ]\n",
    "  print(\n",
    "    f\"Found {len(proteins_with_function)} human proteins\\n\"\n",
    "    f\"with the molecular function '{function}' ({go_term}),\\n\"\n",
    "    f\"and {min_length}<=length<={max_length}.\\n\"\n",
    "    f\"Sampling {num_samples} proteins at random.\\n\"\n",
    "  )\n",
    "  sequences = list(\n",
    "    proteins_with_function.sample(num_samples, random_state=42)[\"Sequence\"]\n",
    "  )\n",
    "  sequences_by_function[function] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff680486",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.dataset import get_mean_embeddings\n",
    "\n",
    "display([get_mean_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe5f6c",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = EsmModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff24168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean protein embeddings for each location.\n",
    "protein_embeddings = {\n",
    "  loc: get_mean_embeddings(sequences_by_function[loc], tokenizer, model)\n",
    "  for loc in [\"extracellular\", \"membrane\"]\n",
    "}\n",
    "\n",
    "# Reformat data.\n",
    "labels, embeddings = [], []\n",
    "for location, embedding in protein_embeddings.items():\n",
    "  labels.extend([location] * embedding.shape[0])\n",
    "  embeddings.append(embedding)\n",
    "  print(f\"{location}: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4228fde",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Two-dimensional t-SNE projection of the 320-dimensional embeddings from a small ESM2 model. Even with this lightweight model, we observe a tendency for extracellular and membrane proteins to form separate clusters, suggesting that the embeddings contain information relevant to cellular localization",
      "name": "membrane_protein_embeddings"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embeddings_tsne = TSNE(n_components=2, random_state=42).fit_transform(\n",
    "  np.vstack(embeddings)\n",
    ")\n",
    "embeddings_tsne_df = pd.DataFrame(\n",
    "  {\n",
    "    \"first_dimension\": embeddings_tsne[:, 0],\n",
    "    \"second_dimension\": embeddings_tsne[:, 1],\n",
    "    \"location\": np.array(labels),\n",
    "  }\n",
    ")\n",
    "\n",
    "fig = sns.scatterplot(\n",
    "  data=embeddings_tsne_df,\n",
    "  x=\"first_dimension\",\n",
    "  y=\"second_dimension\",\n",
    "  hue=\"location\",\n",
    "  style=\"location\",\n",
    "  s=50,\n",
    "  alpha=0.7,\n",
    ")\n",
    "plt.title(\"t-SNE of Protein Embeddings\")\n",
    "fig.set_xlabel(\"First Dimension\")\n",
    "fig.set_ylabel(\"Second Dimension\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331fc69",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "\n",
    "from dlfb.utils.context import assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15779a",
   "metadata": {},
   "source": [
    "## 2.4. Preparing the Data\n",
    "### 2.4.1. Loading the CAFA3 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\n",
    "  assets(\"proteins/datasets/train_terms.tsv.zip\"), sep=\"\\t\", compression=\"infer\"\n",
    ")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a5a45",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.dataset import get_go_term_descriptions\n",
    "\n",
    "display([\"import obonet\", get_go_term_descriptions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_term_descriptions = get_go_term_descriptions(\n",
    "  store_path=assets(\"proteins/datasets/go_term_descriptions.csv\")\n",
    ")\n",
    "print(go_term_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.merge(go_term_descriptions, on=\"term\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ff095",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[labels[\"aspect\"] == \"MFO\"]\n",
    "print(labels[\"description\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences_file = assets(\"proteins/datasets/train_sequences.fasta\")\n",
    "fasta_sequences = SeqIO.parse(open(sequences_file), \"fasta\")\n",
    "\n",
    "data = []\n",
    "for fasta in fasta_sequences:\n",
    "  data.append(\n",
    "    {\n",
    "      \"EntryID\": fasta.id,\n",
    "      \"Sequence\": str(fasta.seq),\n",
    "      \"Length\": len(fasta.seq),\n",
    "    }\n",
    "  )\n",
    "sequence_df = pd.DataFrame(data)\n",
    "print(sequence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3999ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_file = assets(\"proteins/datasets/train_taxonomy.tsv.zip\")\n",
    "taxonomy = pd.read_csv(taxonomy_file, sep=\"\\t\", compression=\"infer\")\n",
    "print(taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = sequence_df.merge(taxonomy, on=\"EntryID\")\n",
    "sequence_df = sequence_df[sequence_df[\"taxonomyID\"] == 9606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = sequence_df.merge(labels, on=\"EntryID\")\n",
    "print(\n",
    "  f'Dataset contains {sequence_df[\"EntryID\"].nunique()} human proteins '\n",
    "  f'with {sequence_df[\"term\"].nunique()} molecular functions.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f2a50",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Distribution of the number of molecular functions annotated per protein. The y-axis is shown on a logarithmic scale to make rare cases more visible. While most proteins have fewer than 20 annotated functions, a small number are associated with over 50 distinct molecular roles.",
      "name": "functions_per_protein"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "sequence_df.groupby(\"EntryID\")[\"term\"].nunique().plot.hist(\n",
    "  bins=100, figsize=(5, 3), color=\"grey\", log=True\n",
    ")\n",
    "plt.xlabel(\"Number of Molecular Function Annotations per Protein\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Distribution of Function Counts per Protein\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ca015",
   "metadata": {},
   "outputs": [],
   "source": [
    "uninteresting_functions = [\n",
    "  \"GO:0003674\",  # \"molecular function\". Applies to 100% of proteins.\n",
    "  \"GO:0005488\",  # \"binding\". Applies to 93% of proteins.\n",
    "  \"GO:0005515\",  # \"protein binding\". Applies to 89% of proteins.\n",
    "]\n",
    "\n",
    "sequence_df = sequence_df[~sequence_df[\"term\"].isin(uninteresting_functions)]\n",
    "sequence_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ca16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_functions = (\n",
    "  sequence_df[\"term\"]\n",
    "  .value_counts()[sequence_df[\"term\"].value_counts() >= 50]\n",
    "  .index\n",
    ")\n",
    "\n",
    "sequence_df = sequence_df[sequence_df[\"term\"].isin(common_functions)]\n",
    "sequence_df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = (\n",
    "  sequence_df[[\"EntryID\", \"Sequence\", \"Length\", \"term\"]]\n",
    "  .assign(value=1)\n",
    "  .pivot(\n",
    "    index=[\"EntryID\", \"Sequence\", \"Length\"], columns=\"term\", values=\"value\"\n",
    "  )\n",
    "  .fillna(0)\n",
    "  .astype(int)\n",
    "  .reset_index()\n",
    ")\n",
    "print(sequence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df[\"EntryID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df[\"Sequence\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39086149",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_df[sequence_df[\"EntryID\"].isin([\"P0DP23\", \"P0DP24\", \"P0DP25\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6895cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_df.shape)\n",
    "sequence_df = sequence_df[sequence_df[\"Length\"] <= 500]\n",
    "print(sequence_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ce241",
   "metadata": {},
   "source": [
    "### 2.4.2. Splitting the Dataset into Subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 60% of the proteins will go into the training set.\n",
    "train_sequence_ids, valid_test_sequence_ids = train_test_split(\n",
    "  list(set(sequence_df[\"EntryID\"])), test_size=0.40, random_state=42\n",
    ")\n",
    "\n",
    "# Split the remaining 40% evenly between validation and test sets.\n",
    "valid_sequence_ids, test_sequence_ids = train_test_split(\n",
    "  valid_test_sequence_ids, test_size=0.50, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_splits = {\n",
    "  \"train\": sequence_df[sequence_df[\"EntryID\"].isin(train_sequence_ids)],\n",
    "  \"valid\": sequence_df[sequence_df[\"EntryID\"].isin(valid_sequence_ids)],\n",
    "  \"test\": sequence_df[sequence_df[\"EntryID\"].isin(test_sequence_ids)],\n",
    "}\n",
    "\n",
    "for split, df in sequence_splits.items():\n",
    "  print(f\"{split} has {len(df)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527635f5",
   "metadata": {},
   "source": [
    "### 2.4.3. Converting Protein Sequences into Their Mean Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86887377",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.dataset import (\n",
    "  load_sequence_embeddings,\n",
    "  store_sequence_embeddings,\n",
    ")\n",
    "\n",
    "display([store_sequence_embeddings, load_sequence_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = EsmModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "for split, df in sequence_splits.items():\n",
    "  store_sequence_embeddings(\n",
    "    sequence_df=df,\n",
    "    store_prefix=assets(f\"proteins/datasets/protein_dataset_{split}\"),\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8860c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_sequence_embeddings(\n",
    "  assets(\"proteins/datasets/protein_dataset_train\"),\n",
    "  model_checkpoint=model_checkpoint,\n",
    ")\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740a2ee",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.dataset import convert_to_tfds\n",
    "\n",
    "display([\"import tensorflow as tf\", convert_to_tfds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5df577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = convert_to_tfds(train_df, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b344a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "batch = next(train_ds.batch(batch_size).as_numpy_iterator())\n",
    "batch[\"embedding\"].shape, batch[\"target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf220ae",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.dataset import build_dataset\n",
    "\n",
    "display([build_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88dae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_splits = build_dataset(\n",
    "  assets(\"proteins/datasets/protein_dataset\"), model_checkpoint=model_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ccc3d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dlfb.proteins.dataset import (\n",
    "  build_dataset,\n",
    "  get_go_term_descriptions,\n",
    "  load_sequence_embeddings,\n",
    ")\n",
    "from dlfb.utils.context import assets\n",
    "\n",
    "go_term_descriptions = get_go_term_descriptions(\n",
    "  store_path=assets(\"proteins/datasets/go_term_descriptions.csv\")\n",
    ")\n",
    "\n",
    "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "\n",
    "train_df = load_sequence_embeddings(\n",
    "  store_file_prefix=f\"{assets('proteins/datasets/protein_dataset')}_train\",\n",
    "  model_checkpoint=model_checkpoint,\n",
    ")\n",
    "\n",
    "dataset_splits = build_dataset(\n",
    "  assets(\"proteins/datasets/protein_dataset\"), model_checkpoint=model_checkpoint\n",
    ")\n",
    "batch = next(dataset_splits[\"train\"].as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6e168",
   "metadata": {},
   "source": [
    "## 2.5. Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec16cec",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.model import Model\n",
    "\n",
    "display(\n",
    "  [\"import flax.linen as nn\\nfrom flax.training import train_state\\n\", Model]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb716c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = list(train_df.columns[train_df.columns.str.contains(\"GO:\")])\n",
    "\n",
    "mlp = Model(num_targets=len(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3469315",
   "metadata": {},
   "source": [
    "### 2.5.1. Defining the Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacbc87",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.train import train_step\n",
    "\n",
    "display([train_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278713d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.train import compute_metrics\n",
    "\n",
    "display([\"import sklearn\", compute_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a700c93",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.train import calculate_per_target_metrics, eval_step\n",
    "\n",
    "display([eval_step, calculate_per_target_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e7ce0",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from dlfb.proteins.train import train\n",
    "\n",
    "display([train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da48404",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "from dlfb.utils.restore import restorable\n",
    "\n",
    "# Initiate training state with dummy data from a single batch.\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, rng_init = jax.random.split(key=rng, num=2)\n",
    "\n",
    "state, metrics = restorable(train)(\n",
    "  state=mlp.create_train_state(\n",
    "    rng=rng_init, dummy_input=batch[\"embedding\"], tx=optax.adam(0.001)\n",
    "  ),\n",
    "  dataset_splits=dataset_splits,\n",
    "  batch_size=32,\n",
    "  num_steps=300,\n",
    "  eval_every=30,\n",
    "  store_path=assets(\"proteins/models/mlp\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca02b82",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Training and evaluation of the MLP model over 300 steps. Left - Loss curves for the training and validation splits show rapid convergence, with stability reached after ~30 steps. Right - auPRC, precision, and recall improve gradually. Accuracy and auROC metrics are very high due to class imbalance and are not very informative for this problem",
      "name": "mlp_model_eval"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dlfb.utils.metric_plots import DEFAULT_SPLIT_COLORS\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "\n",
    "# Plot training loss curve.\n",
    "learning_data = pd.concat(\n",
    "  pd.DataFrame(metrics[split]).melt(\"step\").assign(split=split)\n",
    "  for split in [\"train\", \"valid\"]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "  ax=ax[0],\n",
    "  x=\"step\",\n",
    "  y=\"value\",\n",
    "  hue=\"split\",\n",
    "  data=learning_data[learning_data[\"variable\"] == \"loss\"],\n",
    "  palette=DEFAULT_SPLIT_COLORS,\n",
    ")\n",
    "ax[0].set_title(\"Loss over training steps.\")\n",
    "\n",
    "# Plot validation metrics curves.\n",
    "sns.lineplot(\n",
    "  ax=ax[1],\n",
    "  x=\"step\",\n",
    "  y=\"value\",\n",
    "  hue=\"variable\",\n",
    "  style=\"variable\",\n",
    "  data=learning_data[learning_data[\"variable\"] != \"loss\"],\n",
    "  palette=\"Set2\",\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax[1].set_title(\"Validation metrics over training steps.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40d33d",
   "metadata": {},
   "source": [
    "### 2.5.2. Examining the Model Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = load_sequence_embeddings(\n",
    "  store_file_prefix=f\"{assets('proteins/datasets/protein_dataset')}_valid\",\n",
    "  model_checkpoint=model_checkpoint,\n",
    ")\n",
    "\n",
    "# Use batch size of 1 to avoid dropping the remainder.\n",
    "valid_probs = []\n",
    "for valid_batch in dataset_splits[\"valid\"].batch(1).as_numpy_iterator():\n",
    "  logits = state.apply_fn({\"params\": state.params}, x=valid_batch[\"embedding\"])\n",
    "  valid_probs.extend(jax.nn.sigmoid(logits))\n",
    "\n",
    "valid_true_df = valid_df[[\"EntryID\"] + targets].set_index(\"EntryID\")\n",
    "valid_prob_df = pd.DataFrame(\n",
    "  np.stack(valid_probs), columns=targets, index=valid_true_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca7f9d",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Heatmap overview of protein function prediction. The left panel shows the ground truth functional annotations for each protein in the validation set, while the right panel shows the model's predicted probabilities. Both matrices are sparse, with vertical bands reflecting common function labels.",
      "name": "predicted_functional_annotation"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     }
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(11, 4))\n",
    "\n",
    "sns.heatmap(\n",
    "  ax=ax[0],\n",
    "  data=valid_true_df,\n",
    "  yticklabels=False,\n",
    "  xticklabels=False,\n",
    "  cmap=\"flare\",\n",
    ")\n",
    "ax[0].set_title(\"True functional annotations by protein.\")\n",
    "ax[0].set_xlabel(\"Functional category\")\n",
    "\n",
    "sns.heatmap(\n",
    "  ax=ax[1],\n",
    "  data=valid_prob_df,\n",
    "  yticklabels=False,\n",
    "  xticklabels=False,\n",
    "  cmap=\"flare\",\n",
    ")\n",
    "ax[1].set_title(\"Predicted functional annotations by protein.\")\n",
    "ax[1].set_xlabel(\"Functional category\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ed207",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_function = {}\n",
    "for function in targets:\n",
    "  metrics_by_function[function] = compute_metrics(\n",
    "    valid_true_df[function].values, valid_prob_df[function].values\n",
    "  )\n",
    "\n",
    "overview_valid = (\n",
    "  pd.DataFrame(metrics_by_function)\n",
    "  .T.merge(go_term_descriptions, left_index=True, right_on=\"term\")\n",
    "  .set_index(\"term\")\n",
    "  .sort_values(\"auprc\", ascending=False)\n",
    ")\n",
    "print(overview_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df11aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of occurences of each function in the training set.\n",
    "overview_valid = overview_valid.merge(\n",
    "  pd.DataFrame(train_df[targets].sum(), columns=[\"train_n\"]),\n",
    "  left_index=True,\n",
    "  right_index=True,\n",
    ")\n",
    "print(overview_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e6662",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Relationship between training frequency and predictive performance (auPRC) across protein functions. Commonly observed functions in the training set tend to be predicted more accurately by the model.",
      "name": "auprc_over_train_n"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "50%"
     }
    }
   },
   "outputs": [],
   "source": [
    "fig = sns.scatterplot(\n",
    "  x=\"train_n\", y=\"auprc\", data=overview_valid, alpha=0.5, s=50, color=\"grey\"\n",
    ")\n",
    "fig.set_xlabel(\"# Train instances\")\n",
    "fig.set_ylabel(\"Validation auPRC\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b93de",
   "metadata": {},
   "source": [
    "### 2.5.3. Evaluating Model Usefulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d201a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coin_flip_predictions(\n",
    "  valid_true_df: pd.DataFrame, targets: list[str]\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Make random coin flip predictions for each protein function.\"\"\"\n",
    "  predictions = np.random.choice([0.0, 1.0], size=valid_true_df.shape)\n",
    "  return pd.DataFrame(predictions, columns=targets, index=valid_true_df.index)\n",
    "\n",
    "\n",
    "def make_proportional_predictions(\n",
    "  valid_true_df: pd.DataFrame, train_df: pd.DataFrame, targets: list[str]\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Make random protein function predictions proportional to frequency.\"\"\"\n",
    "  percent_1_train = dict(train_df[targets].mean())\n",
    "  proportional_preds = []\n",
    "  for target_column in targets:\n",
    "    prob_1 = percent_1_train[target_column]\n",
    "    prob_0 = 1 - prob_1\n",
    "    proportional_preds.append(\n",
    "      np.random.choice([0.0, 1.0], size=len(valid_true_df), p=[prob_0, prob_1])\n",
    "    )\n",
    "  return pd.DataFrame(\n",
    "    np.stack(proportional_preds).T, columns=targets, index=valid_true_df.index\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_methods = {\n",
    "  \"coin_flip_baseline\": make_coin_flip_predictions(valid_true_df, targets),\n",
    "  \"proportional_guess_baseline\": make_proportional_predictions(\n",
    "    valid_true_df, train_df, targets\n",
    "  ),\n",
    "  \"model\": valid_prob_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e80051",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_method = {}\n",
    "for method, preds_df in prediction_methods.items():\n",
    "  metrics_by_method[method] = pd.DataFrame(\n",
    "    [\n",
    "      compute_metrics(valid_true_df.iloc[i], preds_df.iloc[i])\n",
    "      for i in range(len(valid_true_df))\n",
    "    ]\n",
    "  ).mean()\n",
    "\n",
    "print(pd.DataFrame(metrics_by_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae16ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "auprc_by_function = {}\n",
    "\n",
    "for method, preds_df in prediction_methods.items():\n",
    "  metrics_by_function = {}\n",
    "\n",
    "  for function in targets:\n",
    "    metrics_by_function[function] = compute_metrics(\n",
    "      valid_true_df[function], preds_df[function]\n",
    "    )\n",
    "\n",
    "  auprc_by_function[method] = (\n",
    "    pd.DataFrame(metrics_by_function)\n",
    "    .T.merge(go_term_descriptions, left_index=True, right_on=\"term\")\n",
    "    .set_index(\"term\")\n",
    "    .sort_values(\"auprc\", ascending=False)\n",
    "  )[\"auprc\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbbad3",
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Top 20 protein functions ranked by model auPRC on the validation set. Bars show the auPRC achieved by the model, compared against two simple baselines, i.e. coin flips and proportional guessing"
     },
     "image": {
      "classes": "shadow bg-primary",
      "width": "600px"
     },
     "name": "best_predicted_functions"
    }
   },
   "outputs": [],
   "source": [
    "best_performing = (\n",
    "  pd.DataFrame(auprc_by_function)\n",
    "  .merge(go_term_descriptions, left_index=True, right_on=\"term\")\n",
    "  .set_index(\"term\")\n",
    "  .sort_values(\"model\", ascending=False)\n",
    "  .head(20)\n",
    "  .melt(\"description\")\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "  x=\"description\",\n",
    "  y=\"value\",\n",
    "  hue=\"variable\",\n",
    "  data=best_performing,\n",
    ")\n",
    "ax.set_title(\"The model's 20 best performing protein functions\")\n",
    "ax.set_ylabel(\"Validation auPRC\")\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d40be0",
   "metadata": {},
   "source": [
    "### 2.5.4. Conducting a Final Check on the Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = []\n",
    "\n",
    "for split in [\"valid\", \"test\"]:\n",
    "  split_metrics = []\n",
    "\n",
    "  for eval_batch in dataset_splits[split].batch(32).as_numpy_iterator():\n",
    "    split_metrics.append(eval_step(state, eval_batch))\n",
    "\n",
    "  eval_metrics.append(\n",
    "    {\"split\": split, **pd.DataFrame(split_metrics).mean(axis=0).to_dict()}\n",
    "  )\n",
    "print(pd.DataFrame(eval_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a050c64",
   "metadata": {},
   "source": [
    "## 2.6. Improvements and Extensions\n",
    "### 2.6.1. Biological and Analytical Exploration\n",
    "### 2.6.2. Machine Learning Improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7a4be",
   "metadata": {},
   "source": [
    "## 2.7. Summary\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all,-execution",
   "formats": "ipynb,md:myst",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,jupytext,language_info",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
