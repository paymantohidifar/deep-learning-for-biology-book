{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95038adf",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction\n",
    "\n",
    "Learn about the promise and challenges of deep learning in biology. You will be walked through practical questions to consider before launching a new project—like what your model could replace, whether deep learning is even necessary, and how to structure your workflow. This chapter also includes a short technical introduction covering JAX/Flax, Python patterns common in machine learning, working environments, and practical setup tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b66ce8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da084f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab521299",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7f2b9",
   "metadata": {},
   "source": [
    "## Using Code Examples\n",
    "\n",
    "Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/deep-learning-for-biology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973c6d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65242e",
   "metadata": {},
   "source": [
    "## Deciding What Your Model Will Replace\n",
    "\n",
    "This section from the introductory chapter focuses on the strategic framing of biological deep learning projects, arguing that defining the \"real-world\" target of a model is more critical than the initial choice of architecture.\n",
    "\n",
    "### Key Summary Points\n",
    "\n",
    "* **Avoid the \"Tinker Trap\":** Deep learning in biology is intellectually stimulating, which often leads researchers to spend excessive time on technical minutiae. To remain focused, one must identify the existing process the model is intended to replace or improve.\n",
    "* **Domain-Specific Impact Areas:**\n",
    "    * **Healthcare & Drug Discovery:** Models aim to replace slow or manual tasks such as dermatological diagnosis, culture-based pathogen detection, manual MRI tumor segmentation, and exhaustive wet-lab screening for drug-target interactions.\n",
    "    * **Molecular Biology:** Computational tools like *AlphaFold* provide 3D protein structures that would otherwise require months of expensive X-ray crystallography or Cryo-EM. Other models act as digital alternatives to RNA-seq (gene expression) or manual variant interpretation.\n",
    "    * **Ecology:** AI replaces labor-intensive field work, such as in-person biodiversity surveys (via acoustics) or manual crop scouting (via satellite/drone imagery), and offers non-invasive alternatives to physical animal tagging.\n",
    "* **Quantifying Success:** Researchers should estimate the potential impact in terms of time, cost, or labor.\n",
    "* **Innovation vs. Replacement:** Not all models replace old workflows. Some enable entirely new capabilities, such as generating *de novo* biological sequences or linking disparate data types that were previously incompatible. In these cases, success must be evaluated without established benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f8077",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac18ac",
   "metadata": {},
   "source": [
    "## Determining Your Criteria for Success\n",
    "\n",
    "Define success metrics early to avoid endless, unfocused experimentation and wasted time in deep learning projects.\n",
    "\n",
    "**Five Types of Success Criteria:**\n",
    "\n",
    "1. **Performance Metrics**\n",
    "   - **Examples:** accuracy, AUC, F1 score\n",
    "   - Goals may include matching human expert performance, achieving experimental correlation, or maintaining low false-positive rates\n",
    "\n",
    "2. **Interpretability Requirements**\n",
    "   - Focus on explainability and transparency of model decisions\n",
    "   - Important for domain expert trust, calibrated uncertainty estimates, and understandable feature attributions\n",
    "\n",
    "3. **Model Size and Inference Efficiency**\n",
    "   - Critical for resource-constrained environments (smartphones, embedded devices)\n",
    "   - Metrics include inference time, memory usage, energy consumption, and performance per FLOP (floating point operation)\n",
    "   - May prioritize efficiency over raw accuracy for real-time applications\n",
    "\n",
    "4. **Training Efficiency**\n",
    "   - Relevant when compute resources are limited or in educational settings\n",
    "   - May focus on CPU-compatible models rather than GPU-dependent ones\n",
    "   - Prioritizes fast training and minimal hardware requirements\n",
    "\n",
    "5. **Generalizability**\n",
    "   - Aims for models that work across multiple datasets or tasks\n",
    "   - Relevant for foundational models designed for broad applicability\n",
    "   - Values flexibility and reusability over single-task optimization\n",
    "\n",
    "**Key Takeaway:**\n",
    "Establishing clear success criteria upfront helps determine when a project is complete and ensures efforts remain focused and realistic while balancing multiple objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b8dca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef17caa",
   "metadata": {},
   "source": [
    "\n",
    "## Invest Heavily in Evaluations\n",
    "\n",
    "Evaluation strategy should be a top priority from the start, not an afterthought. It guides the entire project and determines whether your work produces meaningful results.\n",
    "\n",
    "**What Strong Evaluation Involves:**\n",
    "- Defining precise measurement methods and metrics\n",
    "- Establishing validation procedures\n",
    "- Selecting appropriate baselines for comparison\n",
    "- Creating a well-designed evaluation strategy before building models\n",
    "\n",
    "**Benefits of Strong Evaluations:**\n",
    "- Measure progress accurately\n",
    "- Detect bugs in models or pipelines\n",
    "- Estimate task difficulty\n",
    "- Build intuition about the problem\n",
    "- Provide a known point of comparison to assess if the model is learning meaningfully\n",
    "\n",
    "**Recommended Time Allocation:**\n",
    "A rough guideline for successful machine learning projects:\n",
    "- **50%** - Designing evaluation strategies and running baselines\n",
    "- **25%** - Curating or processing data\n",
    "- **25%** - Model architecture development\n",
    "\n",
    "**Critical Warning:**\n",
    "Without good evaluations, you operate blindly — unable to determine if your model is improving, understand trade-offs, or verify meaningful learning is occurring.\n",
    "\n",
    "**Key Principle:**\n",
    "Evaluation is not an end-stage activity. It should be designed at the beginning and used to guide decisions throughout the entire project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4ff94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a5474",
   "metadata": {},
   "source": [
    "## Designing Baselines\n",
    "\n",
    "This section explains the importance of **baselines** as practical evaluation tools in machine learning — simple methods that establish minimum performance thresholds to compare against more complex models.\n",
    "\n",
    "### Purpose of Baselines\n",
    "- Measure progress and understand task difficulty\n",
    "- Catch bugs early in model development\n",
    "- Sometimes surprisingly competitive with complex models\n",
    "- Signal when something is wrong if models can't beat them\n",
    "\n",
    "### Classification Baselines\n",
    "\n",
    "1. **Random prediction**: Equal probability for all classes (zero information baseline)\n",
    "\n",
    "2. **Weighted random prediction**: Sample proportional to class frequencies in training data (useful for imbalanced datasets)\n",
    "\n",
    "3. **Majority class**: Always predict most common class (strong baseline for highly imbalanced problems)\n",
    "\n",
    "4. **Nearest neighbor**: Predict label of most similar training example (effective for low-dimensional or structured data)\n",
    "\n",
    "### Regression Baselines\n",
    "\n",
    "1. **Mean/median prediction**: Always predict training set average or median\n",
    "\n",
    "2. **Single-feature linear regression**: Fit line using strongest individual predictor (tests incremental value of complexity)\n",
    "\n",
    "3. **K-nearest neighbor regression**: Average target values of k most similar examples\n",
    "\n",
    "### Domain-Specific Heuristics\n",
    "\n",
    "- Apply simple rules based on domain knowledge\n",
    "- Examples:\n",
    "  - **Diagnostics:** threshold-based classification on biomarkers\n",
    "  - **Medical imaging:** rank by average pixel intensity\n",
    "  - **Genomics:** assign mutations to nearest gene\n",
    "\n",
    "### Key Takeaway\n",
    "If your model can't beat basic baselines, investigate your data, features, or modeling approach before adding complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed22bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ada72",
   "metadata": {},
   "source": [
    "## Time-Boxing Your Project\n",
    "\n",
    "Time-boxing is the practice of setting a fixed, non-negotiable timeframe for a project or specific task. In deep learning research—where projects can become open-ended and \"failed\" experiments are common—this strategy ensures that even unsuccessful ideas provide value without draining unlimited resources.\n",
    "\n",
    "### Strategies for Effective Time-Boxing\n",
    "\n",
    "* **Establish a Rigid Deadline:** Determine a realistic total duration for the project (e.g., two weeks or three months). The project should pause or stop once this limit is reached, regardless of whether the target metrics were achieved.\n",
    "* **Define Clear Checkpoints:** Break the timeline into intermediate milestones to monitor progress. Key checkpoints might include:\n",
    "    * Completion of data preprocessing.\n",
    "    * Training and evaluation of a baseline model.\n",
    "    * Reaching a specific performance threshold.\n",
    "* **Micro Time-Boxing:** Apply the same principle to specific sub-tasks or experimental ideas. For example, allocate exactly one week to test a new model architecture; if it does not show improvement within that window, abandon it and move on.\n",
    "* **Structured Reflection:** Use the end of the time-box to evaluate outcomes. Focus on what was learned and what technical insights can be applied to future work, transforming a \"failed\" project into a stepping stone.\n",
    "* **Mitigate Scope Creep:** Guard against the urge to justify extensions or \"one more tweak.\" When perfectionism or indecision stalls progress, consult with a mentor or collaborator to regain perspective and maintain focus on the broader goals.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Time-boxing is a tool for maintaining focus and avoiding burnout. It forces a decision-making point where you must evaluate the project's viability, ensuring that your energy is always directed toward the most promising research avenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8e2c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
