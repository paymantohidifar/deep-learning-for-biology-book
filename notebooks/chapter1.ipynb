{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95038adf",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction\n",
    "\n",
    "Learn about the promise and challenges of deep learning in biology. You will be walked through practical questions to consider before launching a new project—like what your model could replace, whether deep learning is even necessary, and how to structure your workflow. This chapter also includes a short technical introduction covering JAX/Flax, Python patterns common in machine learning, working environments, and practical setup tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b66ce8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da084f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab521299",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7f2b9",
   "metadata": {},
   "source": [
    "## Using Code Examples\n",
    "\n",
    "Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/deep-learning-for-biology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973c6d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070df83a",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Before jumping into code, we walk through how to frame a project, evaluate your data, and avoid common pitfalls. A bit of structure and planning up front will make your work more reproducible, more flexible, and ultimately more useful and impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29262c8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65242e",
   "metadata": {},
   "source": [
    "### Deciding What Your Model Will Replace\n",
    "\n",
    "This section from the introductory chapter focuses on the strategic framing of biological deep learning projects, arguing that defining the \"real-world\" target of a model is more critical than the initial choice of architecture.\n",
    "\n",
    "#### Key Summary Points\n",
    "\n",
    "* **Avoid the \"Tinker Trap\":** Deep learning in biology is intellectually stimulating, which often leads researchers to spend excessive time on technical minutiae. To remain focused, one must identify the existing process the model is intended to replace or improve.\n",
    "* **Domain-Specific Impact Areas:**\n",
    "    * **Healthcare & Drug Discovery:** Models aim to replace slow or manual tasks such as dermatological diagnosis, culture-based pathogen detection, manual MRI tumor segmentation, and exhaustive wet-lab screening for drug-target interactions.\n",
    "    * **Molecular Biology:** Computational tools like *AlphaFold* provide 3D protein structures that would otherwise require months of expensive X-ray crystallography or Cryo-EM. Other models act as digital alternatives to RNA-seq (gene expression) or manual variant interpretation.\n",
    "    * **Ecology:** AI replaces labor-intensive field work, such as in-person biodiversity surveys (via acoustics) or manual crop scouting (via satellite/drone imagery), and offers non-invasive alternatives to physical animal tagging.\n",
    "* **Quantifying Success:** Researchers should estimate the potential impact in terms of time, cost, or labor.\n",
    "* **Innovation vs. Replacement:** Not all models replace old workflows. Some enable entirely new capabilities, such as generating *de novo* biological sequences or linking disparate data types that were previously incompatible. In these cases, success must be evaluated without established benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f8077",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac18ac",
   "metadata": {},
   "source": [
    "### Determining Your Criteria for Success\n",
    "\n",
    "Define success metrics early to avoid endless, unfocused experimentation and wasted time in deep learning projects.\n",
    "\n",
    "**Five Types of Success Criteria:**\n",
    "\n",
    "1. **Performance Metrics**\n",
    "   - **Examples:** accuracy, AUC, F1 score\n",
    "   - Goals may include matching human expert performance, achieving experimental correlation, or maintaining low false-positive rates\n",
    "\n",
    "2. **Interpretability Requirements**\n",
    "   - Focus on explainability and transparency of model decisions\n",
    "   - Important for domain expert trust, calibrated uncertainty estimates, and understandable feature attributions\n",
    "\n",
    "3. **Model Size and Inference Efficiency**\n",
    "   - Critical for resource-constrained environments (smartphones, embedded devices)\n",
    "   - Metrics include inference time, memory usage, energy consumption, and performance per FLOP (floating point operation)\n",
    "   - May prioritize efficiency over raw accuracy for real-time applications\n",
    "\n",
    "4. **Training Efficiency**\n",
    "   - Relevant when compute resources are limited or in educational settings\n",
    "   - May focus on CPU-compatible models rather than GPU-dependent ones\n",
    "   - Prioritizes fast training and minimal hardware requirements\n",
    "\n",
    "5. **Generalizability**\n",
    "   - Aims for models that work across multiple datasets or tasks\n",
    "   - Relevant for foundational models designed for broad applicability\n",
    "   - Values flexibility and reusability over single-task optimization\n",
    "\n",
    "**Key Takeaway:**\n",
    "Establishing clear success criteria upfront helps determine when a project is complete and ensures efforts remain focused and realistic while balancing multiple objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b8dca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef17caa",
   "metadata": {},
   "source": [
    "\n",
    "### Invest Heavily in Evaluations\n",
    "\n",
    "Evaluation strategy should be a top priority from the start, not an afterthought. It guides the entire project and determines whether your work produces meaningful results.\n",
    "\n",
    "**What Strong Evaluation Involves:**\n",
    "- Defining precise measurement methods and metrics\n",
    "- Establishing validation procedures\n",
    "- Selecting appropriate baselines for comparison\n",
    "- Creating a well-designed evaluation strategy before building models\n",
    "\n",
    "**Benefits of Strong Evaluations:**\n",
    "- Measure progress accurately\n",
    "- Detect bugs in models or pipelines\n",
    "- Estimate task difficulty\n",
    "- Build intuition about the problem\n",
    "- Provide a known point of comparison to assess if the model is learning meaningfully\n",
    "\n",
    "**Recommended Time Allocation:**\n",
    "A rough guideline for successful machine learning projects:\n",
    "- **50%** - Designing evaluation strategies and running baselines\n",
    "- **25%** - Curating or processing data\n",
    "- **25%** - Model architecture development\n",
    "\n",
    "**Critical Warning:**\n",
    "Without good evaluations, you operate blindly — unable to determine if your model is improving, understand trade-offs, or verify meaningful learning is occurring.\n",
    "\n",
    "**Key Principle:**\n",
    "Evaluation is not an end-stage activity. It should be designed at the beginning and used to guide decisions throughout the entire project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4ff94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a5474",
   "metadata": {},
   "source": [
    "### Designing Baselines\n",
    "\n",
    "This section explains the importance of **baselines** as practical evaluation tools in machine learning — simple methods that establish minimum performance thresholds to compare against more complex models.\n",
    "\n",
    "#### Purpose of Baselines\n",
    "- Measure progress and understand task difficulty\n",
    "- Catch bugs early in model development\n",
    "- Sometimes surprisingly competitive with complex models\n",
    "- Signal when something is wrong if models can't beat them\n",
    "\n",
    "#### Classification Baselines\n",
    "\n",
    "1. **Random prediction**: Equal probability for all classes (zero information baseline)\n",
    "\n",
    "2. **Weighted random prediction**: Sample proportional to class frequencies in training data (useful for imbalanced datasets)\n",
    "\n",
    "3. **Majority class**: Always predict most common class (strong baseline for highly imbalanced problems)\n",
    "\n",
    "4. **Nearest neighbor**: Predict label of most similar training example (effective for low-dimensional or structured data)\n",
    "\n",
    "### Regression Baselines\n",
    "\n",
    "1. **Mean/median prediction**: Always predict training set average or median\n",
    "\n",
    "2. **Single-feature linear regression**: Fit line using strongest individual predictor (tests incremental value of complexity)\n",
    "\n",
    "3. **K-nearest neighbor regression**: Average target values of k most similar examples\n",
    "\n",
    "#### Domain-Specific Heuristics\n",
    "\n",
    "- Apply simple rules based on domain knowledge\n",
    "- Examples:\n",
    "  - **Diagnostics:** threshold-based classification on biomarkers\n",
    "  - **Medical imaging:** rank by average pixel intensity\n",
    "  - **Genomics:** assign mutations to nearest gene\n",
    "\n",
    "#### Key Takeaway\n",
    "If your model can't beat basic baselines, investigate your data, features, or modeling approach before adding complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed22bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ada72",
   "metadata": {},
   "source": [
    "### Time-Boxing Your Project\n",
    "\n",
    "Time-boxing is the practice of setting a fixed, non-negotiable timeframe for a project or specific task. In deep learning research—where projects can become open-ended and \"failed\" experiments are common—this strategy ensures that even unsuccessful ideas provide value without draining unlimited resources.\n",
    "\n",
    "#### Strategies for Effective Time-Boxing\n",
    "\n",
    "* **Establish a Rigid Deadline:** Determine a realistic total duration for the project (e.g., two weeks or three months). The project should pause or stop once this limit is reached, regardless of whether the target metrics were achieved.\n",
    "* **Define Clear Checkpoints:** Break the timeline into intermediate milestones to monitor progress. Key checkpoints might include:\n",
    "    * Completion of data preprocessing.\n",
    "    * Training and evaluation of a baseline model.\n",
    "    * Reaching a specific performance threshold.\n",
    "* **Micro Time-Boxing:** Apply the same principle to specific sub-tasks or experimental ideas. For example, allocate exactly one week to test a new model architecture; if it does not show improvement within that window, abandon it and move on.\n",
    "* **Structured Reflection:** Use the end of the time-box to evaluate outcomes. Focus on what was learned and what technical insights can be applied to future work, transforming a \"failed\" project into a stepping stone.\n",
    "* **Mitigate Scope Creep:** Guard against the urge to justify extensions or \"one more tweak.\" When perfectionism or indecision stalls progress, consult with a mentor or collaborator to regain perspective and maintain focus on the broader goals.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Time-boxing is a tool for maintaining focus and avoiding burnout. It forces a decision-making point where you must evaluate the project's viability, ensuring that your energy is always directed toward the most promising research avenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8e2c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eca31a",
   "metadata": {},
   "source": [
    "### Deciding Whether You Really Need Deep Learning\n",
    "\n",
    "While deep learning is a powerful tool in the biological sciences, it is not always the optimal solution. This section emphasizes the importance of evaluating whether a simpler, traditional approach can meet your project's goals more efficiently.\n",
    "\n",
    "#### Key Considerations for Choosing Your Approach\n",
    "\n",
    "* **Evaluate Simpler Alternatives:** Before committing to a deep learning architecture, consider if linear regression, decision trees, or basic statistical techniques are sufficient.\n",
    "* **Implementation and Setup:** Traditional methods are generally quicker to implement, easier to set up, and require less specialized expertise to maintain.\n",
    "* **Computational Efficiency:** Simpler models are far less resource-intensive. They can often run on standard hardware (CPUs) with minimal training time, whereas deep learning typically requires expensive GPU resources.\n",
    "* **Interpretability and Debugging:** Deep learning models are notoriously \"black boxes\" and difficult to troubleshoot. Simpler methods are often easier to explain to stakeholders, troubleshoot for errors, and validate against biological ground truth.\n",
    "* **Weighted Trade-offs:** The smarter path is often the one that delivers the required performance with the least amount of complexity. If a traditional method provides the necessary insights, the overhead of deep learning may not be justified.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The decision to use deep learning should be based on necessity rather than novelty. Prioritizing simplicity when possible leads to more robust, interpretable, and cost-effective biological research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7371f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890f6ce",
   "metadata": {},
   "source": [
    "### Ensuring That You Have Enough Good Data\n",
    "\n",
    "In the context of biological deep learning, where data acquisition can be expensive and prone to technical noise, the mantra of \"garbage in, garbage out\" is particularly relevant. This section highlights that the sophistication of your model cannot compensate for poor underlying data.\n",
    "\n",
    "#### Critical Data Requirements\n",
    "\n",
    "* **Sufficient Quantity:** Deep learning models generally require thousands of labeled examples to generalize effectively.\n",
    "    * **Benchmarking:** Consult existing literature to determine the standard dataset size for your specific biological task.\n",
    "    * **Transfer Learning:** If your dataset is small (e.g., a rare disease cohort), use transfer learning. Start with a model pre-trained on a massive, related dataset (like ImageNet for microscopy or UniProt for protein sequences) and fine-tune it on your specific data.\n",
    "* **Sufficient Quality:** The reliability of your model is capped by the cleanliness and consistency of your data.\n",
    "    * **Error Impact:** Inconsistent labeling or high levels of experimental noise can cause models to fail catastrophically.\n",
    "    * **Curation:** High-quality, curated data is often more valuable than a larger volume of \"noisy\" data. Prioritizing rigorous quality control (QC) and thoughtful curation is essential for building trustworthy models.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Success in deep learning is a balance between scale and precision. While you need enough data to capture biological variance, that data must be clean enough for the model to learn meaningful patterns rather than experimental artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cf8d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffcd26",
   "metadata": {},
   "source": [
    "### Assembling a Team\n",
    "\n",
    "Collaborating effectively is a catalyst for success in biological deep learning, where the complexity of the data often requires a blend of computational and experimental expertise.\n",
    "\n",
    "#### Strategies for Finding and Building a Team\n",
    "\n",
    "* **Engage with Digital Communities:** Use platforms like Reddit, Discord, X, and specialized Slack groups to share ideas and meet potential partners.\n",
    "* **Participate in Structured Challenges:** Join hackathons or competitions on platforms like Kaggle or Zindi to meet people with shared interests and receive immediate feedback.\n",
    "* **Prioritize Interdisciplinary Diversity:** Aim for a \"cross-pollination\" of skills. Biologists should seek out machine learning experts, and vice versa, to ensure the model is both mathematically sound and biologically relevant.\n",
    "* **Consult Domain Experts:** Reach out to authors of relevant papers or attendees at conferences. Genuine interest in a specific biological problem often leads to successful \"cold\" outreach and expert guidance.\n",
    "\n",
    "#### Best Practices for Effective Collaboration\n",
    "\n",
    "* **Establish Clear Governance:** Define specific roles, responsibilities, and decision-making processes early to prevent misunderstandings and scope creep.\n",
    "* **Utilize a Shared Tech Stack:** Implement collaborative tools such as:\n",
    "    * **Version Control:** Git for code management.\n",
    "    * **Shared Environments:** Google Colab for interactive modeling.\n",
    "    * **Task Tracking:** Notion, Trello, or simple shared documents to organize workflows.\n",
    "* **Encourage Specialization:** Allow team members to focus on their strengths, whether that is data engineering, infrastructure, modeling, or biological interpretation.\n",
    "* **Pilot the Partnership:** Start with a small, low-pressure \"sprint\" or exploration to test compatibility before committing to a long-term research project.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "While solo research is possible, interdisciplinary teams often produce more robust and innovative results. By combining deep domain knowledge with technical ML expertise and using structured communication tools, you can significantly accelerate the \"Get Started\" phase of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abac6fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74194836",
   "metadata": {},
   "source": [
    "### You Don't Need a Supercomputer or a PhD\n",
    "\n",
    "It is a common misconception that deep learning in biology is reserved for those with elite credentials or massive infrastructure. In reality, the field is increasingly accessible to anyone with curiosity and a laptop.\n",
    "\n",
    "#### Challenging Common Misconceptions\n",
    "\n",
    "* **The \"Huge Compute\" Myth:** You do not need a supercomputer to make a meaningful impact.\n",
    "    * **Iterative Prototyping:** Start with small, lightweight models to test ideas quickly before scaling up.\n",
    "    * **Accessible Hardware:** Utilize free GPU resources from platforms like **Google Colab** or **Kaggle**. For larger tasks, scalable cloud instances (AWS, GCP, Azure) allow you to pay only for what you use.\n",
    "    * **Analysis over Training:** Significant research involves analyzing or fine-tuning existing models rather than training them from scratch, which requires much less computational power.\n",
    "* **The \"Expert-Only\" Myth:** You do not need a PhD in both ML and Biology to contribute.\n",
    "    * **Modern Tooling:** High-level frameworks (like PyTorch or JAX) have lowered the barrier to entry for building complex architectures.\n",
    "    * **Open Source Ecosystem:** Leverage pre-trained models and open-source codebases to build upon the work of others.\n",
    "    * **Abundant Learning Resources:** Tutorials, walkthroughs, and videos offer accessible pathways to mastering the necessary concepts outside of traditional academia.\n",
    "    * **Uncharted Problems:** Many biological questions have yet to be approached with a machine learning lens, leaving plenty of room for newcomers to find niche areas of discovery.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The barrier to entry for biological deep learning is lower than it has ever been. By starting small, utilizing free resources, and leveraging the open-source community, you can contribute to the field regardless of your current budget or formal title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bddb85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a730f0",
   "metadata": {},
   "source": [
    "## Technical Introduction\n",
    "\n",
    "This section introduces the specific software ecosystem used in the book—**JAX** and **Flax**—and explains the rationale for choosing these tools for biological deep learning projects.\n",
    "\n",
    "### The JAX and Flax Ecosystem\n",
    "\n",
    "* **JAX:** A system for high-performance numerical computing that transforms Python and NumPy code into optimized machine code for accelerators (GPUs/TPUs).\n",
    "* **Flax:** A flexible neural network library designed specifically to run on top of JAX.\n",
    "* **`dlfb` (Deep Learning for Biology):** A custom companion library provided with the book to handle common utilities and repetitive tasks (https://github.com/deep-learning-for-biology/dlfb.git)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b8656",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b40721",
   "metadata": {},
   "source": [
    "### Why Use JAX and Flax for Biology?\n",
    "\n",
    "* **Familiarity:** JAX uses the `jax.numpy` ($jnp$) API, which is almost identical to standard NumPy, making the transition seamless for those already doing scientific computing in Python.\n",
    "* **Functional Clarity:** JAX follows a \"pure function\" style. This explicit approach reduces hidden states, making the underlying math of biological models easier to understand and debug.\n",
    "* **First-Class Transformations:** JAX offers powerful, composable tools:\n",
    "    * `jit`: Just-In-Time compilation via the XLA (Accelerated Linear Algebra) compiler for speed.\n",
    "    * `grad`: Automatic differentiation for calculating gradients.\n",
    "    * `vmap`: Automatic vectorization to handle batches of data (like thousands of protein sequences) without manual loops.\n",
    "* **Research Alignment:** JAX is the preferred tool for modern \"AI for Science\" research, including major breakthroughs like AlphaFold.\n",
    "\n",
    "#### Trade-offs and Considerations\n",
    "\n",
    "* **Learning Curve:** JAX requires a shift toward functional programming, which may feel different than the object-oriented approach of PyTorch.\n",
    "* **Ecosystem Size:** The JAX community is smaller than PyTorch's, and APIs (like the shift from Flax `linen` to the newer `nnx`) can evolve quickly.\n",
    "* **Framework Interoperability:** The book occasionally uses **PyTorch** (e.g., for Hugging Face model embeddings) because certain tools are more mature in that ecosystem.\n",
    "\n",
    "#### Advanced Performance Optimization\n",
    "\n",
    "While the book focuses on clarity, it identifies four key areas for scaling real-world biological models:\n",
    "\n",
    "* **Numerical Precision:** Using formats like $bfloat16$ to speed up matrix multiplications on specialized hardware (Tensor Cores).\n",
    "* **Profiling:** Using tools like `jax.profiler` to identify computational and memory bottlenecks.\n",
    "* **Memory Efficiency:** Using **gradient checkpointing** (`remat`) to train deeper models by trading computation for memory.\n",
    "* **Distributed Training:** Scaling models across multiple GPUs or TPUs for massive datasets.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Choosing JAX and Flax aligns your work with the \"bleeding edge\" of biological research while providing a transparent, mathematically grounded framework for learning.\n",
    "\n",
    "For those seeking a deeper technical dive or troubleshooting support, the text recommends two specific JAX resources:\n",
    "* **Official JAX Tutorials:** The primary source for detailed, hands-on learning and practical application of the framework (https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html).\n",
    "* **The \"Sharp Bits\" Notebook:** An essential reference guide that documents common pitfalls and non-intuitive behaviors unique to JAX's functional programming model (https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1011e74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8475e",
   "metadata": {},
   "source": [
    "### Python tips\n",
    "\n",
    "This section covers essential Python concepts frequently encountered in machine learning code, particularly with JAX and Flax frameworks.\n",
    "\n",
    "#### 1. Type Annotations and Docstrings\n",
    "\n",
    "Python is dynamically typed, which is flexible but can hide bugs. Type annotations improve readability, enable static type checking (mypy) or Vs Code's Pylance, and simplify debugging.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Basic function without type hints\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "# Improved function with type hints and docstring\n",
    "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between two NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground-truth values.\n",
    "        y_pred (np.ndarray): Predicted values.\n",
    "    \"\"\"\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Clarifies input/output types\n",
    "- Enhances IDE documentation and autocomplete\n",
    "- Improves code readability\n",
    "- Enables static type checking with tools like mypy\n",
    "\n",
    "#### 2. Decorators\n",
    "\n",
    "Decorators are functions that modify the behavior of other functions, commonly used for performance enhancement, caching, or logging.\n",
    "\n",
    "For example, JIT Compilation with JAX (`@jax.jit`) decorator:\n",
    "\n",
    "**Basic function:**\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n",
    "    return jnp.sum(arr**10)\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "compute_ten_power_sum(arr)\n",
    "# Output: Array(10874275, dtype=int32)\n",
    "```\n",
    "\n",
    "**Method 1 - Apply JIT directly:**\n",
    "```python\n",
    "jitted_compute_ten_power_sum = jax.jit(compute_ten_power_sum)\n",
    "jitted_compute_ten_power_sum(arr)\n",
    "# Output: Array(10874275, dtype=int32)\n",
    "```\n",
    "\n",
    "**Method 2 - Use decorator syntax:**\n",
    "```python\n",
    "@jax.jit\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n",
    "    return jnp.sum(arr**10)\n",
    "\n",
    "compute_ten_power_sum(arr)\n",
    "# Output: Array(10874275, dtype=int32)\n",
    "```\n",
    "\n",
    "**How `@jax.jit` works:**\n",
    "1. Traces the function using special tracer objects (not real data)\n",
    "2. Builds a computation graph (static representation of operations)\n",
    "3. Compiles via XLA (Accelerated Linear Algebra) to optimized machine code\n",
    "4. Caches compiled version for reuse with same input shapes/types\n",
    "5. Result: ~20× speedup on GPU\n",
    "\n",
    "**JIT Debugging Challenges:**\n",
    "- `print()` statements and `pdb` don't work as expected\n",
    "- Side effects are skipped during tracing\n",
    "- Cryptic error messages referencing internal JAX/XLA code\n",
    "\n",
    "**Solution**: Set environment variable `JAX_DISABLE_JIT=True` to globally disable JIT for debugging or you may set directly in your Python code: \n",
    "\n",
    "```python\n",
    "import jax\n",
    "jax.config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "def f(x):\n",
    "    y = jnp.log(x)\n",
    "    if jnp.isnan(y):\n",
    "        breakpoint()\n",
    "    return y\n",
    "\n",
    "jax.jit(f)(-2.)  # ==> Enters PDB breakpoint!\n",
    "\n",
    "```\n",
    "**Strengths and limitations of `jax_disable_jit`**\n",
    "* **Strengths:**\n",
    "    * Easy to apply\n",
    "    * Enables use of Python’s built-in `breakpoint` and `print`\n",
    "    * Throws standard Python exceptions and is compatible with PDB postmortem\n",
    "* **Limitations:**\n",
    "    * Running functions without JIT-compilation can be slow\n",
    "\n",
    "See the [JAX debugging documentation](https://docs.jax.dev/en/latest/debugging/flags.html#jax-disable-jit-configuration-option-and-context-manager) for more details:\n",
    "\n",
    "\n",
    "#### 3. Preconfiguring JAX JIT with `partial`\n",
    "\n",
    "`functools.partial` prefills/binds arguments to create new functions with fixed values, a general utility in Python.\n",
    "\n",
    "**Basic example:**\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "\n",
    "def scale(x, scaling_factor):\n",
    "    return x * scaling_factor\n",
    "\n",
    "# Create new function with scaling_factor fixed to 10\n",
    "scale_by_10 = partial(scale, scaling_factor=10)\n",
    "scale_by_10(3)\n",
    "# Output: 30\n",
    "\n",
    "```\n",
    "\n",
    "Here, `scale_by_10` is a new function that behaves like `scale(x, 10)`.\n",
    "\n",
    "**JAX-specific usage with static arguments:**\n",
    "\n",
    "In the context of JAX, `partial` is often used to customize a decorator before applying it, like this: `@partial(jax.jit, static_argnums=...)`. This is a way to configure the jax.jit decorator itself.\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def summarize(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported average type: {average_method}\")\n",
    "\n",
    "data_array = jnp.array([1.0, 2.0, 100.0])\n",
    "\n",
    "# JAX compiles one version for average_method=\"mean\"\n",
    "print(f\"Mean: {summarize('mean', data_array)}\")\n",
    "\n",
    "# JAX compiles another version for average_method=\"median\"\n",
    "print(f\"Median: {summarize('median', data_array)}\")\n",
    "\n",
    "# Calling with \"mean\" again uses cached compiled version\n",
    "print(f\"Mean again: {summarize('mean', data_array)}\")\n",
    "\n",
    "# Output:\n",
    "# Mean: 34.333335876464844\n",
    "# Median: 2.0\n",
    "# Mean again: 34.333335876464844\n",
    "```\n",
    "\n",
    "If we didn’t mark `average` as static with `static_argnums=(0,)`, JAX would throw an error, because it can’t trace control flow that depends on strings unless it knows their value ahead of time. Marking arguments as static tells JAX to compile a separate,\n",
    "specialized version of the function for each unique value of that static argument it encounters.\n",
    "\n",
    "**Static vs Dynamic arguments:**\n",
    "* **Dynamic**: Numerical inputs (`jax.Array`, `float`, `int`) - can vary without recompilation if shapes/types remain constant\n",
    "* **Static**: Strings, Python objects, functions - affect control flow; must mark with `static_argnums` or use closures\n",
    "\n",
    "\n",
    "#### 4. Closures\n",
    "\n",
    "**Definition**: Functions that \"remember\" their enclosing scope's variables.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "def outer_function(x):\n",
    "    def inner_function(y):\n",
    "        return x + y  # inner_function \"closes over\" x\n",
    "    return inner_function\n",
    "\n",
    "add_five = outer_function(5)  # x is 5\n",
    "result = add_five(10)  # y is 10\n",
    "print(f\"Closure result: {result}\")\n",
    "# Output: Closure result: 15\n",
    "```\n",
    "\n",
    "**Usage in JAX ML code:**\n",
    "- Extensively used for loss functions, regularizers, augmentation pipelines\n",
    "- Avoids passing configuration values as arguments (which might require `static_argnums`)\n",
    "- Values are \"closed over\" instead\n",
    "\n",
    "#### 5. Generators\n",
    "\n",
    "Iterates over data lazily (one item at a time) - essential for large datasets that don't fit in memory.\n",
    "\n",
    "**Simple generator:**\n",
    "```python\n",
    "from typing import Iterator\n",
    "\n",
    "def data_generator() -> Iterator[dict]:\n",
    "    \"\"\"Yield data samples with features and labels.\"\"\"\n",
    "    for i in range(5):\n",
    "        yield {\"feature\": i, \"label\": i % 2}\n",
    "\n",
    "# Example usage\n",
    "generator = data_generator()\n",
    "next(generator)\n",
    "# Output: {'feature': 0, 'label': 0}\n",
    "```\n",
    "\n",
    "**Integration with TensorFlow Datasets (TFDS):**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "features = np.array([1, 2, 3, 4, 5])\n",
    "labels = np.array([0, 1, 0, 1, 0])\n",
    "\n",
    "# Create TensorFlow dataset from NumPy arrays\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Batch with size 2, drop incomplete final batch\n",
    "batched_dataset = dataset.batch(2, drop_remainder=True)\n",
    "\n",
    "# Create iterator and retrieve first batch\n",
    "ds = iter(batched_dataset)\n",
    "next(ds)\n",
    "# Output:\n",
    "# (<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>,\n",
    "#  <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n",
    "```\n",
    "\n",
    "**Why TFDS with JAX?**\n",
    "- JAX lacks native data-loading library\n",
    "- TFDS provides clean API for batching, shuffling, and prefetching\n",
    "- Custom pipelines offer more control (covered in later chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb590ca",
   "metadata": {},
   "source": [
    "#### `jax.jit` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba14a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10874275\n",
      "CPU times: user 425 μs, sys: 16 μs, total: 441 μs\n",
      "Wall time: 267 μs\n",
      "10874275\n",
      "CPU times: user 24.8 ms, sys: 0 ns, total: 24.8 ms\n",
      "Wall time: 23.7 ms\n",
      "10874275\n",
      "CPU times: user 103 μs, sys: 0 ns, total: 103 μs\n",
      "Wall time: 106 μs\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Computes the sum of 10 raised to the power of each element in the input array.\"\"\"\n",
    "    return jnp.sum(arr ** 10)\n",
    "    \n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# No JIT compilation\n",
    "%time print(compute_ten_power_sum(arr))\n",
    "\n",
    "# Jitted version\n",
    "jitted_compute_ten_power_sum = jax.jit(compute_ten_power_sum)\n",
    "%time print(jitted_compute_ten_power_sum(arr)) # first call (compilation time) takes longer\n",
    "%time print(jitted_compute_ten_power_sum(arr)) # subsequent calls are faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e1d0775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10874275\n",
      "CPU times: user 24.6 ms, sys: 43 μs, total: 24.6 ms\n",
      "Wall time: 23.6 ms\n",
      "10874275\n",
      "CPU times: user 104 μs, sys: 0 ns, total: 104 μs\n",
      "Wall time: 106 μs\n",
      "10874275\n",
      "CPU times: user 66 μs, sys: 0 ns, total: 66 μs\n",
      "Wall time: 68.2 μs\n",
      "10874274\n",
      "CPU times: user 20.2 ms, sys: 989 μs, total: 21.2 ms\n",
      "Wall time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Computes the sum of 10 raised to the power of each element in the input array.\"\"\"\n",
    "    return jnp.sum(arr ** 10)\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "%time print(compute_ten_power_sum(arr)) # first call (compilation time) takes longer\n",
    "%time print(compute_ten_power_sum(arr)) # subsequent calls are faster\n",
    "\n",
    "arr = jnp.array([5, 4, 3, 2, 1])\n",
    "%time print(compute_ten_power_sum(arr)) # if array shape/dtype is the same, no recompilation\n",
    "\n",
    "arr = jnp.array([5, 4, 3, 2])\n",
    "%time print(compute_ten_power_sum(arr)) # different shape, triggers recompilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e4316",
   "metadata": {},
   "source": [
    "#### `jax.jit` with Python's `partial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae83a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "CPU times: user 24.3 ms, sys: 0 ns, total: 24.3 ms\n",
      "Wall time: 23.7 ms\n",
      "3.0\n",
      "CPU times: user 333 μs, sys: 0 ns, total: 333 μs\n",
      "Wall time: 288 μs\n",
      "3.0\n",
      "CPU times: user 57.4 ms, sys: 4.09 ms, total: 61.5 ms\n",
      "Wall time: 38.1 ms\n",
      "3.0\n",
      "CPU times: user 110 μs, sys: 0 ns, total: 110 μs\n",
      "Wall time: 113 μs\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# @partial(jax.jit, static_argnums=(0,)) # using deprecated static_argnums\n",
    "@partial(jax.jit, static_argnames=(\"average_method\",)) # using static_argnames\n",
    "def summarize(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown average method: {average_method}\")\n",
    "    \n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "%time print(summarize(\"mean\", arr))  # JIT compilation for \"mean\"\n",
    "%time print(summarize(\"mean\", arr))  # Subsequent call for \"mean\"\n",
    "%time print(summarize(\"median\", arr))  # JIT compilation for \"median\"\n",
    "%time print(summarize(\"median\", arr))  # Subsequent call for \"median\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a1659",
   "metadata": {},
   "source": [
    "#### Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacd38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "5.0\n",
      "<function outer_function.<locals>.inner_function at 0x7b250675b100>\n"
     ]
    }
   ],
   "source": [
    "def outer_function(x: float):\n",
    "    def inner_function(y: float):\n",
    "        return y + x\n",
    "    return inner_function\n",
    "\n",
    "\n",
    "add_five = outer_function(5.0)\n",
    "print(add_five(3.0))  # Outputs 8.0\n",
    "print(add_five.__closure__[0].cell_contents)  # Inspect closure to see captured variables\n",
    "print(add_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9644429",
   "metadata": {},
   "source": [
    "#### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ead94edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>)\n",
      "End of dataset reached.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "features = jnp.array([1, 2, 3, 4, 5])\n",
    "labels = jnp.array([0, 0, 1 , 1, 0])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "batched_dataset = dataset.batch(2, drop_remainder=True)\n",
    "\n",
    "ds = iter(batched_dataset)\n",
    "try:\n",
    "    print(next(ds))\n",
    "    print(next(ds))\n",
    "    print(next(ds))\n",
    "except StopIteration:\n",
    "    print(\"End of dataset reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945db33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee5c1d",
   "metadata": {},
   "source": [
    "### AnatomyAnatomy of a Training Loop with JAX/Flax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808415ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be569663",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Bonus learning materials\n",
    "\n",
    "### JIT compilation\n",
    "\n",
    "The term **\"Just-in-Time\" (JIT)** refers to the exact moment the compilation happens. In traditional programming languages (like C++ or Fortran), compilation happens before you ever run the program. In JAX, the compilation happens *while the program is running*, specifically the very first time a function is called.\n",
    "\n",
    "Here is a breakdown of why this distinction matters and how it works:\n",
    "\n",
    "#### 1. The Timing: A \"Late\" Compilation\n",
    "\n",
    "In a standard \"Ahead-of-Time\" (AOT) workflow, you compile your code into a binary file, and then you run that file. In JAX, you provide a Python function, and the \"Just-in-Time\" compiler stays idle until you actually trigger that function with real data.\n",
    "    * **Step 1:** You define the function.\n",
    "    * **Step 2:** You call the function with an input of a specific shape (e.g., a protein sequence of length $L = 500$).\n",
    "    * **Step 3 (The \"Just-in-Time\" part):** JAX realizes it doesn't have a compiled version for that specific input shape yet. It pauses, converts the Python code into an optimized XLA kernel, and then executes it.\n",
    "\n",
    "#### 2. Tracing: Learning by Example\n",
    "\n",
    "The reason JAX waits until the \"last second\" (Just-in-Time) is because it needs to see the **shapes** and **types** of your data to optimize effectively. This process is called **Tracing**.\n",
    "\n",
    "When you call a JIT-ed function, JAX sends \"abstract\" versions of your data through the function to see what happens. It records every operation (+, −, ×, ÷) to create a **StableHLO** (a high-level intermediate representation). By waiting until you provide data, JIT can:\n",
    "    * See that your matrix is $1000 \\times 1000$.\n",
    "    * Optimize the machine code specifically for those dimensions.\n",
    "\n",
    "#### 3. Specialization\n",
    "\n",
    "If you call the same function later with a *different* shape (e.g., a sequence of length $L=200$), JAX will compile it again, \"Just-in-Time\" for that new shape. It builds a library of specialized versions of your function in the background.\n",
    "\n",
    "#### Comparison Summary\n",
    "\n",
    "| Feature | Interpreted (Python/NumPy) | Ahead-of-Time (C++/Fortran) | Just-in-Time (JAX/XLA) |\n",
    "| --- | --- | --- | --- |\n",
    "| **When is it compiled?** | Never (translated line-by-line) | Before the program runs | During execution (on first call) |\n",
    "| **Performance** | Slow (High overhead) | Very Fast | Very Fast |\n",
    "| **Flexibility** | High | Low (must re-compile manually) | High (auto-specializes to shapes) |\n",
    "\n",
    "#### Why this is a \"Scientific\" Advantage\n",
    "\n",
    "In biological modeling, we often deal with variable-sized inputs (different DNA lengths, different number of atoms in a molecule). JIT allows us to write flexible Python code that feels \"easy,\" while the compiler works \"Just-in-Time\" to give us the speed of a low-level language like C++.\n",
    "\n",
    "\n",
    "### Closure\n",
    "\n",
    "In Python, a **closure** is a function object that \"remembers\" values in the enclosing scope even if they are no longer present in memory.\n",
    "\n",
    "For a closure to exist, three conditions must be met:\n",
    "\n",
    "1. There must be a **nested function** (a function inside a function).\n",
    "2. The nested function must refer to a value defined in the **enclosing function**.\n",
    "3. The enclosing function must **return** the nested function.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Basic Closure Example\n",
    "\n",
    "Think of a closure as a function with a \"backpack\" where it stores variables from its birthplace.\n",
    "\n",
    "```python\n",
    "def make_multiplier(x):\n",
    "    # This is the enclosing function\n",
    "    def multiplier(y):\n",
    "        # This nested function \"closes over\" the variable x\n",
    "        return x * y\n",
    "    \n",
    "    return multiplier\n",
    "\n",
    "# 'times_three' is now a closure that remembers x = 3\n",
    "times_three = make_multiplier(3)\n",
    "\n",
    "# 'times_five' is a separate closure that remembers x = 5\n",
    "times_five = make_multiplier(5)\n",
    "\n",
    "print(times_three(10))  # Output: 30\n",
    "print(times_five(10))   # Output: 50\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Accessing the \"Backpack\" (`__closure__`)\n",
    "\n",
    "Python stores these remembered variables in a special attribute called `__closure__`. Each item in this attribute is called a **cell**.\n",
    "\n",
    "```python\n",
    "# Continuing from the example above:\n",
    "print(times_three.__closure__[0].cell_contents) \n",
    "# Output: 3\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The `nonlocal` Keyword\n",
    "\n",
    "By default, a closure can read the outer variable but cannot modify it. If you want to change a variable in the enclosing scope, you must use the `nonlocal` keyword. This is common for creating \"counters\" or \"accumulators.\"\n",
    "\n",
    "```python\n",
    "def make_counter():\n",
    "    count = 0\n",
    "    \n",
    "    def increment():\n",
    "        nonlocal count  # Allows modification of the outer 'count'\n",
    "        count += 1\n",
    "        return count\n",
    "    \n",
    "    return increment\n",
    "\n",
    "counter_a = make_counter()\n",
    "print(counter_a())  # Output: 1\n",
    "print(counter_a())  # Output: 2\n",
    "\n",
    "counter_b = make_counter()\n",
    "print(counter_b())  # Output: 1 (Starts its own separate count)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why use Closures?\n",
    "\n",
    "In machine learning (and especially in **JAX** as mentioned in your earlier text), closures are powerful for:\n",
    "\n",
    "* **Data Hiding:** They provide a way to store state without using a full Class object.\n",
    "* **Function Factories:** You can generate specialized versions of a function (like a specific loss function with fixed hyperparameters).\n",
    "* **Decorators:** Closures are the underlying mechanism that makes Python decorators work.\n",
    "\n",
    "### Comparison to Classes\n",
    "\n",
    "If you only have one method in a class, a closure is often a more elegant, lightweight, and memory-efficient solution.\n",
    "\n",
    "| Feature | Closure | Class |\n",
    "| --- | --- | --- |\n",
    "| **Setup** | Lightweight (function) | Heavier (object + methods) |\n",
    "| **State** | Fixed via \"backpack\" | Mutable via `self` |\n",
    "| **Usage** | Functional programming | Object-Oriented programming |\n",
    "\n",
    "---\n",
    "\n",
    "**Would you like me to show you how a closure is used to build a Python decorator, or perhaps how to use a closure to \"bake\" hyperparameters into a JAX loss function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638a3e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
