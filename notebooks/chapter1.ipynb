{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95038adf",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction\n",
    "\n",
    "Learn about the promise and challenges of deep learning in biology. You will be walked through practical questions to consider before launching a new project—like what your model could replace, whether deep learning is even necessary, and how to structure your workflow. This chapter also includes a short technical introduction covering JAX/Flax, Python patterns common in machine learning, working environments, and practical setup tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b66ce8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da084f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab521299",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7f2b9",
   "metadata": {},
   "source": [
    "## Using Code Examples\n",
    "\n",
    "Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/deep-learning-for-biology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973c6d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070df83a",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Before jumping into code, we walk through how to frame a project, evaluate your data, and avoid common pitfalls. A bit of structure and planning up front will make your work more reproducible, more flexible, and ultimately more useful and impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29262c8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65242e",
   "metadata": {},
   "source": [
    "### Deciding What Your Model Will Replace\n",
    "\n",
    "This section from the introductory chapter focuses on the strategic framing of biological deep learning projects, arguing that defining the \"real-world\" target of a model is more critical than the initial choice of architecture. Here are key summary points:\n",
    "\n",
    "* **Avoid the \"Tinker Trap\":** Deep learning in biology is intellectually stimulating, which often leads researchers to spend excessive time on technical minutiae. To remain focused, one must identify the existing process the model is intended to replace or improve.\n",
    "* **Domain-Specific Impact Areas:**\n",
    "    * **Healthcare & Drug Discovery:** Models aim to replace slow or manual tasks such as dermatological diagnosis, culture-based pathogen detection, manual MRI tumor segmentation, and exhaustive wet-lab screening for drug-target interactions.\n",
    "    * **Molecular Biology:** Computational tools like *AlphaFold* provide 3D protein structures that would otherwise require months of expensive X-ray crystallography or Cryo-EM. Other models act as digital alternatives to RNA-seq (gene expression) or manual variant interpretation.\n",
    "    * **Ecology:** AI replaces labor-intensive field work, such as in-person biodiversity surveys (via acoustics) or manual crop scouting (via satellite/drone imagery), and offers non-invasive alternatives to physical animal tagging.\n",
    "* **Quantifying Success:** Researchers should estimate the potential impact in terms of time, cost, or labor.\n",
    "* **Innovation vs. Replacement:** Not all models replace old workflows. Some enable entirely new capabilities, such as generating *de novo* biological sequences or linking disparate data types that were previously incompatible. In these cases, success must be evaluated without established benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f8077",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac18ac",
   "metadata": {},
   "source": [
    "### Determining Your Criteria for Success\n",
    "\n",
    "Define success metrics early to avoid endless, unfocused experimentation and wasted time in deep learning projects.\n",
    "\n",
    "**Five Types of Success Criteria:**\n",
    "\n",
    "1. **Performance Metrics**\n",
    "   - **Examples:** accuracy, AUC, F1 score\n",
    "   - Goals may include matching human expert performance, achieving experimental correlation, or maintaining low false-positive rates\n",
    "\n",
    "2. **Interpretability Requirements**\n",
    "   - Focus on explainability and transparency of model decisions\n",
    "   - Important for domain expert trust, calibrated uncertainty estimates, and understandable feature attributions\n",
    "\n",
    "3. **Model Size and Inference Efficiency**\n",
    "   - Critical for resource-constrained environments (smartphones, embedded devices)\n",
    "   - Metrics include inference time, memory usage, energy consumption, and performance per FLOP (floating point operation)\n",
    "   - May prioritize efficiency over raw accuracy for real-time applications\n",
    "\n",
    "4. **Training Efficiency**\n",
    "   - Relevant when compute resources are limited or in educational settings\n",
    "   - May focus on CPU-compatible models rather than GPU-dependent ones\n",
    "   - Prioritizes fast training and minimal hardware requirements\n",
    "\n",
    "5. **Generalizability**\n",
    "   - Aims for models that work across multiple datasets or tasks\n",
    "   - Relevant for foundational models designed for broad applicability\n",
    "   - Values flexibility and reusability over single-task optimization\n",
    "\n",
    "**Key Takeaway:**\n",
    "Establishing clear success criteria upfront helps determine when a project is complete and ensures efforts remain focused and realistic while balancing multiple objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b8dca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef17caa",
   "metadata": {},
   "source": [
    "\n",
    "### Invest Heavily in Evaluations\n",
    "\n",
    "Evaluation strategy should be a top priority from the start, not an afterthought. It guides the entire project and determines whether your work produces meaningful results.\n",
    "\n",
    "**What Strong Evaluation Involves:**\n",
    "- Defining precise measurement methods and metrics\n",
    "- Establishing validation procedures\n",
    "- Selecting appropriate baselines for comparison\n",
    "- Creating a well-designed evaluation strategy before building models\n",
    "\n",
    "**Benefits of Strong Evaluations:**\n",
    "- Measure progress accurately\n",
    "- Detect bugs in models or pipelines\n",
    "- Estimate task difficulty\n",
    "- Build intuition about the problem\n",
    "- Provide a known point of comparison to assess if the model is learning meaningfully\n",
    "\n",
    "**Recommended Time Allocation:**\n",
    "A rough guideline for successful machine learning projects:\n",
    "- **50%** - Designing evaluation strategies and running baselines\n",
    "- **25%** - Curating or processing data\n",
    "- **25%** - Model architecture development\n",
    "\n",
    "Without good evaluations, you operate blindly — unable to determine if your model is improving, understand trade-offs, or verify meaningful learning is occurring.\n",
    "\n",
    "**Key Takeaway:** Evaluation is not an end-stage activity. It should be designed at the beginning and used to guide decisions throughout the entire project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4ff94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a5474",
   "metadata": {},
   "source": [
    "### Designing Baselines\n",
    "\n",
    "This section explains the importance of **baselines** as practical evaluation tools in machine learning — simple methods that establish minimum performance thresholds to compare against more complex models.\n",
    "\n",
    "#### Purpose of Baselines\n",
    "- Measure progress and understand task difficulty\n",
    "- Catch bugs early in model development\n",
    "- Sometimes surprisingly competitive with complex models\n",
    "- Signal when something is wrong if models can't beat them\n",
    "\n",
    "#### Classification Baselines\n",
    "\n",
    "1. **Random prediction**: Equal probability for all classes (zero information baseline)\n",
    "\n",
    "2. **Weighted random prediction**: Sample proportional to class frequencies in training data (useful for imbalanced datasets)\n",
    "\n",
    "3. **Majority class**: Always predict most common class (strong baseline for highly imbalanced problems)\n",
    "\n",
    "4. **Nearest neighbor**: Predict label of most similar training example (effective for low-dimensional or structured data)\n",
    "\n",
    "### Regression Baselines\n",
    "\n",
    "1. **Mean/median prediction**: Always predict training set average or median\n",
    "\n",
    "2. **Single-feature linear regression**: Fit line using strongest individual predictor (tests incremental value of complexity)\n",
    "\n",
    "3. **K-nearest neighbor regression**: Average target values of k most similar examples\n",
    "\n",
    "#### Domain-Specific Heuristics\n",
    "\n",
    "- Apply simple rules based on domain knowledge\n",
    "- Examples:\n",
    "  - **Diagnostics:** threshold-based classification on biomarkers\n",
    "  - **Medical imaging:** rank by average pixel intensity\n",
    "  - **Genomics:** assign mutations to nearest gene\n",
    "\n",
    "**Key Takeaway:** If your model can't beat basic baselines, investigate your data, features, or modeling approach before adding complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed22bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ada72",
   "metadata": {},
   "source": [
    "### Time-Boxing Your Project\n",
    "\n",
    "Time-boxing is the practice of setting a fixed, non-negotiable timeframe for a project or specific task. In deep learning research—where projects can become open-ended and \"failed\" experiments are common—this strategy ensures that even unsuccessful ideas provide value without draining unlimited resources.\n",
    "\n",
    "#### Strategies for Effective Time-Boxing\n",
    "\n",
    "* **Establish a Rigid Deadline:** Determine a realistic total duration for the project (e.g., two weeks or three months). The project should pause or stop once this limit is reached, regardless of whether the target metrics were achieved.\n",
    "* **Define Clear Checkpoints:** Break the timeline into intermediate milestones to monitor progress. Key checkpoints might include:\n",
    "    * Completion of data preprocessing.\n",
    "    * Training and evaluation of a baseline model.\n",
    "    * Reaching a specific performance threshold.\n",
    "* **Micro Time-Boxing:** Apply the same principle to specific sub-tasks or experimental ideas. For example, allocate exactly one week to test a new model architecture; if it does not show improvement within that window, abandon it and move on.\n",
    "* **Structured Reflection:** Use the end of the time-box to evaluate outcomes. Focus on what was learned and what technical insights can be applied to future work, transforming a \"failed\" project into a stepping stone.\n",
    "* **Mitigate Scope Creep:** Guard against the urge to justify extensions or \"one more tweak.\" When perfectionism or indecision stalls progress, consult with a mentor or collaborator to regain perspective and maintain focus on the broader goals.\n",
    "\n",
    "**Key Takeaway:** Time-boxing is a tool for maintaining focus and avoiding burnout. It forces a decision-making point where you must evaluate the project's viability, ensuring that your energy is always directed toward the most promising research avenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8e2c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eca31a",
   "metadata": {},
   "source": [
    "### Deciding Whether You Really Need Deep Learning\n",
    "\n",
    "While deep learning is a powerful tool in the biological sciences, it is not always the optimal solution. This section emphasizes the importance of evaluating whether a simpler, traditional approach can meet your project's goals more efficiently.\n",
    "\n",
    "#### Key Considerations for Choosing Your Approach\n",
    "\n",
    "* **Evaluate Simpler Alternatives:** Before committing to a deep learning architecture, consider if linear regression, decision trees, or basic statistical techniques are sufficient.\n",
    "* **Implementation and Setup:** Traditional methods are generally quicker to implement, easier to set up, and require less specialized expertise to maintain.\n",
    "* **Computational Efficiency:** Simpler models are far less resource-intensive. They can often run on standard hardware (CPUs) with minimal training time, whereas deep learning typically requires expensive GPU resources.\n",
    "* **Interpretability and Debugging:** Deep learning models are notoriously \"black boxes\" and difficult to troubleshoot. Simpler methods are often easier to explain to stakeholders, troubleshoot for errors, and validate against biological ground truth.\n",
    "* **Weighted Trade-offs:** The smarter path is often the one that delivers the required performance with the least amount of complexity. If a traditional method provides the necessary insights, the overhead of deep learning may not be justified.\n",
    "\n",
    "**Key Takaway:** The decision to use deep learning should be based on necessity rather than novelty. Prioritizing simplicity when possible leads to more robust, interpretable, and cost-effective biological research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7371f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890f6ce",
   "metadata": {},
   "source": [
    "### Ensuring That You Have Enough Good Data\n",
    "\n",
    "In the context of biological deep learning, where data acquisition can be expensive and prone to technical noise, the mantra of \"garbage in, garbage out\" is particularly relevant. This section highlights that the sophistication of your model cannot compensate for poor underlying data.\n",
    "\n",
    "#### Critical Data Requirements\n",
    "\n",
    "* **Sufficient Quantity:** Deep learning models generally require thousands of labeled examples to generalize effectively.\n",
    "    * **Benchmarking:** Consult existing literature to determine the standard dataset size for your specific biological task.\n",
    "    * **Transfer Learning:** If your dataset is small (e.g., a rare disease cohort), use transfer learning. Start with a model pre-trained on a massive, related dataset (like ImageNet for microscopy or UniProt for protein sequences) and fine-tune it on your specific data.\n",
    "* **Sufficient Quality:** The reliability of your model is capped by the cleanliness and consistency of your data.\n",
    "    * **Error Impact:** Inconsistent labeling or high levels of experimental noise can cause models to fail catastrophically.\n",
    "    * **Curation:** High-quality, curated data is often more valuable than a larger volume of \"noisy\" data. Prioritizing rigorous quality control (QC) and thoughtful curation is essential for building trustworthy models.\n",
    "\n",
    "**Key Takeaway:** Success in deep learning is a balance between scale and precision. While you need enough data to capture biological variance, that data must be clean enough for the model to learn meaningful patterns rather than experimental artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cf8d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffcd26",
   "metadata": {},
   "source": [
    "### Assembling a Team\n",
    "\n",
    "Collaborating effectively is a catalyst for success in biological deep learning, where the complexity of the data often requires a blend of computational and experimental expertise.\n",
    "\n",
    "#### Strategies for Finding and Building a Team\n",
    "\n",
    "* **Engage with Digital Communities:** Use platforms like Reddit, Discord, X, and specialized Slack groups to share ideas and meet potential partners.\n",
    "* **Participate in Structured Challenges:** Join hackathons or competitions on platforms like Kaggle or Zindi to meet people with shared interests and receive immediate feedback.\n",
    "* **Prioritize Interdisciplinary Diversity:** Aim for a \"cross-pollination\" of skills. Biologists should seek out machine learning experts, and vice versa, to ensure the model is both mathematically sound and biologically relevant.\n",
    "* **Consult Domain Experts:** Reach out to authors of relevant papers or attendees at conferences. Genuine interest in a specific biological problem often leads to successful \"cold\" outreach and expert guidance.\n",
    "\n",
    "#### Best Practices for Effective Collaboration\n",
    "\n",
    "* **Establish Clear Governance:** Define specific roles, responsibilities, and decision-making processes early to prevent misunderstandings and scope creep.\n",
    "* **Utilize a Shared Tech Stack:** Implement collaborative tools such as:\n",
    "    * **Version Control:** Git for code management.\n",
    "    * **Shared Environments:** Google Colab for interactive modeling.\n",
    "    * **Task Tracking:** Notion, Trello, or simple shared documents to organize workflows.\n",
    "* **Encourage Specialization:** Allow team members to focus on their strengths, whether that is data engineering, infrastructure, modeling, or biological interpretation.\n",
    "* **Pilot the Partnership:** Start with a small, low-pressure \"sprint\" or exploration to test compatibility before committing to a long-term research project.\n",
    "\n",
    "**Key Takeaway:** While solo research is possible, interdisciplinary teams often produce more robust and innovative results. By combining deep domain knowledge with technical ML expertise and using structured communication tools, you can significantly accelerate the \"Get Started\" phase of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abac6fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74194836",
   "metadata": {},
   "source": [
    "### You Don't Need a Supercomputer or a PhD\n",
    "\n",
    "It is a common misconception that deep learning in biology is reserved for those with elite credentials or massive infrastructure. In reality, the field is increasingly accessible to anyone with curiosity and a laptop.\n",
    "\n",
    "#### Challenging Common Misconceptions\n",
    "\n",
    "* **The \"Huge Compute\" Myth:** You do not need a supercomputer to make a meaningful impact.\n",
    "    * **Iterative Prototyping:** Start with small, lightweight models to test ideas quickly before scaling up.\n",
    "    * **Accessible Hardware:** Utilize free GPU resources from platforms like **Google Colab** or **Kaggle**. For larger tasks, scalable cloud instances (AWS, GCP, Azure) allow you to pay only for what you use.\n",
    "    * **Analysis over Training:** Significant research involves analyzing or fine-tuning existing models rather than training them from scratch, which requires much less computational power.\n",
    "* **The \"Expert-Only\" Myth:** You do not need a PhD in both ML and Biology to contribute.\n",
    "    * **Modern Tooling:** High-level frameworks (like PyTorch or JAX) have lowered the barrier to entry for building complex architectures.\n",
    "    * **Open Source Ecosystem:** Leverage pre-trained models and open-source codebases to build upon the work of others.\n",
    "    * **Abundant Learning Resources:** Tutorials, walkthroughs, and videos offer accessible pathways to mastering the necessary concepts outside of traditional academia.\n",
    "    * **Uncharted Problems:** Many biological questions have yet to be approached with a machine learning lens, leaving plenty of room for newcomers to find niche areas of discovery.\n",
    "\n",
    "**Key Takeaway:** The barrier to entry for biological deep learning is lower than it has ever been. By starting small, utilizing free resources, and leveraging the open-source community, you can contribute to the field regardless of your current budget or formal title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bddb85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a730f0",
   "metadata": {},
   "source": [
    "## Technical Introduction\n",
    "\n",
    "This section introduces the specific software ecosystem used in the book—**JAX** and **Flax**—and explains the rationale for choosing these tools for biological deep learning projects.\n",
    "\n",
    "### The JAX and Flax Ecosystem\n",
    "\n",
    "* **JAX:** A system for high-performance numerical computing that transforms Python and NumPy code into optimized machine code for accelerators (GPUs/TPUs).\n",
    "* **Flax:** A flexible neural network library designed specifically to run on top of JAX.\n",
    "* **`dlfb` (Deep Learning for Biology):** A custom companion library provided with the book to handle common utilities and repetitive tasks (https://github.com/deep-learning-for-biology/dlfb.git)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73732ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b40721",
   "metadata": {},
   "source": [
    "### Why Use JAX and Flax for Biology?\n",
    "\n",
    "* **Familiarity:** JAX uses the `jax.numpy` ($jnp$) API, which is almost identical to standard NumPy, making the transition seamless for those already doing scientific computing in Python.\n",
    "* **Functional Clarity:** JAX follows a \"pure function\" style. This explicit approach reduces hidden states, making the underlying math of biological models easier to understand and debug.\n",
    "* **First-Class Transformations:** JAX offers powerful, composable tools:\n",
    "    * `jit`: Just-In-Time compilation via the XLA (Accelerated Linear Algebra) compiler for speed.\n",
    "    * `grad`: Automatic differentiation for calculating gradients.\n",
    "    * `vmap`: Automatic vectorization to handle batches of data (like thousands of protein sequences) without manual loops.\n",
    "* **Research Alignment:** JAX is the preferred tool for modern \"AI for Science\" research, including major breakthroughs like AlphaFold.\n",
    "\n",
    "#### Trade-offs and Considerations\n",
    "\n",
    "* **Learning Curve:** JAX requires a shift toward functional programming, which may feel different than the object-oriented approach of PyTorch.\n",
    "* **Ecosystem Size:** The JAX community is smaller than PyTorch's, and APIs (like the shift from Flax `linen` to the newer `nnx`) can evolve quickly.\n",
    "* **Framework Interoperability:** The book occasionally uses **PyTorch** (e.g., for Hugging Face model embeddings) because certain tools are more mature in that ecosystem.\n",
    "\n",
    "#### Advanced Performance Optimization\n",
    "\n",
    "While the book focuses on clarity, it identifies four key areas for scaling real-world biological models:\n",
    "\n",
    "* **Numerical Precision:** Using formats like $bfloat16$ to speed up matrix multiplications on specialized hardware (Tensor Cores).\n",
    "* **Profiling:** Using tools like `jax.profiler` to identify computational and memory bottlenecks.\n",
    "* **Memory Efficiency:** Using **gradient checkpointing** (`remat`) to train deeper models by trading computation for memory.\n",
    "* **Distributed Training:** Scaling models across multiple GPUs or TPUs for massive datasets.\n",
    "\n",
    "**Key Takeaway:** Choosing JAX and Flax aligns your work with the \"bleeding edge\" of biological research while providing a transparent, mathematically grounded framework for learning.\n",
    "\n",
    "For those seeking a deeper technical dive or troubleshooting support, the text recommends two specific JAX resources:\n",
    "* **Official JAX Tutorials:** The primary source for detailed, hands-on learning and practical application of the framework (https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html).\n",
    "* **The \"Sharp Bits\" Notebook:** An essential reference guide that documents common pitfalls and non-intuitive behaviors unique to JAX's functional programming model (https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10651a41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8475e",
   "metadata": {},
   "source": [
    "### Python tips\n",
    "\n",
    "This section covers essential Python concepts frequently encountered in machine learning code, particularly with JAX and Flax frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638770f6",
   "metadata": {},
   "source": [
    "#### 1. Type Annotations and Docstrings\n",
    "\n",
    "Python is dynamically typed, which is flexible but can hide bugs. Type annotations improve readability, enable static type checking (mypy) or Vs Code's Pylance, and simplify debugging.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Basic function without type hints\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "# Improved function with type hints and docstring\n",
    "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between two NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground-truth values.\n",
    "        y_pred (np.ndarray): Predicted values.\n",
    "    Return:\n",
    "        float: The mean squared error.\n",
    "    \"\"\"\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Clarifies input/output types\n",
    "- Enhances IDE documentation and autocomplete\n",
    "- Improves code readability\n",
    "- Enables static type checking with tools like mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c469c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mean_squared_error in module __main__:\n",
      "\n",
      "mean_squared_error(y_true: numpy.ndarray, y_pred: numpy.ndarray) -> float\n",
      "    Calculate the Mean Squared Error (MSE) between two NumPy arrays.\n",
      "    \n",
      "    Args:\n",
      "        y_true (np.ndarray): Ground-truth values.\n",
      "        y_pred (np.ndarray): Predicted values.\n",
      "    Return:\n",
      "        float: The mean squared error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between two NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground-truth values.\n",
    "        y_pred (np.ndarray): Predicted values.\n",
    "    Return:\n",
    "        float: The mean squared error.\n",
    "    \"\"\"\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "# How to print the docstring of the function \n",
    "help(mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761aa555",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54eae6",
   "metadata": {},
   "source": [
    "#### 2. Decorators\n",
    "\n",
    "Decorators are functions that modify the behavior of other functions, commonly used for performance enhancement, caching, or logging.\n",
    "\n",
    "For example, JIT compilation with JAX (`@jax.jit`) is a decorator (see codes in below cell)\n",
    "\n",
    "**How `@jax.jit` works:**\n",
    "1. Traces the function using special tracer objects (not real data)\n",
    "2. Builds a computation graph (static representation of operations)\n",
    "3. Compiles via XLA (Accelerated Linear Algebra) to optimized machine code\n",
    "4. Caches compiled version for reuse with same input shapes/types\n",
    "5. Results in ~20x speedup on GPU\n",
    "\n",
    "**JIT Debugging Challenges:**\n",
    "- `print()` statements and `pdb` don't work as expected\n",
    "- Side effects are skipped during tracing\n",
    "- Cryptic error messages referencing internal JAX/XLA code\n",
    "\n",
    "**Solution**: Set environment variable `JAX_DISABLE_JIT=True` to globally disable JIT for debugging or you may set directly in your Python code:\n",
    "\n",
    "```python\n",
    "import jax\n",
    "jax.config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "def f(x):\n",
    "    y = jnp.log(x)\n",
    "    if jnp.isnan(y):\n",
    "        breakpoint()\n",
    "    return y\n",
    "\n",
    "jax.jit(f)(-2.)  # ==> Enters PDB breakpoint!\n",
    "\n",
    "```\n",
    "\n",
    "**Strengths and limitations of `jax_disable_jit`**\n",
    "* **Strengths:**\n",
    "    * Easy to apply\n",
    "    * Enables use of Python’s built-in `breakpoint` and `print`\n",
    "    * Throws standard Python exceptions and is compatible with PDB postmortem\n",
    "* **Limitations:**\n",
    "    * Running functions without JIT-compilation can be slow\n",
    "\n",
    "See the [JAX debugging documentation](https://docs.jax.dev/en/latest/debugging/flags.html#jax-disable-jit-configuration-option-and-context-manager) for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba14a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JIT output: 10874275\n",
      "CPU times: user 37.4 ms, sys: 3.03 ms, total: 40.4 ms\n",
      "Wall time: 39.9 ms\n",
      "Jitted function output: 10874275\n",
      "CPU times: user 21.9 ms, sys: 24 μs, total: 21.9 ms\n",
      "Wall time: 21.3 ms\n",
      "Decorator function output: 10874275\n",
      "CPU times: user 17.1 ms, sys: 2.01 ms, total: 19.1 ms\n",
      "Wall time: 18.9 ms\n",
      "Subsequent call: 10874275\n",
      "CPU times: user 88 μs, sys: 0 ns, total: 88 μs\n",
      "Wall time: 91.3 μs\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Basic function\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n",
    "    return jnp.sum(arr**10)\n",
    "# No JIT compilation\n",
    "%time print(f'No JIT output: {compute_ten_power_sum(arr)}')\n",
    "\n",
    "# Method 1 - Apply JIT directly\n",
    "jitted_compute_ten_power_sum = jax.jit(compute_ten_power_sum)\n",
    "# first call (compilation time) takes longer\n",
    "%time print(f'Jitted function output: {jitted_compute_ten_power_sum(arr)}')\n",
    "\n",
    "# Method 2 - Use decorator syntax\n",
    "@jax.jit\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n",
    "    return jnp.sum(arr**10)\n",
    "# first call (compilation time) takes longer\n",
    "%time print(f'Decorator function output: {compute_ten_power_sum(arr)}')\n",
    "%time print(f'Subsequent call: {compute_ten_power_sum(arr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1d0775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10874275\n",
      "CPU times: user 21.2 ms, sys: 2.03 ms, total: 23.2 ms\n",
      "Wall time: 22.3 ms\n",
      "10874275\n",
      "CPU times: user 305 μs, sys: 0 ns, total: 305 μs\n",
      "Wall time: 278 μs\n",
      "10874275\n",
      "CPU times: user 111 μs, sys: 0 ns, total: 111 μs\n",
      "Wall time: 115 μs\n",
      "10874274\n",
      "CPU times: user 19.6 ms, sys: 814 μs, total: 20.4 ms\n",
      "Wall time: 20.2 ms\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Computes the sum of 10 raised to the power of each element in the input array.\"\"\"\n",
    "    return jnp.sum(arr ** 10)\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "%time print(compute_ten_power_sum(arr)) # first call (compilation time) takes longer\n",
    "%time print(compute_ten_power_sum(arr)) # subsequent calls are faster\n",
    "\n",
    "arr = jnp.array([5, 4, 3, 2, 1])\n",
    "%time print(compute_ten_power_sum(arr)) # if array shape/dtype is the same, no recompilation\n",
    "\n",
    "arr = jnp.array([5, 4, 3, 2])\n",
    "%time print(compute_ten_power_sum(arr)) # different shape, triggers recompilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30170cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_10342/2073816543.py\u001b[39m(\u001b[92m10\u001b[39m)\u001b[36mf\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m jnp.isnan(y):\n",
      "\u001b[32m      9\u001b[39m         pdb.set_trace()\n",
      "\u001b[32m---> 10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[32m     11\u001b[39m \n",
      "\u001b[32m     12\u001b[39m jax.jit(f)(-\u001b[32m2.\u001b[39m)  \u001b[38;5;66;03m# ==> Enters PDB breakpoint!\u001b[39;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import pdb\n",
    "\n",
    "jax.config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "def f(x):\n",
    "    y = jnp.log(x)\n",
    "    if jnp.isnan(y):\n",
    "        pdb.set_trace()\n",
    "    return y\n",
    "\n",
    "jax.jit(f)(-2.)  # ==> Enters PDB breakpoint!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc684ac",
   "metadata": {},
   "source": [
    "#### Bonus: More about JIT compilation\n",
    "\n",
    "The term **\"Just-in-Time\" (JIT)** refers to the exact moment the compilation happens. In traditional programming languages (like C++ or Fortran), compilation happens before you ever run the program. In JAX, the compilation happens *while the program is running*, specifically the very first time a function is called.\n",
    "\n",
    "Here is a breakdown of why this distinction matters and how it works:\n",
    "\n",
    "##### 1. The Timing: A \"Late\" Compilation\n",
    "\n",
    "In a standard \"Ahead-of-Time\" (AOT) workflow, you compile your code into a binary file, and then you run that file. In JAX, you provide a Python function, and the \"Just-in-Time\" compiler stays idle until you actually trigger that function with real data.\n",
    "* **Step 1:** You define the function.\n",
    "* **Step 2:** You call the function with an input of a specific shape (e.g., a protein sequence of length $L = 500$).\n",
    "* **Step 3 (The \"Just-in-Time\" part):** JAX realizes it doesn't have a compiled version for that specific input shape yet. It pauses, converts the Python code into an optimized XLA kernel, and then executes it.\n",
    "\n",
    "##### 2. Tracing:\n",
    "\n",
    "The reason JAX waits until the \"last second\" (Just-in-Time) is because it needs to see the **shapes** and **types** of your data to optimize effectively. This process is called **Tracing**.\n",
    "\n",
    "When you call a JIT-ed function, JAX sends \"abstract\" versions of your data through the function to see what happens. It records every operation (+, −, ×, ÷) to create a **StableHLO** (a high-level intermediate representation). By waiting until you provide data, JIT can:\n",
    "* See that your matrix is $1000 \\times 1000$.\n",
    "* Optimize the machine code specifically for those dimensions.\n",
    "\n",
    "##### 3. Specialization\n",
    "\n",
    "If you call the same function later with a *different* shape (e.g., a sequence of length $L=200$), JAX will compile it again, \"Just-in-Time\" for that new shape. It builds a library of specialized versions of your function in the background.\n",
    "\n",
    "##### Summary of Comparison of different compilation methods\n",
    "\n",
    "| Feature | Interpreted (Python/NumPy) | Ahead-of-Time (C++/Fortran) | Just-in-Time (JAX/XLA) |\n",
    "| --- | --- | --- | --- |\n",
    "| **When is it compiled?** | Never (translated line-by-line) | Before the program runs | During execution (on first call) |\n",
    "| **Performance** | Slow (High overhead) | Very Fast | Very Fast |\n",
    "| **Flexibility** | High | Low (must re-compile manually) | High (auto-specializes to shapes) |\n",
    "\n",
    "##### Why this is a \"Scientific\" Advantage\n",
    "\n",
    "In biological modeling, we often deal with variable-sized inputs (different DNA lengths, different number of atoms in a molecule). JIT allows us to write flexible Python code that feels \"easy,\" while the compiler works \"Just-in-Time\" to give us the speed of a low-level language like C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149ad99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc076a1",
   "metadata": {},
   "source": [
    "#### 3. Preconfiguring JAX JIT with `partial`\n",
    "\n",
    "`functools.partial` prefills/binds arguments to create new functions with fixed values, a general utility in Python.\n",
    "\n",
    "**Basic example:**\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "\n",
    "def scale(x, scaling_factor):\n",
    "    return x * scaling_factor\n",
    "\n",
    "# Create new function with scaling_factor fixed to 10\n",
    "scale_by_10 = partial(scale, scaling_factor=10)\n",
    "scale_by_10(3)\n",
    "# Output: 30\n",
    "\n",
    "```\n",
    "\n",
    "Here, `scale_by_10` is a new function that behaves like `scale(x, 10)`.\n",
    "\n",
    "\n",
    "**JAX-specific usage with static arguments:**\n",
    "\n",
    "In the context of JAX, `partial` is often used to customize a decorator before applying it, like this: `@partial(jax.jit, static_argnums=...)`. This is a way to configure the `jax.jit` decorator itself.\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def summarize(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported average type: {average_method}\")\n",
    "\n",
    "data_array = jnp.array([1.0, 2.0, 100.0])\n",
    "\n",
    "# JAX compiles one version for average_method=\"mean\"\n",
    "print(f\"Mean: {summarize('mean', data_array)}\")\n",
    "\n",
    "# JAX compiles another version for average_method=\"median\"\n",
    "print(f\"Median: {summarize('median', data_array)}\")\n",
    "\n",
    "# Calling with \"mean\" again uses cached compiled version\n",
    "print(f\"Mean again: {summarize('mean', data_array)}\")\n",
    "\n",
    "# Output:\n",
    "# Mean: 34.333335876464844\n",
    "# Median: 2.0\n",
    "# Mean again: 34.333335876464844\n",
    "```\n",
    "\n",
    "If we didn’t mark `average` as static with `static_argnums=(0,)`, JAX would throw an error, because it can’t trace control flow that depends on strings unless it knows their value ahead of time. Marking arguments as static tells JAX to compile a separate, specialized version of the function for each unique value of that static argument it encounters.\n",
    "\n",
    "**Static vs Dynamic arguments:**\n",
    "\n",
    "* **Dynamic**: Numerical inputs (`jax.Array`, `float`, `int`) - can vary without recompilation if shapes/types remain constant.\n",
    "* **Static**: Strings, Python objects, functions - affect control flow; must mark with `static_argnums` or `static_argnames` or use closures (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae83a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "CPU times: user 22.5 ms, sys: 3.76 ms, total: 26.2 ms\n",
      "Wall time: 25.9 ms\n",
      "3.0\n",
      "CPU times: user 210 μs, sys: 0 ns, total: 210 μs\n",
      "Wall time: 222 μs\n",
      "3.0\n",
      "CPU times: user 64.4 ms, sys: 8.12 ms, total: 72.5 ms\n",
      "Wall time: 50.8 ms\n",
      "3.0\n",
      "CPU times: user 96 μs, sys: 10 μs, total: 106 μs\n",
      "Wall time: 109 μs\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_disable_jit\", False)\n",
    "\n",
    "# @partial(jax.jit, static_argnums=(0,)) # using deprecated static_argnums\n",
    "@partial(jax.jit, static_argnames=(\"average_method\",)) # using static_argnames\n",
    "def summarize(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown average method: {average_method}\")\n",
    "    \n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "%time print(summarize(\"mean\", arr))  # JIT compilation for \"mean\"\n",
    "%time print(summarize(\"mean\", arr))  # Subsequent call for \"mean\"\n",
    "%time print(summarize(\"median\", arr))  # JIT compilation for \"median\"\n",
    "%time print(summarize(\"median\", arr))  # Subsequent call for \"median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 μs, sys: 901 μs, total: 920 μs\n",
      "Wall time: 796 μs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error interpreting argument to <function summarize_2 at 0x768d6c4d1d00> as an abstract array. The problematic value is of type <class 'str'> and was passed to the function at path average_method.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown average method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m arr = jnp.array([\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m20\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprint(summarize_2(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, arr))\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Packages/anaconda3/envs/compbio/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2511\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2509\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2511\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2513\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2515\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Packages/anaconda3/envs/compbio/lib/python3.11/site-packages/IPython/core/magics/execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Packages/anaconda3/envs/compbio/lib/python3.11/site-packages/IPython/core/magics/execution.py:1397\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1395\u001b[39m st = clock2()\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m     out = \u001b[38;5;28meval\u001b[39m(code, glob, local_ns)\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1399\u001b[39m     captured_exception = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed eval>:1\u001b[39m\n",
      "    \u001b[31m[... skipping hidden 3 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Packages/anaconda3/envs/compbio/lib/python3.11/site-packages/jax/_src/pjit.py:613\u001b[39m, in \u001b[36m_infer_input_type\u001b[39m\u001b[34m(fun, dbg, explicit_args)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    612\u001b[39m   arg_description = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdbg.arg_names[i]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mdbg.arg_names\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# pytype: disable=name-error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    614\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError interpreting argument to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as an abstract array.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m The problematic value is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and was passed to\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# pytype: disable=name-error\u001b[39;00m\n\u001b[32m    616\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m the function at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    617\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThis typically means that a jit-wrapped function was called with a non-array\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    618\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m argument, and this argument was not marked as static using the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    619\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m static_argnums or static_argnames parameters of jax.jit.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    620\u001b[39m   ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.mutable_array_checks.value:\n\u001b[32m    622\u001b[39m   check_no_aliased_ref_args(\u001b[38;5;28;01mlambda\u001b[39;00m: dbg, avals, explicit_args)\n",
      "\u001b[31mTypeError\u001b[39m: Error interpreting argument to <function summarize_2 at 0x768d6c4d1d00> as an abstract array. The problematic value is of type <class 'str'> and was passed to the function at path average_method.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit."
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def summarize_2(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown average method: {average_method}\")\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 20])\n",
    "%time print(summarize_2(\"mean\", arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9147fa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4afd9",
   "metadata": {},
   "source": [
    "#### 4. Closures\n",
    "\n",
    "In Python, a **closure** is a function object that \"remembers\" values in the enclosing scope even if they are no longer present in memory.\n",
    "\n",
    "For a closure to exist, three conditions must be met:\n",
    "1. There must be a **nested function** (a function inside a function).\n",
    "2. The nested function must refer to a value defined in the **enclosing function**.\n",
    "3. The enclosing function must return the nested function.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "def outer_function(x):\n",
    "    def inner_function(y):\n",
    "        return x + y  # inner_function \"closes over\" x\n",
    "    return inner_function\n",
    "\n",
    "add_five = outer_function(5)  # x is 5\n",
    "result = add_five(10)  # y is 10\n",
    "print(f\"Closure result: {result}\")\n",
    "# Output: Closure result: 15\n",
    "```\n",
    "\n",
    "#### The `nonlocal` Keyword \n",
    "\n",
    "By default, a closure can read the outer variable but cannot modify it. If you want to change a variable in the enclosing scope, you must use the `nonlocal` keyword. This is common for creating \"counters\" or \"accumulators.\"\n",
    "\n",
    "```python\n",
    "def make_counter():\n",
    "    count = 0\n",
    "    def increment():\n",
    "        nonlocal count  # Allows modification of the outer 'count'\n",
    "        count += 1\n",
    "        return count\n",
    "    \n",
    "    return increment\n",
    "\n",
    "counter_a = make_counter()\n",
    "print(counter_a())  # Output: 1\n",
    "print(counter_a())  # Output: 2\n",
    "\n",
    "counter_b = make_counter()\n",
    "print(counter_b())  # Output: 1 (Starts its own separate count)\n",
    "\n",
    "```\n",
    "\n",
    "#### Why use Closures?\n",
    "\n",
    "In machine learning (and especially in JAX), closures are powerful for:\n",
    "\n",
    "* **Data Hiding:** They provide a way to store state without using a full Class object.\n",
    "* **Function Factories:** You can generate specialized versions of a function (like a specific loss function with fixed hyperparameters).\n",
    "* **Decorators:** Closures are the underlying mechanism that makes Python decorators work.\n",
    "\n",
    "### Comparison to Classes\n",
    "\n",
    "If you only have one method in a class, a closure is often a more elegant, lightweight, and memory-efficient solution.\n",
    "\n",
    "| Feature | Closure | Class |\n",
    "| --- | --- | --- |\n",
    "| **Setup** | Lightweight (function) | Heavier (object + methods) |\n",
    "| **State** | Fixed via \"backpack\" | Mutable via `self` |\n",
    "| **Usage** | Functional programming | Object-Oriented programming |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aacd38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "5.0\n",
      "<function outer_function.<locals>.inner_function at 0x7e9fd82ccb80>\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Example 1 \n",
    "def outer_function(x: float):\n",
    "    def inner_function(y: float):\n",
    "        return y + x\n",
    "    return inner_function\n",
    "\n",
    "\n",
    "add_five = outer_function(5.0)\n",
    "print(add_five(3.0))  # Outputs 8.0\n",
    "print(add_five.__closure__[0].cell_contents)  # Inspect closure to see captured variables\n",
    "print(add_five)\n",
    "\n",
    "\n",
    "# Example 2\n",
    "def make_counter():\n",
    "    count = 0\n",
    "    def counter():\n",
    "        nonlocal count\n",
    "        count += 1\n",
    "        return count\n",
    "    return counter\n",
    "counter = make_counter()\n",
    "print(counter())  # Outputs 1\n",
    "print(counter())  # Outputs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1611cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10743417",
   "metadata": {},
   "source": [
    "#### 5. Generators\n",
    "\n",
    "Iterates over data lazily (one item at a time) - essential for large datasets that don't fit in memory.\n",
    "\n",
    "**Simple generator:**\n",
    "\n",
    "```python\n",
    "from typing import Iterator\n",
    "\n",
    "def data_generator() -> Iterator[dict]:\n",
    "    \"\"\"Yield data samples with features and labels.\"\"\"\n",
    "    for i in range(5):\n",
    "        yield {\"feature\": i, \"label\": i % 2}\n",
    "\n",
    "# Example usage\n",
    "generator = data_generator()\n",
    "next(generator)\n",
    "# Output: {'feature': 0, 'label': 0}\n",
    "```\n",
    "\n",
    "**Integration with TensorFlow Datasets (TFDS):**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "features = np.array([1, 2, 3, 4, 5])\n",
    "labels = np.array([0, 1, 0, 1, 0])\n",
    "\n",
    "# Create TensorFlow dataset from NumPy arrays\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Batch with size 2, drop incomplete final batch\n",
    "batched_dataset = dataset.batch(2, drop_remainder=True)\n",
    "\n",
    "# Create iterator and retrieve first batch\n",
    "ds = iter(batched_dataset)\n",
    "next(ds)\n",
    "# Output:\n",
    "# (<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>,\n",
    "#  <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n",
    "```\n",
    "\n",
    "**Why TFDS with JAX?**\n",
    "- JAX lacks native data-loading library\n",
    "- TFDS provides clean API for batching, shuffling, and prefetching\n",
    "- Custom pipelines offer more control (covered in later chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1297adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 0, 'label': 0}\n",
      "{'feature': 1, 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "def data_generator() -> Iterator[dict]:\n",
    "    \"\"\"Yield data samples with features and labels.\"\"\"\n",
    "    for i in range(5):\n",
    "        yield {\"feature\": i, \"label\": i % 2}\n",
    "\n",
    "# Example usage\n",
    "generator = data_generator()\n",
    "print(next(generator))\n",
    "# Output: {'feature': 0, 'label': 0}\n",
    "print(next(generator))\n",
    "# Output: {'feature': 1, 'label': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead94edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>)\n",
      "End of dataset reached.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import jax.numpy as jnp\n",
    "\n",
    "features = jnp.array([1, 2, 3, 4, 5])\n",
    "labels = jnp.array([0, 0, 1 , 1, 0])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "batched_dataset = dataset.batch(2, drop_remainder=True)\n",
    "\n",
    "ds = iter(batched_dataset)\n",
    "try:\n",
    "    print(next(ds))\n",
    "    print(next(ds))\n",
    "    print(next(ds))\n",
    "except StopIteration:\n",
    "    print(\"End of dataset reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945db33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee5c1d",
   "metadata": {},
   "source": [
    "### Anatomy of a Training Loop with JAX/Flax\n",
    "\n",
    "The core structure of training a model remains fairly consistent in machine learning projects. Here are the list of core steps when you are trainining a model:\n",
    "1. Defining a dataset\n",
    "2. Defining a model\n",
    "3. Creating a training state\n",
    "4. Defining a loss function\n",
    "5. Defining the training step\n",
    "6. Handling auxilary outputs in the loss function\n",
    "7. Defining the training loop\n",
    "\n",
    "In the following section, we will go through each step with a working example:\n",
    "\n",
    "#### 1. Defining a dataset\n",
    "\n",
    "Let's create a dataset with a linear relationship between the feature $x$ and the label $y$. We will use JAX random generator to add noise to the data.\n",
    "\n",
    "**Note on randomness in JAX:** In JAX, randomness is handled differently than in standard NumPy or PyTorch. Because JAX is functional and deterministic, it uses *Explicit Pseudo-Random Number Generation (PRNG)*. This means you must manually manage and \"pass\" the state of the random number generator.\n",
    "\n",
    "* **Initializing the Key:**\n",
    "\n",
    "    ```python\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "\n",
    "    ```\n",
    "    Above code creates a \"seed\" or a master key. In JAX, the `rng` (Random Number Generator) state is an array of *two integers*. Unlike `np.random.seed()`, which sets a global hidden state, JAX requires this explicit key to ensure that if you run the same code twice, you get the exact same results (reproducibility).\n",
    "\n",
    "* **Splitting the Key:**\n",
    "\n",
    "    ```python\n",
    "    rng, rng_data, rng_noise = jax.random.split(rng, 3)\n",
    "\n",
    "    ```\n",
    "\n",
    "    The above code \"splits\" the master key into three new, independent sub-keys. This is the most important rule in JAX: *Never reuse a key.* If you used the same `rng` to generate both your data and your noise, they would be correlated. The outputs of `split` in above code are:\n",
    "    * `rng`: A new \"lead\" key to be used for future splits.\n",
    "    * `rng_data`: A key specifically for generating the $x$ values.\n",
    "    * `rng_noise`: A key reserved for generating noise to be added to the label.\n",
    "\n",
    "In NumPy, the state is updated behind the scenes (mutated). In JAX, the state is passed explicitly (functional).\n",
    "\n",
    "| Step | NumPy (Implicit) | JAX (Explicit) |\n",
    "| --- | --- | --- |\n",
    "| **Initialization** | `np.random.seed(42)` | `key = jax.random.PRNGKey(42)` |\n",
    "| **State Update** | Automatic | `key, subkey = jax.random.split(key, 2)` |\n",
    "| **Generation** | `np.random.uniform()` | `jax.random.uniform(subkey)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c87fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master key: [ 0 42]\n",
      "Master sub-key: [1832780943  270669613]\n",
      "Data key: [  64467757 2916123636]\n",
      "Noise key: [2465931498  255383827]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In JAX, randomness is handled explicitly by passing a random key.\n",
    "# We create a key here to seed the random number generator.\n",
    "rng = jax.random.PRNGKey(42)\n",
    "print(f\"Master key: {rng}\")\n",
    "rng, rng_data, rng_noise = jax.random.split(rng, 3)\n",
    "print(f\"Master sub-key: {rng}\")\n",
    "print(f\"Data key: {rng_data}\")\n",
    "print(f\"Noise key: {rng_noise}\")\n",
    "\n",
    "# Generate toy data: x values uniformly sampled between 0 and 1.\n",
    "x_data = jax.random.uniform(rng_data, shape=(100, 1), minval=0.0, maxval=1.0) # Default range is [0,1]\n",
    "\n",
    "# Generate Gaussian noise to be added to the data.\n",
    "noise = jax.random.normal(rng_noise, shape=(100, 1)) * 0.1\n",
    "\n",
    "# Define label (target): y = 2 * x + 1 + noise.\n",
    "y_data = 2 * x_data + 1 + noise\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(x_data, y_data, alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Toy dataset: y = 2x +1 + noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c7f4b",
   "metadata": {},
   "source": [
    "#### 2. Defining a model\n",
    "\n",
    "This section explains how to define and initialize a model using *Flax*, highlighting the library's functional design and its unique approach to parameter management.\n",
    "\n",
    "* **Flax Modules**: Models are created by inheriting from `nn.Module`. Unlike PyTorch, these modules are stateless blueprints; they do not store the weights internally.\n",
    "* **The `@nn.compact` Decorator**: This allows for \"inline\" layer definition. You can declare and use layers (like `nn.Dense`) directly within the `__call__` method, simplifying the code for sequential architectures.\n",
    "* **Lazy Shape Inference**: Flax doesn't know the dimensions of your weights (the `kernel` and `bias`) until you provide a sample input. This \"just-in-time\" shape inference provides more control and clarity during JAX transformations.\n",
    "* **Parameter Initialization**: Using `model.init()`, Flax generates a nested dictionary (often called `variables` or `params`) containing the actual numerical arrays for the weights.\n",
    "\n",
    "##### Comparison: Flax vs. Object-Oriented Frameworks\n",
    "\n",
    "| Feature | Flax (Functional) | PyTorch/Keras (OO) |\n",
    "| --- | --- | --- |\n",
    "| **State Storage** | Parameters stored in a separate dict | Parameters stored inside the Layer object |\n",
    "| **Shape Definition** | Inferred during `.init()` call | Usually defined during instantiation |\n",
    "| **Architecture** | Explicit and JIT-friendly | Automatic and state-heavy |\n",
    "\n",
    "Here’s a minimal example, a single linear (dense) layer with one output unit and no activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e19a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'Dense_0': {'kernel': Array([[-0.5220277]], dtype=float32), 'bias': Array([0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "# 1. Define the model architecture.\n",
    "class LinearModel(nn.Module):\n",
    "    # The @nn.compact decorator allows you to define parameters \n",
    "    # (like nn.Dense) inside the __call__ method.\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Creates a dense layer with 1 output unit.\n",
    "        # Computes y = xW + b. \n",
    "        # On the first call, it uses 'x' to figure out the input shape.\n",
    "        return nn.Dense(features=1)(x)\n",
    "\n",
    "# 2. Instantiate the blueprint.\n",
    "# This doesn't create weights yet; it just creates the model structure.\n",
    "model = LinearModel()\n",
    "\n",
    "# 3. Setup Randomness.\n",
    "# JAX requires explicit PRNG keys for initialization.\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# 4. Initialize Parameters (The \"Dry Run\").\n",
    "# .init() calls __call__ with dummy data to determine weight shapes.\n",
    "# variables will contain the actual weight/bias arrays.\n",
    "variables = model.init(rng, jnp.ones([1, 1]))\n",
    "\n",
    "# 5. Inspect the State\n",
    "# This prints the parameter dictionary: Kernel and bias .\n",
    "print(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5fe71",
   "metadata": {},
   "source": [
    "Here:\n",
    "* `kernel` is the learned weight matrix (shape [1, 1], since our input and output dimensions are both 1).\n",
    "* `bias` is the learned bias term added after the matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8593ff",
   "metadata": {},
   "source": [
    "#### 3. Creating a training state\n",
    "\n",
    "This section introduces the *TrainState*, a crucial Flax utility that centralizes the components required for a training loop into a single, immutable container. Here are key takeaways:\n",
    "\n",
    "* **Centralized Container**: `TrainState` bundles the *model parameters*, the *optimizer*, and the *forward pass function* (`apply_fn`) into one object.\n",
    "* **The Optimizer (Tx)**: Uses the *Optax* library, where optimizers are treated as \"gradient transformations.\"\n",
    "* **Functional Immutability**: Following JAX's core principles, the `TrainState` is *immutable*. Updating the model does not change the state in place; instead, it returns a new state object with updated parameters.\n",
    "* **Memory Efficiency**: Despite creating new objects for each update, JAX’s XLA compiler manages memory efficiently, reusing buffers under the hood to prevent overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91bd4c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=0, apply_fn=<bound method Module.apply of LinearModel()>, params={'Dense_0': {'kernel': Array([[-0.5220277]], dtype=float32), 'bias': Array([0.], dtype=float32)}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7e9f3a62b1a0>, update=<function chain.<locals>.update_fn at 0x7e9f3a62b380>), opt_state=(ScaleByAdamState(count=Array(0, dtype=int32), mu={'Dense_0': {'bias': Array([0.], dtype=float32), 'kernel': Array([[0.]], dtype=float32)}}, nu={'Dense_0': {'bias': Array([0.], dtype=float32), 'kernel': Array([[0.]], dtype=float32)}}), EmptyState()))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# 1. Define an optimizer — here we use Adam with a learning rate of 0.1.\n",
    "# (Note: in most real settings you'd use a smaller learning rate like 1e-3).\n",
    "tx = optax.adam(0.1)\n",
    "\n",
    "# 2. Create the training state.\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,       # The model's forward pass function.\n",
    "    params=variables[\"params\"], # The initialized model parameters.\n",
    "    tx=tx,                      # The optimizer.\n",
    ")\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd7ea9",
   "metadata": {},
   "source": [
    "#### 4. Defining a loss function\n",
    "\n",
    "This section describes the creation of a *Loss Function*, specifically *Mean Squared Error (MSE)*, which serves as the objective for the optimization process. It highlights how JAX's functional paradigm influences the way model logic and parameters interact.\n",
    "\n",
    "Here is how cost function would be defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b74c2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.2768\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss(params, x, y):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) loss.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of model parameters (weights and biases).\n",
    "        x: Input features of shape (batch_size, input_dim).\n",
    "        y: Target labels of shape (batch_size, output_dim).\n",
    "    \n",
    "    Returns:\n",
    "        float: The mean squared error loss value.\n",
    "    \"\"\"\n",
    "    # Run a forward pass of the model to get predictions.\n",
    "    # We pass the stateless model the params it needs for this specific computation.\n",
    "    predictions = model.apply({\"params\": params}, x)\n",
    "    \n",
    "    # Compute MSE loss: Mean((pred - target)^2)\n",
    "    # This results in a single scalar value that JAX can differentiate.\n",
    "    return jnp.mean((predictions - y) ** 2)\n",
    "\n",
    "# Calculate initial loss using the random parameters from our earlier initialization\n",
    "loss = calculate_loss(variables[\"params\"], x_data, y_data)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "# # calculate_loss performs the following calculations   \n",
    "# w = variables[\"params\"]['Dense_0']['kernel']\n",
    "# b = variables[\"params\"]['Dense_0']['bias']\n",
    "# pred = w * x_data + b\n",
    "# jnp.mean((pred - y_data) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d247794",
   "metadata": {},
   "source": [
    "Here are the key takeaways:\n",
    "* **Objective Measurement**: The loss function quantifies the discrepancy between the model's predictions and the ground-truth targets.\n",
    "* **Functional Purity**: Even though the function references the `model` defined in the outer scope, it remains \"pure.\" This is because the model logic is constant, while all variable state (the weights) is explicitly passed in via the `params` argument.\n",
    "* **Forward Pass**: The `model.apply` method is used inside the loss function to map inputs $x$ to predictions $\\hat y$ using the provided parameters $\\theta$.\n",
    "* **Starting Point**: Evaluating the loss with initial (random) parameters provides a baseline. In the above example, the loss starts at approximately 5.2768, and the goal of training is to minimize this scalar value.\n",
    "\n",
    "#### Why this is the \"JAX Way\"\n",
    "\n",
    "You'll note that the `calculate_loss` function is designed to be wrapped by `jax.grad`. JAX requires that the **first argument** of the function being differentiated is the variable you want to update (in this case, `params`). By placing `params` first, you allow JAX to compute the gradient of the loss with respect to every weight in your neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e22235",
   "metadata": {},
   "source": [
    "#### 5. Defining the Training Step\n",
    "\n",
    "This section defines the **Training Step**, the core loop iteration where the model actually learns. It integrates JAX's transformation power (JIT and grad) with Flax's `TrainState` to create an optimized update mechanism.\n",
    "\n",
    "Here are the key takeaways:\n",
    "\n",
    "* **Compilation with `jax.jit`:** Wrapping the training step in `@jax.jit` compiles the entire operation into a single optimized XLA graph. This is essential for high performance on GPUs and TPUs.\n",
    "* **Efficient Differentiation**: `jax.value_and_grad` is used to return both the *scalar loss* (for logging/monitoring) and the *gradients* (for updating weights) in a single pass, avoiding redundant calculations.\n",
    "* **The Closure Pattern**: Defining the loss function *inside* the training step allows it to \"close over\" the `state`, `x`, and `y` variables. This simplifies the function signature, as `jax.grad` only needs to focus on the parameters.\n",
    "* **State Update**: `state.apply_gradients(grads=grads)` produces a new, updated `TrainState` containing the new parameters and the updated optimizer state.\n",
    "\n",
    "We cabn implement a training step in two ways:\n",
    "\n",
    "* **Version 1: Direct Argument Passing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3509f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_v1(state, x, y):\n",
    "    \"\"\"Perform a single training step (direct-argument version).\n",
    "\n",
    "    Computes the mean-squared error loss and its gradients with respect to the\n",
    "    model parameters, applies optimizer updates, and returns the new training\n",
    "    state and the scalar loss.\n",
    "\n",
    "    Args:\n",
    "        state: A flax.training.train_state.TrainState containing params, apply_fn, and optimizer state.\n",
    "        x: Input batch (jax.Array) with shape (batch_size, ...).\n",
    "        y: Target batch (jax.Array) with shape (batch_size, ...).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[new_state, loss]: \n",
    "            new_state: Updated TrainState after applying gradients.\n",
    "            loss: Scalar loss value (jax.Array) for the provided batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the loss and its gradients with respect to the parameters.\n",
    "    # jax.value_and_grad(compute_loss) expects (params, x, y)\n",
    "    loss, grads = jax.value_and_grad(calculate_loss)(state.params, x, y)\n",
    "    \n",
    "    # Apply gradient updates and return the new immutable state.\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    return new_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba4063",
   "metadata": {},
   "source": [
    "* **Version 2: Using a Closure (Recommended)**\n",
    "\n",
    "    This version is more common in JAX/Flax development as it keeps the differentiation logic focused strictly on the `params`. By using the **Closure Pattern** inside a JIT-compiled function, you ensure that the XLA compiler can see the entire computation at once. This allows it to perform *buffer assignment optimization*, effectively making the \"immutable\" update as fast as an in-place  pointer update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e22c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, x, y):\n",
    "    \"\"\"\n",
    "    Perform a single training step using a closure over `state`, `x`, and `y`.\n",
    "\n",
    "    Args:\n",
    "        state (flax.training.train_state.TrainState): current training state (params, apply_fn, optimizer).\n",
    "        x (jax.Array): input batch.\n",
    "        y (jax.Array): target batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[new_state, loss]:\n",
    "            new_state: Updated TrainState after applying gradients.\n",
    "            loss: Scalar MSE loss for the batch (jax.Array).\n",
    "\n",
    "    Notes:\n",
    "        - The inner function `calculate_loss(params)` closes over state, x, and y so that\n",
    "          jax.value_and_grad only differentiates w.r.t. `params`.\n",
    "        - The function is JIT-compiled for performance.\n",
    "    \"\"\"\n",
    "    def calculate_loss(params):\n",
    "        # Accesses state, x and y directly.\n",
    "        predictions = state.apply_fn({\"params\": params}, x)\n",
    "        return jnp.mean((predictions - y) ** 2)\n",
    "\n",
    "    # Calculate loss and gradients with respect to params.\n",
    "    loss, grads = jax.value_and_grad(calculate_loss)(state.params)\n",
    "    \n",
    "    # Update the state with the calculated gradients.\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, x, y):\n",
    "\n",
    "    def calculate_loss(params):\n",
    "\n",
    "        predictions = state.apply_fn({'params': params}, x)\n",
    "        return jnp.mean((predictions - y) ** 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638b128",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b9a59",
   "metadata": {},
   "source": [
    "### Bonus: Difference between `linen.nn` and `nnx` by an example\n",
    "\n",
    "In the JAX ecosystem, **Linen** (the traditional Flax API) and **NNX** (the newer, experimental Flax API) represent two fundamentally different philosophies for building neural networks.\n",
    "\n",
    "The primary difference lies in how they handle *State* (the weights, biases, and batch stats).\n",
    "\n",
    "\n",
    "#### The Core Philosophical Difference\n",
    "\n",
    "* **Linen: Purely Functional:** Linen follows JAX’s functional purity strictly. A `Linen` module is just a set of instructions; it *does not store data*.\n",
    "    * **The Model:** A stateless blueprint.\n",
    "    * **The State:** A separate, immutable dictionary (`params`).\n",
    "    * **The Workflow:** You must pass the variables into the model every time you call it: `model.apply(variables, x)`.\n",
    "\n",
    "* **NNX: Object-Oriented JAX:** NNX introduces a \"Reference-based\" approach that feels much more like PyTorch or standard Python classes, while still being compatible with JAX transformations.\n",
    "    * **The Model:** An object that actually holds its own state.\n",
    "    * **The State:** Attributes of the class (e.g., `self.kernel`).\n",
    "    * **The Workflow:** You call the model directly: `model(x)`.\n",
    "\n",
    "#### Side-by-Side Code Comparison\n",
    "\n",
    "* **Flax Linen (Functional):**\n",
    "\n",
    "    ```python\n",
    "    import flax.linen as nn\n",
    "\n",
    "    class LinearLinen(nn.Module):\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            return nn.Dense(1)(x)\n",
    "\n",
    "    model = LinearLinen()\n",
    "    variables = model.init(rng, x)  # Returns a dict\n",
    "    out = model.apply(variables, x) # Must pass variables back in\n",
    "\n",
    "    ```\n",
    "\n",
    "* **Flax NNX (Stateful/Object-Oriented)**\n",
    "\n",
    "    ```python\n",
    "    from flax import nnx\n",
    "\n",
    "    class LinearNNX(nnx.Module):\n",
    "        def __init__(self, din, dout, rngs):\n",
    "            self.dense = nnx.Linear(din, dout, rngs=rngs)\n",
    "\n",
    "        def __call__(self, x):\n",
    "            return self.dense(x)\n",
    "\n",
    "    model = LinearNNX(1, 1, rngs=nnx.Rngs(0)) # Model owns its parameters\n",
    "    out = model(x)                             # Call like a normal function\n",
    "\n",
    "    ```\n",
    "\n",
    "#### Key Differences Table\n",
    "\n",
    "| Feature | Flax Linen | Flax NNX |\n",
    "| --- | --- | --- |\n",
    "| **State Management** | **Stateless**: Params are in a separate dict. | **Stateful**: Params are attributes of the object. |\n",
    "| **Transformations** | Use `jax.jit(model.apply)`. | Use `nnx.jit(model)`. |\n",
    "| **Shape Inference** | Excellent (via `@nn.compact`). | More explicit (usually defined in `__init__`). |\n",
    "| **Initialization** | Requires a \"dummy\" forward pass. | Standard Python initialization. |\n",
    "| **Complexity** | Higher \"boilerplate\" for state management. | Lower boilerplate; feels like PyTorch. |\n",
    "\n",
    "\n",
    "#### Why NNX is developed?\n",
    "\n",
    "Linen is mathematically beautiful but can be mentally taxing for complex architectures (like GANs or models with many moving parts) because you are constantly piping dictionaries through functions. NNX was designed to:\n",
    "\n",
    "* Make it easier for developers coming from PyTorch/Keras.\n",
    "* Allow objects to track their own state (like optimizer state or RNG counters) internally.\n",
    "* It uses a clever \"functionalization\" process under the hood so that it still plays perfectly with `jax.grad` and `jax.jit`.\n",
    "\n",
    "#### Which one should we use?\n",
    "\n",
    "* **Use Linen:** If you prefer the absolute clarity of \"data in, data out\" functional programming.\n",
    "* **Use NNX:** If you find the dictionary-passing in Linen tedious and want a more modern, object-oriented feel that is likely the future of the Flax ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0491e17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
