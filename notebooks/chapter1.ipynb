{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95038adf",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction\n",
    "\n",
    "Learn about the promise and challenges of deep learning in biology. You will be walked through practical questions to consider before launching a new project—like what your model could replace, whether deep learning is even necessary, and how to structure your workflow. This chapter also includes a short technical introduction covering JAX/Flax, Python patterns common in machine learning, working environments, and practical setup tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b66ce8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da084f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Using Code Examples](#using-code-examples)\n",
    "- [Getting Started](#getting-started)\n",
    "  - [Deciding What Your Model Will Replace](#deciding-what-your-model-will-replace)\n",
    "  - [Determining Your Criteria for Success](#determining-your-criteria-for-success)\n",
    "  - [Invest Heavily in Evaluations](#invest-heavily-in-evaluations)\n",
    "  - [Designing Baselines](#designing-baselines)\n",
    "  - [Time-Boxing Your Project](#time-boxing-your-project)\n",
    "  - [Deciding Whether You Really Need Deep Learning](#deciding-whether-you-really-need-deep-learning)\n",
    "  - [Ensuring That You Have Enough Good Data](#ensuring-that-you-have-enough-good-data)\n",
    "  - [Assembling a Team](#assembling-a-team)\n",
    "  - [You Don't Need a Supercomputer or a PhD](#you-dont-need-a-supercomputer-or-a-phd)\n",
    "- [Technical Introduction](#technical-introduction)\n",
    "  - [The JAX and Flax Ecosystem](#the-jax-and-flax-ecosystem)\n",
    "  - [Why Use JAX and Flax for Biology?](#why-use-jax-and-flax-for-biology)\n",
    "  - [Python tips](#python-tips)\n",
    "  - [Anatomy of a Training Loop with JAX/Flax](#anatomy-of-a-training-loop-with-jaxflax)\n",
    "- [Machine Learning Tips](#machine-learning-tips)\n",
    "- [Selecting a Working Environment](#selecting-a-working-environment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab521299",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7f2b9",
   "metadata": {},
   "source": [
    "## Using Code Examples\n",
    "\n",
    "Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/deep-learning-for-biology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973c6d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070df83a",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Before jumping into code, we walk through how to frame a project, evaluate your data, and avoid common pitfalls. A bit of structure and planning up front will make your work more reproducible, more flexible, and ultimately more useful and impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29262c8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65242e",
   "metadata": {},
   "source": [
    "### Deciding What Your Model Will Replace\n",
    "\n",
    "This section from the introductory chapter focuses on the strategic framing of biological deep learning projects, arguing that defining the \"real-world\" target of a model is more critical than the initial choice of architecture. Here are key summary points:\n",
    "\n",
    "* **Avoid the \"Tinker Trap\":** Deep learning in biology is intellectually stimulating, which often leads researchers to spend excessive time on technical minutiae. To remain focused, one must identify the existing process the model is intended to replace or improve.\n",
    "* **Domain-Specific Impact Areas:**\n",
    "    * **Healthcare & Drug Discovery:** Models aim to replace slow or manual tasks such as dermatological diagnosis, culture-based pathogen detection, manual MRI tumor segmentation, and exhaustive wet-lab screening for drug-target interactions.\n",
    "    * **Molecular Biology:** Computational tools like *AlphaFold* provide 3D protein structures that would otherwise require months of expensive X-ray crystallography or Cryo-EM. Other models act as digital alternatives to RNA-seq (gene expression) or manual variant interpretation.\n",
    "    * **Ecology:** AI replaces labor-intensive field work, such as in-person biodiversity surveys (via acoustics) or manual crop scouting (via satellite/drone imagery), and offers non-invasive alternatives to physical animal tagging.\n",
    "* **Quantifying Success:** Researchers should estimate the potential impact in terms of time, cost, or labor.\n",
    "* **Innovation vs. Replacement:** Not all models replace old workflows. Some enable entirely new capabilities, such as generating *de novo* biological sequences or linking disparate data types that were previously incompatible. In these cases, success must be evaluated without established benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f8077",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac18ac",
   "metadata": {},
   "source": [
    "### Determining Your Criteria for Success\n",
    "\n",
    "Define success metrics early to avoid endless, unfocused experimentation and wasted time in deep learning projects.\n",
    "\n",
    "**Five Types of Success Criteria:**\n",
    "\n",
    "1. **Performance Metrics**\n",
    "   - **Examples:** accuracy, AUC, F1 score\n",
    "   - Goals may include matching human expert performance, achieving experimental correlation, or maintaining low false-positive rates\n",
    "\n",
    "2. **Interpretability Requirements**\n",
    "   - Focus on explainability and transparency of model decisions\n",
    "   - Important for domain expert trust, calibrated uncertainty estimates, and understandable feature attributions\n",
    "\n",
    "3. **Model Size and Inference Efficiency**\n",
    "   - Critical for resource-constrained environments (smartphones, embedded devices)\n",
    "   - Metrics include inference time, memory usage, energy consumption, and performance per FLOP (floating point operation)\n",
    "   - May prioritize efficiency over raw accuracy for real-time applications\n",
    "\n",
    "4. **Training Efficiency**\n",
    "   - Relevant when compute resources are limited or in educational settings\n",
    "   - May focus on CPU-compatible models rather than GPU-dependent ones\n",
    "   - Prioritizes fast training and minimal hardware requirements\n",
    "\n",
    "5. **Generalizability**\n",
    "   - Aims for models that work across multiple datasets or tasks\n",
    "   - Relevant for foundational models designed for broad applicability\n",
    "   - Values flexibility and reusability over single-task optimization\n",
    "\n",
    "**Key Takeaway:**\n",
    "Establishing clear success criteria upfront helps determine when a project is complete and ensures efforts remain focused and realistic while balancing multiple objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b8dca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef17caa",
   "metadata": {},
   "source": [
    "\n",
    "### Invest Heavily in Evaluations\n",
    "\n",
    "Evaluation strategy should be a top priority from the start, not an afterthought. It guides the entire project and determines whether your work produces meaningful results.\n",
    "\n",
    "**What Strong Evaluation Involves:**\n",
    "- Defining precise measurement methods and metrics\n",
    "- Establishing validation procedures\n",
    "- Selecting appropriate baselines for comparison\n",
    "- Creating a well-designed evaluation strategy before building models\n",
    "\n",
    "**Benefits of Strong Evaluations:**\n",
    "- Measure progress accurately\n",
    "- Detect bugs in models or pipelines\n",
    "- Estimate task difficulty\n",
    "- Build intuition about the problem\n",
    "- Provide a known point of comparison to assess if the model is learning meaningfully\n",
    "\n",
    "**Recommended Time Allocation:**\n",
    "A rough guideline for successful machine learning projects:\n",
    "- **50%** - Designing evaluation strategies and running baselines\n",
    "- **25%** - Curating or processing data\n",
    "- **25%** - Model architecture development\n",
    "\n",
    "Without good evaluations, you operate blindly — unable to determine if your model is improving, understand trade-offs, or verify meaningful learning is occurring.\n",
    "\n",
    "**Key Takeaway:** Evaluation is not an end-stage activity. It should be designed at the beginning and used to guide decisions throughout the entire project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4ff94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a5474",
   "metadata": {},
   "source": [
    "### Designing Baselines\n",
    "\n",
    "This section explains the importance of **baselines** as practical evaluation tools in machine learning — simple methods that establish minimum performance thresholds to compare against more complex models.\n",
    "\n",
    "#### Purpose of Baselines\n",
    "- Measure progress and understand task difficulty\n",
    "- Catch bugs early in model development\n",
    "- Sometimes surprisingly competitive with complex models\n",
    "- Signal when something is wrong if models can't beat them\n",
    "\n",
    "#### Classification Baselines\n",
    "\n",
    "1. **Random prediction**: Equal probability for all classes (zero information baseline)\n",
    "\n",
    "2. **Weighted random prediction**: Sample proportional to class frequencies in training data (useful for imbalanced datasets)\n",
    "\n",
    "3. **Majority class**: Always predict most common class (strong baseline for highly imbalanced problems)\n",
    "\n",
    "4. **Nearest neighbor**: Predict label of most similar training example (effective for low-dimensional or structured data)\n",
    "\n",
    "### Regression Baselines\n",
    "\n",
    "1. **Mean/median prediction**: Always predict training set average or median\n",
    "\n",
    "2. **Single-feature linear regression**: Fit line using strongest individual predictor (tests incremental value of complexity)\n",
    "\n",
    "3. **K-nearest neighbor regression**: Average target values of k most similar examples\n",
    "\n",
    "#### Domain-Specific Heuristics\n",
    "\n",
    "- Apply simple rules based on domain knowledge\n",
    "- Examples:\n",
    "  - **Diagnostics:** threshold-based classification on biomarkers\n",
    "  - **Medical imaging:** rank by average pixel intensity\n",
    "  - **Genomics:** assign mutations to nearest gene\n",
    "\n",
    "**Key Takeaway:** If your model can't beat basic baselines, investigate your data, features, or modeling approach before adding complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed22bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ada72",
   "metadata": {},
   "source": [
    "### Time-Boxing Your Project\n",
    "\n",
    "Time-boxing is the practice of setting a fixed, non-negotiable timeframe for a project or specific task. In deep learning research—where projects can become open-ended and \"failed\" experiments are common—this strategy ensures that even unsuccessful ideas provide value without draining unlimited resources.\n",
    "\n",
    "#### Strategies for Effective Time-Boxing\n",
    "\n",
    "* **Establish a Rigid Deadline:** Determine a realistic total duration for the project (e.g., two weeks or three months). The project should pause or stop once this limit is reached, regardless of whether the target metrics were achieved.\n",
    "* **Define Clear Checkpoints:** Break the timeline into intermediate milestones to monitor progress. Key checkpoints might include:\n",
    "    * Completion of data preprocessing.\n",
    "    * Training and evaluation of a baseline model.\n",
    "    * Reaching a specific performance threshold.\n",
    "* **Micro Time-Boxing:** Apply the same principle to specific sub-tasks or experimental ideas. For example, allocate exactly one week to test a new model architecture; if it does not show improvement within that window, abandon it and move on.\n",
    "* **Structured Reflection:** Use the end of the time-box to evaluate outcomes. Focus on what was learned and what technical insights can be applied to future work, transforming a \"failed\" project into a stepping stone.\n",
    "* **Mitigate Scope Creep:** Guard against the urge to justify extensions or \"one more tweak.\" When perfectionism or indecision stalls progress, consult with a mentor or collaborator to regain perspective and maintain focus on the broader goals.\n",
    "\n",
    "**Key Takeaway:** Time-boxing is a tool for maintaining focus and avoiding burnout. It forces a decision-making point where you must evaluate the project's viability, ensuring that your energy is always directed toward the most promising research avenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8e2c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eca31a",
   "metadata": {},
   "source": [
    "### Deciding Whether You Really Need Deep Learning\n",
    "\n",
    "While deep learning is a powerful tool in the biological sciences, it is not always the optimal solution. This section emphasizes the importance of evaluating whether a simpler, traditional approach can meet your project's goals more efficiently.\n",
    "\n",
    "#### Key Considerations for Choosing Your Approach\n",
    "\n",
    "* **Evaluate Simpler Alternatives:** Before committing to a deep learning architecture, consider if linear regression, decision trees, or basic statistical techniques are sufficient.\n",
    "* **Implementation and Setup:** Traditional methods are generally quicker to implement, easier to set up, and require less specialized expertise to maintain.\n",
    "* **Computational Efficiency:** Simpler models are far less resource-intensive. They can often run on standard hardware (CPUs) with minimal training time, whereas deep learning typically requires expensive GPU resources.\n",
    "* **Interpretability and Debugging:** Deep learning models are notoriously \"black boxes\" and difficult to troubleshoot. Simpler methods are often easier to explain to stakeholders, troubleshoot for errors, and validate against biological ground truth.\n",
    "* **Weighted Trade-offs:** The smarter path is often the one that delivers the required performance with the least amount of complexity. If a traditional method provides the necessary insights, the overhead of deep learning may not be justified.\n",
    "\n",
    "**Key Takaway:** The decision to use deep learning should be based on necessity rather than novelty. Prioritizing simplicity when possible leads to more robust, interpretable, and cost-effective biological research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7371f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890f6ce",
   "metadata": {},
   "source": [
    "### Ensuring That You Have Enough Good Data\n",
    "\n",
    "In the context of biological deep learning, where data acquisition can be expensive and prone to technical noise, the mantra of \"garbage in, garbage out\" is particularly relevant. This section highlights that the sophistication of your model cannot compensate for poor underlying data.\n",
    "\n",
    "#### Critical Data Requirements\n",
    "\n",
    "* **Sufficient Quantity:** Deep learning models generally require thousands of labeled examples to generalize effectively.\n",
    "    * **Benchmarking:** Consult existing literature to determine the standard dataset size for your specific biological task.\n",
    "    * **Transfer Learning:** If your dataset is small (e.g., a rare disease cohort), use transfer learning. Start with a model pre-trained on a massive, related dataset (like ImageNet for microscopy or UniProt for protein sequences) and fine-tune it on your specific data.\n",
    "* **Sufficient Quality:** The reliability of your model is capped by the cleanliness and consistency of your data.\n",
    "    * **Error Impact:** Inconsistent labeling or high levels of experimental noise can cause models to fail catastrophically.\n",
    "    * **Curation:** High-quality, curated data is often more valuable than a larger volume of \"noisy\" data. Prioritizing rigorous quality control (QC) and thoughtful curation is essential for building trustworthy models.\n",
    "\n",
    "**Key Takeaway:** Success in deep learning is a balance between scale and precision. While you need enough data to capture biological variance, that data must be clean enough for the model to learn meaningful patterns rather than experimental artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cf8d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffcd26",
   "metadata": {},
   "source": [
    "### Assembling a Team\n",
    "\n",
    "Collaborating effectively is a catalyst for success in biological deep learning, where the complexity of the data often requires a blend of computational and experimental expertise.\n",
    "\n",
    "#### Strategies for Finding and Building a Team\n",
    "\n",
    "* **Engage with Digital Communities:** Use platforms like Reddit, Discord, X, and specialized Slack groups to share ideas and meet potential partners.\n",
    "* **Participate in Structured Challenges:** Join hackathons or competitions on platforms like Kaggle or Zindi to meet people with shared interests and receive immediate feedback.\n",
    "* **Prioritize Interdisciplinary Diversity:** Aim for a \"cross-pollination\" of skills. Biologists should seek out machine learning experts, and vice versa, to ensure the model is both mathematically sound and biologically relevant.\n",
    "* **Consult Domain Experts:** Reach out to authors of relevant papers or attendees at conferences. Genuine interest in a specific biological problem often leads to successful \"cold\" outreach and expert guidance.\n",
    "\n",
    "#### Best Practices for Effective Collaboration\n",
    "\n",
    "* **Establish Clear Governance:** Define specific roles, responsibilities, and decision-making processes early to prevent misunderstandings and scope creep.\n",
    "* **Utilize a Shared Tech Stack:** Implement collaborative tools such as:\n",
    "    * **Version Control:** Git for code management.\n",
    "    * **Shared Environments:** Google Colab for interactive modeling.\n",
    "    * **Task Tracking:** Notion, Trello, or simple shared documents to organize workflows.\n",
    "* **Encourage Specialization:** Allow team members to focus on their strengths, whether that is data engineering, infrastructure, modeling, or biological interpretation.\n",
    "* **Pilot the Partnership:** Start with a small, low-pressure \"sprint\" or exploration to test compatibility before committing to a long-term research project.\n",
    "\n",
    "**Key Takeaway:** While solo research is possible, interdisciplinary teams often produce more robust and innovative results. By combining deep domain knowledge with technical ML expertise and using structured communication tools, you can significantly accelerate the \"Get Started\" phase of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abac6fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74194836",
   "metadata": {},
   "source": [
    "### You Don't Need a Supercomputer or a PhD\n",
    "\n",
    "It is a common misconception that deep learning in biology is reserved for those with elite credentials or massive infrastructure. In reality, the field is increasingly accessible to anyone with curiosity and a laptop.\n",
    "\n",
    "#### Challenging Common Misconceptions\n",
    "\n",
    "* **The \"Huge Compute\" Myth:** You do not need a supercomputer to make a meaningful impact.\n",
    "    * **Iterative Prototyping:** Start with small, lightweight models to test ideas quickly before scaling up.\n",
    "    * **Accessible Hardware:** Utilize free GPU resources from platforms like **Google Colab** or **Kaggle**. For larger tasks, scalable cloud instances (AWS, GCP, Azure) allow you to pay only for what you use.\n",
    "    * **Analysis over Training:** Significant research involves analyzing or fine-tuning existing models rather than training them from scratch, which requires much less computational power.\n",
    "* **The \"Expert-Only\" Myth:** You do not need a PhD in both ML and Biology to contribute.\n",
    "    * **Modern Tooling:** High-level frameworks (like PyTorch or JAX) have lowered the barrier to entry for building complex architectures.\n",
    "    * **Open Source Ecosystem:** Leverage pre-trained models and open-source codebases to build upon the work of others.\n",
    "    * **Abundant Learning Resources:** Tutorials, walkthroughs, and videos offer accessible pathways to mastering the necessary concepts outside of traditional academia.\n",
    "    * **Uncharted Problems:** Many biological questions have yet to be approached with a machine learning lens, leaving plenty of room for newcomers to find niche areas of discovery.\n",
    "\n",
    "**Key Takeaway:** The barrier to entry for biological deep learning is lower than it has ever been. By starting small, utilizing free resources, and leveraging the open-source community, you can contribute to the field regardless of your current budget or formal title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bddb85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a730f0",
   "metadata": {},
   "source": [
    "## Technical Introduction\n",
    "\n",
    "This section introduces the specific software ecosystem used in the book—**JAX** and **Flax**—and explains the rationale for choosing these tools for biological deep learning projects.\n",
    "\n",
    "### The JAX and Flax Ecosystem\n",
    "\n",
    "* **JAX:** A system for high-performance numerical computing that transforms Python and NumPy code into optimized machine code for accelerators (GPUs/TPUs).\n",
    "* **Flax:** A flexible neural network library designed specifically to run on top of JAX.\n",
    "* **`dlfb` (Deep Learning for Biology):** A custom companion library provided with the book to handle common utilities and repetitive tasks (https://github.com/deep-learning-for-biology/dlfb.git)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73732ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b40721",
   "metadata": {},
   "source": [
    "### Why Use JAX and Flax for Biology?\n",
    "\n",
    "* **Familiarity:** JAX uses the `jax.numpy` ($jnp$) API, which is almost identical to standard NumPy, making the transition seamless for those already doing scientific computing in Python.\n",
    "* **Functional Clarity:** JAX follows a \"pure function\" style. This explicit approach reduces hidden states, making the underlying math of biological models easier to understand and debug.\n",
    "* **First-Class Transformations:** JAX offers powerful, composable tools:\n",
    "    * `jit`: Just-In-Time compilation via the XLA (Accelerated Linear Algebra) compiler for speed.\n",
    "    * `grad`: Automatic differentiation for calculating gradients.\n",
    "    * `vmap`: Automatic vectorization to handle batches of data (like thousands of protein sequences) without manual loops.\n",
    "* **Research Alignment:** JAX is the preferred tool for modern \"AI for Science\" research, including major breakthroughs like AlphaFold.\n",
    "\n",
    "#### Trade-offs and Considerations\n",
    "\n",
    "* **Learning Curve:** JAX requires a shift toward functional programming, which may feel different than the object-oriented approach of PyTorch.\n",
    "* **Ecosystem Size:** The JAX community is smaller than PyTorch's, and APIs (like the shift from Flax `linen` to the newer `nnx`) can evolve quickly.\n",
    "* **Framework Interoperability:** The book occasionally uses **PyTorch** (e.g., for Hugging Face model embeddings) because certain tools are more mature in that ecosystem.\n",
    "\n",
    "#### Advanced Performance Optimization\n",
    "\n",
    "While the book focuses on clarity, it identifies four key areas for scaling real-world biological models:\n",
    "\n",
    "* **Numerical Precision:** Using formats like $bfloat16$ to speed up matrix multiplications on specialized hardware (Tensor Cores).\n",
    "* **Profiling:** Using tools like `jax.profiler` to identify computational and memory bottlenecks.\n",
    "* **Memory Efficiency:** Using **gradient checkpointing** (`remat`) to train deeper models by trading computation for memory.\n",
    "* **Distributed Training:** Scaling models across multiple GPUs or TPUs for massive datasets.\n",
    "\n",
    "**Key Takeaway:** Choosing JAX and Flax aligns your work with the \"bleeding edge\" of biological research while providing a transparent, mathematically grounded framework for learning.\n",
    "\n",
    "For those seeking a deeper technical dive or troubleshooting support, the text recommends two specific JAX resources:\n",
    "* **Official JAX Tutorials:** The primary source for detailed, hands-on learning and practical application of the framework (https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html).\n",
    "* **The \"Sharp Bits\" Notebook:** An essential reference guide that documents common pitfalls and non-intuitive behaviors unique to JAX's functional programming model (https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10651a41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8475e",
   "metadata": {},
   "source": [
    "### Python tips\n",
    "\n",
    "This section covers essential Python concepts frequently encountered in machine learning code, particularly with JAX and Flax frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638770f6",
   "metadata": {},
   "source": [
    "#### 1. Type Annotations and Docstrings\n",
    "\n",
    "Python is dynamically typed, which is flexible but can hide bugs. Type annotations improve readability, enable static type checking (mypy) or Vs Code's Pylance, and simplify debugging.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Basic function without type hints\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "# Improved function with type hints and docstring\n",
    "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between two NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground-truth values.\n",
    "        y_pred (np.ndarray): Predicted values.\n",
    "    Return:\n",
    "        float: The mean squared error.\n",
    "    \"\"\"\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Clarifies input/output types\n",
    "- Enhances IDE documentation and autocomplete\n",
    "- Improves code readability\n",
    "- Enables static type checking with tools like mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c469c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between two NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground-truth values.\n",
    "        y_pred (np.ndarray): Predicted values.\n",
    "    Return:\n",
    "        float: The mean squared error.\n",
    "    \"\"\"\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    return np.mean(squared_errors)\n",
    "\n",
    "# How to print the docstring of the function \n",
    "help(mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761aa555",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54eae6",
   "metadata": {},
   "source": [
    "#### 2. Decorators\n",
    "\n",
    "Decorators are functions that modify the behavior of other functions, commonly used for performance enhancement, caching, or logging.\n",
    "\n",
    "For example, JIT compilation with JAX (`@jax.jit`) is a decorator (see codes in below cell)\n",
    "\n",
    "**How `@jax.jit` works:**\n",
    "1. Traces the function using special tracer objects (not real data)\n",
    "2. Builds a computation graph (static representation of operations)\n",
    "3. Compiles via XLA (Accelerated Linear Algebra) to optimized machine code\n",
    "4. Caches compiled version for reuse with same input shapes/types\n",
    "5. Results in ~20x speedup on GPU\n",
    "\n",
    "**JIT Debugging Challenges:**\n",
    "- `print()` statements and `pdb` don't work as expected\n",
    "- Side effects are skipped during tracing\n",
    "- Cryptic error messages referencing internal JAX/XLA code\n",
    "\n",
    "**Solution**: Set environment variable `JAX_DISABLE_JIT=True` to globally disable JIT for debugging or you may set directly in your Python code:\n",
    "\n",
    "```python\n",
    "import jax\n",
    "jax.config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "def f(x):\n",
    "    y = jnp.log(x)\n",
    "    if jnp.isnan(y):\n",
    "        breakpoint()\n",
    "    return y\n",
    "\n",
    "jax.jit(f)(-2.)  # ==> Enters PDB breakpoint!\n",
    "\n",
    "```\n",
    "\n",
    "**Strengths and limitations of `jax_disable_jit`**\n",
    "* **Strengths:**\n",
    "    * Easy to apply\n",
    "    * Enables use of Python’s built-in `breakpoint` and `print`\n",
    "    * Throws standard Python exceptions and is compatible with PDB postmortem\n",
    "* **Limitations:**\n",
    "    * Running functions without JIT-compilation can be slow\n",
    "\n",
    "See the [JAX debugging documentation](https://docs.jax.dev/en/latest/debugging/flags.html#jax-disable-jit-configuration-option-and-context-manager) for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Basic function\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n",
    "    return jnp.sum(arr**10)\n",
    "# No JIT compilation\n",
    "%time print(f'No JIT output: {compute_ten_power_sum(arr)}')\n",
    "\n",
    "# Method 1 - Apply JIT directly\n",
    "jitted_compute_ten_power_sum = jax.jit(compute_ten_power_sum)\n",
    "# first call (compilation time) takes longer\n",
    "%time print(f'Jitted function output: {jitted_compute_ten_power_sum(arr)}')\n",
    "\n",
    "# Method 2 - Use decorator syntax\n",
    "@jax.jit\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Raise values to the power of 10 and then sum.\"\"\"\n",
    "    return jnp.sum(arr**10)\n",
    "# first call (compilation time) takes longer\n",
    "%time print(f'Decorator function output: {compute_ten_power_sum(arr)}')\n",
    "%time print(f'Subsequent call: {compute_ten_power_sum(arr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_ten_power_sum(arr: jax.Array) -> float:\n",
    "    \"\"\"Computes the sum of 10 raised to the power of each element in the input array.\"\"\"\n",
    "    return jnp.sum(arr ** 10)\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "%time print(compute_ten_power_sum(arr)) # first call (compilation time) takes longer\n",
    "%time print(compute_ten_power_sum(arr)) # subsequent calls are faster\n",
    "\n",
    "arr = jnp.array([5, 4, 3, 2, 1])\n",
    "%time print(compute_ten_power_sum(arr)) # if array shape/dtype is the same, no recompilation\n",
    "\n",
    "arr = jnp.array([5, 4, 3, 2])\n",
    "%time print(compute_ten_power_sum(arr)) # different shape, triggers recompilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30170cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import pdb\n",
    "\n",
    "jax.config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "def f(x):\n",
    "    y = jnp.log(x)\n",
    "    if jnp.isnan(y):\n",
    "        pdb.set_trace()\n",
    "    return y\n",
    "\n",
    "jax.jit(f)(-2.)  # ==> Enters PDB breakpoint!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc684ac",
   "metadata": {},
   "source": [
    "#### Bonus: More about JIT compilation\n",
    "\n",
    "The term **\"Just-in-Time\" (JIT)** refers to the exact moment the compilation happens. In traditional programming languages (like C++ or Fortran), compilation happens before you ever run the program. In JAX, the compilation happens *while the program is running*, specifically the very first time a function is called.\n",
    "\n",
    "Here is a breakdown of why this distinction matters and how it works:\n",
    "\n",
    "##### 1. The Timing: A \"Late\" Compilation\n",
    "\n",
    "In a standard \"Ahead-of-Time\" (AOT) workflow, you compile your code into a binary file, and then you run that file. In JAX, you provide a Python function, and the \"Just-in-Time\" compiler stays idle until you actually trigger that function with real data.\n",
    "* **Step 1:** You define the function.\n",
    "* **Step 2:** You call the function with an input of a specific shape (e.g., a protein sequence of length $L = 500$).\n",
    "* **Step 3 (The \"Just-in-Time\" part):** JAX realizes it doesn't have a compiled version for that specific input shape yet. It pauses, converts the Python code into an optimized XLA kernel, and then executes it.\n",
    "\n",
    "##### 2. Tracing:\n",
    "\n",
    "The reason JAX waits until the \"last second\" (Just-in-Time) is because it needs to see the **shapes** and **types** of your data to optimize effectively. This process is called **Tracing**.\n",
    "\n",
    "When you call a JIT-ed function, JAX sends \"abstract\" versions of your data through the function to see what happens. It records every operation (+, −, ×, ÷) to create a **StableHLO** (a high-level intermediate representation). By waiting until you provide data, JIT can:\n",
    "* See that your matrix is $1000 \\times 1000$.\n",
    "* Optimize the machine code specifically for those dimensions.\n",
    "\n",
    "##### 3. Specialization\n",
    "\n",
    "If you call the same function later with a *different* shape (e.g., a sequence of length $L=200$), JAX will compile it again, \"Just-in-Time\" for that new shape. It builds a library of specialized versions of your function in the background.\n",
    "\n",
    "##### Summary of Comparison of different compilation methods\n",
    "\n",
    "| Feature | Interpreted (Python/NumPy) | Ahead-of-Time (C++/Fortran) | Just-in-Time (JAX/XLA) |\n",
    "| --- | --- | --- | --- |\n",
    "| **When is it compiled?** | Never (translated line-by-line) | Before the program runs | During execution (on first call) |\n",
    "| **Performance** | Slow (High overhead) | Very Fast | Very Fast |\n",
    "| **Flexibility** | High | Low (must re-compile manually) | High (auto-specializes to shapes) |\n",
    "\n",
    "##### Why this is a \"Scientific\" Advantage\n",
    "\n",
    "In biological modeling, we often deal with variable-sized inputs (different DNA lengths, different number of atoms in a molecule). JIT allows us to write flexible Python code that feels \"easy,\" while the compiler works \"Just-in-Time\" to give us the speed of a low-level language like C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149ad99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc076a1",
   "metadata": {},
   "source": [
    "#### 3. Preconfiguring JAX JIT with `partial`\n",
    "\n",
    "`functools.partial` prefills/binds arguments to create new functions with fixed values, a general utility in Python.\n",
    "\n",
    "**Basic example:**\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "\n",
    "def scale(x, scaling_factor):\n",
    "    return x * scaling_factor\n",
    "\n",
    "# Create new function with scaling_factor fixed to 10\n",
    "scale_by_10 = partial(scale, scaling_factor=10)\n",
    "scale_by_10(3)\n",
    "# Output: 30\n",
    "\n",
    "```\n",
    "\n",
    "Here, `scale_by_10` is a new function that behaves like `scale(x, 10)`.\n",
    "\n",
    "\n",
    "**JAX-specific usage with static arguments:**\n",
    "\n",
    "In the context of JAX, `partial` is often used to customize a decorator before applying it, like this: `@partial(jax.jit, static_argnums=...)`. This is a way to configure the `jax.jit` decorator itself.\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def summarize(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported average type: {average_method}\")\n",
    "\n",
    "data_array = jnp.array([1.0, 2.0, 100.0])\n",
    "\n",
    "# JAX compiles one version for average_method=\"mean\"\n",
    "print(f\"Mean: {summarize('mean', data_array)}\")\n",
    "\n",
    "# JAX compiles another version for average_method=\"median\"\n",
    "print(f\"Median: {summarize('median', data_array)}\")\n",
    "\n",
    "# Calling with \"mean\" again uses cached compiled version\n",
    "print(f\"Mean again: {summarize('mean', data_array)}\")\n",
    "\n",
    "# Output:\n",
    "# Mean: 34.333335876464844\n",
    "# Median: 2.0\n",
    "# Mean again: 34.333335876464844\n",
    "```\n",
    "\n",
    "If we didn’t mark `average` as static with `static_argnums=(0,)`, JAX would throw an error, because it can’t trace control flow that depends on strings unless it knows their value ahead of time. Marking arguments as static tells JAX to compile a separate, specialized version of the function for each unique value of that static argument it encounters.\n",
    "\n",
    "**Static vs Dynamic arguments:**\n",
    "\n",
    "* **Dynamic**: Numerical inputs (`jax.Array`, `float`, `int`) - can vary without recompilation if shapes/types remain constant.\n",
    "* **Static**: Strings, Python objects, functions - affect control flow; must mark with `static_argnums` or `static_argnames` or use closures (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae83a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_disable_jit\", False)\n",
    "\n",
    "# @partial(jax.jit, static_argnums=(0,)) # using deprecated static_argnums\n",
    "@partial(jax.jit, static_argnames=(\"average_method\",)) # using static_argnames\n",
    "def summarize(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown average method: {average_method}\")\n",
    "    \n",
    "arr = jnp.array([1, 2, 3, 4, 5])\n",
    "%time print(summarize(\"mean\", arr))  # JIT compilation for \"mean\"\n",
    "%time print(summarize(\"mean\", arr))  # Subsequent call for \"mean\"\n",
    "%time print(summarize(\"median\", arr))  # JIT compilation for \"median\"\n",
    "%time print(summarize(\"median\", arr))  # Subsequent call for \"median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def summarize_2(average_method: str, x: jax.Array) -> float:\n",
    "    if average_method == \"mean\":\n",
    "        return jnp.mean(x)\n",
    "    elif average_method == \"median\":\n",
    "        return jnp.median(x)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown average method: {average_method}\")\n",
    "\n",
    "arr = jnp.array([1, 2, 3, 4, 20])\n",
    "%time print(summarize_2(\"mean\", arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9147fa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4afd9",
   "metadata": {},
   "source": [
    "#### 4. Closures\n",
    "\n",
    "In Python, a **closure** is a function object that \"remembers\" values in the enclosing scope even if they are no longer present in memory.\n",
    "\n",
    "For a closure to exist, three conditions must be met:\n",
    "1. There must be a **nested function** (a function inside a function).\n",
    "2. The nested function must refer to a value defined in the **enclosing function**.\n",
    "3. The enclosing function must return the nested function.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "def outer_function(x):\n",
    "    def inner_function(y):\n",
    "        return x + y  # inner_function \"closes over\" x\n",
    "    return inner_function\n",
    "\n",
    "add_five = outer_function(5)  # x is 5\n",
    "result = add_five(10)  # y is 10\n",
    "print(f\"Closure result: {result}\")\n",
    "# Output: Closure result: 15\n",
    "```\n",
    "\n",
    "#### The `nonlocal` Keyword \n",
    "\n",
    "By default, a closure can read the outer variable but cannot modify it. If you want to change a variable in the enclosing scope, you must use the `nonlocal` keyword. This is common for creating \"counters\" or \"accumulators.\"\n",
    "\n",
    "```python\n",
    "def make_counter():\n",
    "    count = 0\n",
    "    def increment():\n",
    "        nonlocal count  # Allows modification of the outer 'count'\n",
    "        count += 1\n",
    "        return count\n",
    "    \n",
    "    return increment\n",
    "\n",
    "counter_a = make_counter()\n",
    "print(counter_a())  # Output: 1\n",
    "print(counter_a())  # Output: 2\n",
    "\n",
    "counter_b = make_counter()\n",
    "print(counter_b())  # Output: 1 (Starts its own separate count)\n",
    "\n",
    "```\n",
    "\n",
    "#### Why use Closures?\n",
    "\n",
    "In machine learning (and especially in JAX), closures are powerful for:\n",
    "\n",
    "* **Data Hiding:** They provide a way to store state without using a full Class object.\n",
    "* **Function Factories:** You can generate specialized versions of a function (like a specific loss function with fixed hyperparameters).\n",
    "* **Decorators:** Closures are the underlying mechanism that makes Python decorators work.\n",
    "\n",
    "### Comparison to Classes\n",
    "\n",
    "If you only have one method in a class, a closure is often a more elegant, lightweight, and memory-efficient solution.\n",
    "\n",
    "| Feature | Closure | Class |\n",
    "| --- | --- | --- |\n",
    "| **Setup** | Lightweight (function) | Heavier (object + methods) |\n",
    "| **State** | Fixed via \"backpack\" | Mutable via `self` |\n",
    "| **Usage** | Functional programming | Object-Oriented programming |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 \n",
    "def outer_function(x: float):\n",
    "    def inner_function(y: float):\n",
    "        return y + x\n",
    "    return inner_function\n",
    "\n",
    "\n",
    "add_five = outer_function(5.0)\n",
    "print(add_five(3.0))  # Outputs 8.0\n",
    "print(add_five.__closure__[0].cell_contents)  # Inspect closure to see captured variables\n",
    "print(add_five)\n",
    "\n",
    "\n",
    "# Example 2\n",
    "def make_counter():\n",
    "    count = 0\n",
    "    def counter():\n",
    "        nonlocal count\n",
    "        count += 1\n",
    "        return count\n",
    "    return counter\n",
    "counter = make_counter()\n",
    "print(counter())  # Outputs 1\n",
    "print(counter())  # Outputs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1611cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10743417",
   "metadata": {},
   "source": [
    "#### 5. Generators\n",
    "\n",
    "Iterates over data lazily (one item at a time) - essential for large datasets that don't fit in memory.\n",
    "\n",
    "**Simple generator:**\n",
    "\n",
    "```python\n",
    "from typing import Iterator\n",
    "\n",
    "def data_generator() -> Iterator[dict]:\n",
    "    \"\"\"Yield data samples with features and labels.\"\"\"\n",
    "    for i in range(5):\n",
    "        yield {\"feature\": i, \"label\": i % 2}\n",
    "\n",
    "# Example usage\n",
    "generator = data_generator()\n",
    "next(generator)\n",
    "# Output: {'feature': 0, 'label': 0}\n",
    "```\n",
    "\n",
    "**Integration with TensorFlow Datasets (TFDS):**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "features = np.array([1, 2, 3, 4, 5])\n",
    "labels = np.array([0, 1, 0, 1, 0])\n",
    "\n",
    "# Create TensorFlow dataset from NumPy arrays\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Batch with size 2, drop incomplete final batch\n",
    "batched_dataset = dataset.batch(2, drop_remainder=True)\n",
    "\n",
    "# Create iterator and retrieve first batch\n",
    "ds = iter(batched_dataset)\n",
    "next(ds)\n",
    "# Output:\n",
    "# (<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>,\n",
    "#  <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n",
    "```\n",
    "\n",
    "**Why TFDS with JAX?**\n",
    "- JAX lacks native data-loading library\n",
    "- TFDS provides clean API for batching, shuffling, and prefetching\n",
    "- Custom pipelines offer more control (covered in later chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "def data_generator() -> Iterator[dict]:\n",
    "    \"\"\"Yield data samples with features and labels.\"\"\"\n",
    "    for i in range(5):\n",
    "        yield {\"feature\": i, \"label\": i % 2}\n",
    "\n",
    "# Example usage\n",
    "generator = data_generator()\n",
    "print(next(generator))\n",
    "# Output: {'feature': 0, 'label': 0}\n",
    "print(next(generator))\n",
    "# Output: {'feature': 1, 'label': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead94edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import jax.numpy as jnp\n",
    "\n",
    "features = jnp.array([1, 2, 3, 4, 5])\n",
    "labels = jnp.array([0, 0, 1 , 1, 0])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "batched_dataset = dataset.batch(2, drop_remainder=True)\n",
    "\n",
    "ds = iter(batched_dataset)\n",
    "try:\n",
    "    print(next(ds))\n",
    "    print(next(ds))\n",
    "    print(next(ds))\n",
    "except StopIteration:\n",
    "    print(\"End of dataset reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945db33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee5c1d",
   "metadata": {},
   "source": [
    "### Anatomy of a Training Loop with JAX/Flax\n",
    "\n",
    "The core structure of training a model remains fairly consistent in machine learning projects. Here are the list of core steps when you are trainining a model:\n",
    "1. Defining a dataset\n",
    "2. Defining a model\n",
    "3. Creating a training state\n",
    "4. Defining a loss function\n",
    "5. Defining the training step\n",
    "6. Handling auxilary outputs in the loss function\n",
    "7. Defining the training loop\n",
    "\n",
    "In the following section, we will go through each step with a working example:\n",
    "\n",
    "#### 1. Defining a dataset\n",
    "\n",
    "Let's create a dataset with a linear relationship between the feature $x$ and the label $y$. We will use JAX random generator to add noise to the data.\n",
    "\n",
    "**Note on randomness in JAX:** In JAX, randomness is handled differently than in standard NumPy or PyTorch. Because JAX is functional and deterministic, it uses *Explicit Pseudo-Random Number Generation (PRNG)*. This means you must manually manage and \"pass\" the state of the random number generator.\n",
    "\n",
    "* **Initializing the Key:**\n",
    "\n",
    "    ```python\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "\n",
    "    ```\n",
    "    Above code creates a \"seed\" or a master key. In JAX, the `rng` (Random Number Generator) state is an array of *two integers*. Unlike `np.random.seed()`, which sets a global hidden state, JAX requires this explicit key to ensure that if you run the same code twice, you get the exact same results (reproducibility).\n",
    "\n",
    "* **Splitting the Key:**\n",
    "\n",
    "    ```python\n",
    "    rng, rng_data, rng_noise = jax.random.split(rng, 3)\n",
    "\n",
    "    ```\n",
    "\n",
    "    The above code \"splits\" the master key into three new, independent sub-keys. This is the most important rule in JAX: *Never reuse a key.* If you used the same `rng` to generate both your data and your noise, they would be correlated. The outputs of `split` in above code are:\n",
    "    * `rng`: A new \"lead\" key to be used for future splits.\n",
    "    * `rng_data`: A key specifically for generating the $x$ values.\n",
    "    * `rng_noise`: A key reserved for generating noise to be added to the label.\n",
    "\n",
    "In NumPy, the state is updated behind the scenes (mutated). In JAX, the state is passed explicitly (functional).\n",
    "\n",
    "| Step | NumPy (Implicit) | JAX (Explicit) |\n",
    "| --- | --- | --- |\n",
    "| **Initialization** | `np.random.seed(42)` | `key = jax.random.PRNGKey(42)` |\n",
    "| **State Update** | Automatic | `key, subkey = jax.random.split(key, 2)` |\n",
    "| **Generation** | `np.random.uniform()` | `jax.random.uniform(subkey)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In JAX, randomness is handled explicitly by passing a random key.\n",
    "# We create a key here to seed the random number generator.\n",
    "rng = jax.random.PRNGKey(42)\n",
    "print(f\"Master key: {rng}\")\n",
    "rng, rng_data, rng_noise = jax.random.split(rng, 3)\n",
    "print(f\"Master sub-key: {rng}\")\n",
    "print(f\"Data key: {rng_data}\")\n",
    "print(f\"Noise key: {rng_noise}\")\n",
    "\n",
    "# Generate toy data: x values uniformly sampled between 0 and 1.\n",
    "x_data = jax.random.uniform(rng_data, shape=(100, 1), minval=0.0, maxval=1.0) # Default range is [0,1]\n",
    "\n",
    "# Generate Gaussian noise to be added to the data.\n",
    "noise = jax.random.normal(rng_noise, shape=(100, 1)) * 0.1\n",
    "\n",
    "# Define label (target): y = 2 * x + 1 + noise.\n",
    "y_data = 2 * x_data + 1 + noise\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(x_data, y_data, alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Toy dataset: y = 2x +1 + noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c7f4b",
   "metadata": {},
   "source": [
    "#### 2. Defining a model\n",
    "\n",
    "This section explains how to define and initialize a model using *Flax*, highlighting the library's functional design and its unique approach to parameter management.\n",
    "\n",
    "* **Flax Modules**: Models are created by inheriting from `nn.Module`. Unlike PyTorch, these modules are stateless blueprints; they do not store the weights internally.\n",
    "* **The `@nn.compact` Decorator**: This allows for \"inline\" layer definition. We can declare and use layers (like `nn.Dense`) directly within the `__call__` method, simplifying the code for sequential architectures. In the explicit style (Without `nn.compact` decorator), we define our sub-layers in `setup()` and then simply call them in `__call__()`. When the model is initialized in the dry-run, `setup()` is called first then `__call__()` is called.\n",
    "\n",
    "    ```python\n",
    "    class LinearModel(nn.Module):\n",
    "        def setup(self):\n",
    "            \"\"\"Initializes the layers of the model explicitly.\n",
    "            \n",
    "            This method is called internally by Flax when the model is \n",
    "            initialized or applied for the first time.\n",
    "            \"\"\"\n",
    "            # We define the layer as a class attribute here.\n",
    "            # Note: We still don't define the input shape; \n",
    "            # Flax handles that during model.init()\n",
    "            self.dense = nn.Dense(features=1)\n",
    "\n",
    "        def __call__(self, x):\n",
    "            \"\"\"Defines the forward pass using pre-defined layers.\n",
    "            \n",
    "            Args:\n",
    "                x: Input data array.\n",
    "                \n",
    "            Returns:\n",
    "                The output of the dense layer.\n",
    "            \"\"\"\n",
    "            # We simply reference the attribute defined in setup.\n",
    "            return self.dense(x)\n",
    "        \n",
    "    ```\n",
    "\n",
    "\n",
    "* **Lazy Shape Inference**: Flax doesn't know the dimensions of your weights (the `kernel` and `bias`) until you provide a sample input. This \"just-in-time\" shape inference provides more control and clarity during JAX transformations.\n",
    "* **Parameter Initialization**: Using `model.init()`, Flax generates a nested dictionary (often called `variables` or `params`) containing the actual numerical arrays for the weights.\n",
    "\n",
    "##### Comparison: Flax vs. Object-Oriented Frameworks\n",
    "\n",
    "| Feature | Flax (Functional) | PyTorch/Keras (OO) |\n",
    "| --- | --- | --- |\n",
    "| **State Storage** | Parameters stored in a separate dict | Parameters stored inside the Layer object |\n",
    "| **Shape Definition** | Inferred during `.init()` call | Usually defined during instantiation |\n",
    "| **Architecture** | Explicit and JIT-friendly | Automatic and state-heavy |\n",
    "\n",
    "Here’s a minimal example, a single linear (dense) layer with one output unit and no activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "# 1. Define the model architecture.\n",
    "class LinearModel(nn.Module):\n",
    "    # The @nn.compact decorator allows you to define parameters \n",
    "    # (like nn.Dense) inside the __call__ method.\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Creates a dense layer with 1 output unit.\n",
    "        # Computes y = xW + b. \n",
    "        # On the first call, it uses 'x' to figure out the input shape.\n",
    "        return nn.Dense(features=1)(x)\n",
    "\n",
    "# 2. Instantiate the blueprint.\n",
    "# This doesn't create weights yet; it just creates the model structure.\n",
    "model = LinearModel()\n",
    "\n",
    "# 3. Setup Randomness.\n",
    "# JAX requires explicit PRNG keys for initialization.\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# 4. Initialize Parameters (The \"Dry Run\").\n",
    "# .init() calls __call__ with dummy data to determine weight shapes.\n",
    "# variables will contain the actual weight/bias arrays.\n",
    "variables = model.init(rng, jnp.ones([1, 1]))\n",
    "\n",
    "# 5. Inspect the State\n",
    "# This prints the parameter dictionary: Kernel and bias .\n",
    "print(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5fe71",
   "metadata": {},
   "source": [
    "Here:\n",
    "* `kernel` is the learned weight matrix (shape [1, 1], since our input and output dimensions are both 1).\n",
    "* `bias` is the learned bias term added after the matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8593ff",
   "metadata": {},
   "source": [
    "#### 3. Creating a training state\n",
    "\n",
    "This section introduces the *TrainState*, a crucial Flax utility that centralizes the components required for a training loop into a single, immutable container. Here are key takeaways:\n",
    "\n",
    "* **Centralized Container**: `TrainState` bundles the *model parameters*, the *optimizer*, and the *forward pass function* (`apply_fn`) into one object.\n",
    "* **The Optimizer (Tx)**: Uses the *Optax* library, where optimizers are treated as \"gradient transformations.\"\n",
    "* **Functional Immutability**: Following JAX's core principles, the `TrainState` is *immutable*. Updating the model does not change the state in place; instead, it returns a new state object with updated parameters.\n",
    "* **Memory Efficiency**: Despite creating new objects for each update, JAX’s XLA compiler manages memory efficiently, reusing buffers under the hood to prevent overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# 1. Define an optimizer — here we use Adam with a learning rate of 0.1.\n",
    "# (Note: in most real settings you'd use a smaller learning rate like 1e-3).\n",
    "tx = optax.adam(1.0)\n",
    "\n",
    "# 2. Create the training state.\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,       # The model's forward pass function.\n",
    "    params=variables[\"params\"], # The initialized model parameters.\n",
    "    tx=tx,                      # The optimizer.\n",
    ")\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd7ea9",
   "metadata": {},
   "source": [
    "#### 4. Defining a loss function\n",
    "\n",
    "This section describes the creation of a *Loss Function*, specifically *Mean Squared Error (MSE)*, which serves as the objective for the optimization process. It highlights how JAX's functional paradigm influences the way model logic and parameters interact.\n",
    "\n",
    "Here is how cost function would be defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(params, x, y):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) loss.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of model parameters (weights and biases).\n",
    "        x: Input features of shape (batch_size, input_dim).\n",
    "        y: Target labels of shape (batch_size, output_dim).\n",
    "    \n",
    "    Returns:\n",
    "        float: The mean squared error loss value.\n",
    "    \"\"\"\n",
    "    # Run a forward pass of the model to get predictions.\n",
    "    # We pass the stateless model the params it needs for this specific computation.\n",
    "    predictions = model.apply({\"params\": params}, x)\n",
    "    \n",
    "    # Compute MSE loss: Mean((pred - target)^2)\n",
    "    # This results in a single scalar value that JAX can differentiate.\n",
    "    return jnp.mean((predictions - y) ** 2)\n",
    "\n",
    "# Calculate initial loss using the random parameters from our earlier initialization\n",
    "loss = calculate_loss(variables[\"params\"], x_data, y_data)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "# # calculate_loss performs the following calculations   \n",
    "# w = variables[\"params\"]['Dense_0']['kernel']\n",
    "# b = variables[\"params\"]['Dense_0']['bias']\n",
    "# pred = w * x_data + b\n",
    "# jnp.mean((pred - y_data) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d247794",
   "metadata": {},
   "source": [
    "Here are the key takeaways:\n",
    "* **Objective Measurement**: The loss function quantifies the discrepancy between the model's predictions and the ground-truth targets.\n",
    "* **Functional Purity**: Even though the function references the `model` defined in the outer scope, it remains \"pure.\" This is because the model logic is constant, while all variable state (the weights) is explicitly passed in via the `params` argument.\n",
    "* **Forward Pass**: The `model.apply` method is used inside the loss function to map inputs $x$ to predictions $\\hat y$ using the provided parameters $\\theta$.\n",
    "* **Starting Point**: Evaluating the loss with initial (random) parameters provides a baseline. In the above example, the loss starts at approximately 5.2768, and the goal of training is to minimize this scalar value.\n",
    "\n",
    "#### Why this is the \"JAX Way\"\n",
    "\n",
    "You'll note that the `calculate_loss` function is designed to be wrapped by `jax.grad`. JAX requires that the **first argument** of the function being differentiated is the variable you want to update (in this case, `params`). By placing `params` first, you allow JAX to compute the gradient of the loss with respect to every weight in your neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e22235",
   "metadata": {},
   "source": [
    "#### 5. Defining the Training Step\n",
    "\n",
    "This section defines the **Training Step**, the core loop iteration where the model actually learns. It integrates JAX's transformation power (JIT and grad) with Flax's `TrainState` to create an optimized update mechanism.\n",
    "\n",
    "Here are the key takeaways:\n",
    "\n",
    "* **Compilation with `jax.jit`:** Wrapping the training step in `@jax.jit` compiles the entire operation into a single optimized XLA graph. This is essential for high performance on GPUs and TPUs.\n",
    "* **Efficient Differentiation**: `jax.value_and_grad` is used to return both the *scalar loss* (for logging/monitoring) and the *gradients* (for updating weights) in a single pass, avoiding redundant calculations.\n",
    "* **The Closure Pattern**: Defining the loss function *inside* the training step allows it to \"close over\" the `state`, `x`, and `y` variables. This simplifies the function signature, as `jax.grad` only needs to focus on the parameters.\n",
    "* **State Update**: `state.apply_gradients(grads=grads)` produces a new, updated `TrainState` containing the new parameters and the updated optimizer state.\n",
    "\n",
    "We cabn implement a training step in two ways:\n",
    "\n",
    "* **Version 1: Direct Argument Passing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_v1(state, x, y):\n",
    "    \"\"\"Perform a single training step (direct-argument version).\n",
    "\n",
    "    Computes the mean-squared error loss and its gradients with respect to the\n",
    "    model parameters, applies optimizer updates, and returns the new training\n",
    "    state and the scalar loss.\n",
    "\n",
    "    Args:\n",
    "        state: A flax.training.train_state.TrainState containing params, apply_fn, and optimizer state.\n",
    "        x: Input batch (jax.Array) with shape (batch_size, ...).\n",
    "        y: Target batch (jax.Array) with shape (batch_size, ...).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[new_state, loss]: \n",
    "            new_state: Updated TrainState after applying gradients.\n",
    "            loss: Scalar loss value (jax.Array) for the provided batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the loss and its gradients with respect to the parameters.\n",
    "    # jax.value_and_grad(compute_loss) expects (params, x, y)\n",
    "    loss, grads = jax.value_and_grad(calculate_loss)(state.params, x, y)\n",
    "    \n",
    "    # Apply gradient updates and return the new immutable state.\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    return new_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba4063",
   "metadata": {},
   "source": [
    "* **Version 2: Using a Closure (Recommended)**\n",
    "\n",
    "    This version is more common in JAX/Flax development as it keeps the differentiation logic focused strictly on the `params`. By using the **Closure Pattern** inside a JIT-compiled function, you ensure that the XLA compiler can see the entire computation at once. This allows it to perform *buffer assignment optimization*, effectively making the \"immutable\" update as fast as an in-place  pointer update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_v2(state, x, y):\n",
    "    \"\"\"\n",
    "    Perform a single training step using a closure over `state`, `x`, and `y`.\n",
    "\n",
    "    Args:\n",
    "        state (flax.training.train_state.TrainState): current training state (params, apply_fn, optimizer).\n",
    "        x (jax.Array): input batch.\n",
    "        y (jax.Array): target batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[new_state, loss]:\n",
    "            new_state: Updated TrainState after applying gradients.\n",
    "            loss: Scalar MSE loss for the batch (jax.Array).\n",
    "\n",
    "    Notes:\n",
    "        - The inner function `calculate_loss(params)` closes over state, x, and y so that\n",
    "          jax.value_and_grad only differentiates w.r.t. `params`.\n",
    "        - The function is JIT-compiled for performance.\n",
    "    \"\"\"\n",
    "    def calculate_loss(params):\n",
    "        # Accesses state, x and y directly.\n",
    "        predictions = state.apply_fn({\"params\": params}, x)\n",
    "        return jnp.mean((predictions - y) ** 2)\n",
    "\n",
    "    # Calculate loss and gradients with respect to params.\n",
    "    loss, grads = jax.value_and_grad(calculate_loss)(state.params)\n",
    "    \n",
    "    # Update the state with the calculated gradients.\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173eab2",
   "metadata": {},
   "source": [
    "#### 6. Handling auxiliary outputs in the loss function\n",
    "\n",
    "This section explains how to handle functions that return more than just a single scalar value. In machine learning, we often need the loss function to provide extra data—like predictions or internal metrics—for logging purposes without including those values in the gradient calculation.\n",
    "\n",
    "##### Key Concepts\n",
    "\n",
    "* **Scalar Requirement**: By default, `jax.value_and_grad` requires the target function to return a single scalar. Returning a tuple or a dictionary will result in a `TypeError`.\n",
    "* **The `has_aux` Flag**: By setting `has_aux=True`, you inform JAX that the function returns a tuple: `(loss, auxiliary_data)`.\n",
    "* **Gradient Exclusion**: JAX will only compute gradients for the first element (the loss). The second element (auxiliary data) is passed through unchanged and ignored by the automatic differentiation engine.\n",
    "* **Workflow Efficiency**: This pattern allows you to capture predictions during the forward pass of the training step, saving you from having to run the model a second time just for logging or visualization.\n",
    "\n",
    "Here is the updated training step using the auxiliary output pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, x, y):\n",
    "    \"\"\"Performs a single training step, returning loss and auxiliary predictions.\n",
    "\n",
    "    Args:\n",
    "        state: The current TrainState containing params and optimizer state.\n",
    "        x: Input features of shape (batch_size, features).\n",
    "        y: Target values of shape (batch_size, 1).\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (new_state, (loss, predictions)), where new_state is the \n",
    "        updated TrainState, loss is the scalar MSE, and predictions are the \n",
    "        model outputs from the forward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_loss(params):\n",
    "        \"\"\"Internal loss function that returns auxiliary data.\n",
    "\n",
    "        Args:\n",
    "            params: Model parameters to differentiate against.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of (scalar_loss, predictions).\n",
    "        \"\"\"\n",
    "        predictions = state.apply_fn({\"params\": params}, x)\n",
    "        loss = jnp.mean((predictions - y) ** 2)\n",
    "        return loss, predictions # Return loss and auxiliary information\n",
    "\n",
    "    # Use has_aux=True to handle the extra 'predictions' output\n",
    "    (loss, predictions), grads = jax.value_and_grad(\n",
    "        calculate_loss, \n",
    "        has_aux=True\n",
    "    )(state.params)\n",
    "\n",
    "    # Apply the gradients to the state to get updated parameters\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    return state, (loss, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b85e89",
   "metadata": {},
   "source": [
    "#### 7. Defining the training loop\n",
    "\n",
    "This section describes the final step of the model development process: the **Training Loop**. It ties together the model, dataset, and training step to iteratively optimize the parameters until they converge on the true underlying function.\n",
    "\n",
    "##### Key Concepts\n",
    "\n",
    "* **Step vs. Epoch:**\n",
    "    * **Step:** A single parameter update using one batch of data.\n",
    "    * **Epoch:** A complete pass through the entire dataset. In this specific toy example, since the whole dataset is processed at once, one step equals one epoch.\n",
    "* **Model Convergence:** The process where the loss function decreases and stabilizes, indicating the model has \"learned\" the linear relationship.\n",
    "* **Inference & Validation:** After training, the model is evaluated on new test data (`x_test`) to verify that it generalizes well and isn't just memorizing the training noise.\n",
    "* **Foundational Workflow:** This pattern—*Define $\\rightarrow$ Initialize $\\rightarrow$ Train $\\rightarrow$ Evaluate*—is the blueprint for almost all deep learning projects, regardless of complexity.\n",
    "\n",
    "##### 1. The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c81e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "# Number of full passes through the training data.\n",
    "for epoch in range(num_epochs):\n",
    "    # The train_step returns a NEW state (immutability) and the current loss\n",
    "    state, (loss, _) = train_step(state, x_data, y_data)\n",
    "    \n",
    "    # Log progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a220bd",
   "metadata": {},
   "source": [
    "##### 2. Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0befabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data (x values between 0 and 1).\n",
    "# We use linspace to create a smooth line for the true relationship.\n",
    "x_test = jnp.linspace(0, 1, 10).reshape(-1, 1)\n",
    "y_test = 2 * x_test + 1 # Ground truth: linear function without noise.\n",
    "\n",
    "# Get model predictions using the final trained parameters.\n",
    "# We access the apply_fn and params stored in our TrainState object.\n",
    "y_pred = state.apply_fn({\"params\": state.params}, x_test)\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(x_test, y_test, label=\"True values\")\n",
    "plt.plot(x_test, y_pred, color=\"red\", label=\"Model predictions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Linear Model Predictions vs. True Relationship\")\n",
    "plt.show()\n",
    "\n",
    "# Print the final learned parameters (weights and bias)\n",
    "kernel = state.params['Dense_0']['kernel']\n",
    "bias = state.params['Dense_0']['bias']\n",
    "w = float(kernel[0, 0])\n",
    "b = float(bias[0])\n",
    "print(f\"W: {w:.3f}, b: {b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a49e7",
   "metadata": {},
   "source": [
    "##### Summary of Results\n",
    "\n",
    "The model successfully reduced the loss from an initial 5.2768 down to a stable 0.0100. This indicates that the parameters $W$ and $b$ have been optimized to closely match the original coefficients (2 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debc0b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce139ff",
   "metadata": {},
   "source": [
    "### Bonus: Full Implementation of a Training Loop for a Quadratic Model\n",
    "\n",
    "In the following section, we will build a training loop to train a model for $y = x^2 + 2x + 1 + noise$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba414e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "\n",
    "# 1. Define the train dataset.\n",
    "## Create random key.\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, data_key, noise_key = jax.random.split(key, 3)\n",
    "\n",
    "## Generate x and y data.\n",
    "x_data = jax.random.uniform(shape=(100, 1), minval=-5, maxval=5, key=data_key)\n",
    "noise = jax.random.normal(key=noise_key, shape=(100, 1))\n",
    "y_data = x_data ** 2 + 2 * x_data + 1 + noise\n",
    "\n",
    "## Visualize the dataset\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "## Engineer features to capture quadratic relationship between features and labels\n",
    "X_data = jnp.concatenate([x_data, x_data ** 2], axis=1)\n",
    "\n",
    "\n",
    "# 2. Define a model.\n",
    "## Define a custom class that inherits from nn.Module.\n",
    "class LinearModel(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return nn.Dense(features=1)(x) # one output unit for regression\n",
    "\n",
    "## Instantiate the model.\n",
    "model2 = LinearModel()\n",
    "\n",
    "## Initialize the weights with random numbers\n",
    "rng = jax.random.PRNGKey(42)\n",
    "variables = model2.init(rng, jnp.ones([1,2])) # input shape is (1,2) because we have 2 features (x and x^2)\n",
    "print(f'Initialized weights\\n: {variables}')\n",
    "\n",
    "\n",
    "# 3. Create a training state.\n",
    "## Define the optimizer.\n",
    "tx = optax.adam(learning_rate=0.1)\n",
    "\n",
    "## Create the state.\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model2.apply,          # Forward pass function\n",
    "    params=variables['params'],     # Initialized model parameters\n",
    "    tx=tx                           # optimizer for back prop\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Define a explicit loss function\n",
    "def calculate_loss(params, x, y):\n",
    "\n",
    "    preds = model2.apply({'params': params}, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "loss = calculate_loss(variables['params'], X_data, y_data)\n",
    "print(f'Loss at iteration zero: {loss}')\n",
    "\n",
    "\n",
    "# 5. Define a training step.\n",
    "## Use closure over compute_loss to capture state, x, and y\n",
    "## so that jax.value_and_grad only differentiates w.r.t. params.\n",
    "@jax.jit\n",
    "def train_step(state, x, y):\n",
    "\n",
    "    def compute_loss(params):\n",
    "        preds = state.apply_fn({'params': params}, x)\n",
    "        loss = jnp.mean((preds - y) ** 2)\n",
    "        return loss, preds\n",
    "    \n",
    "    (loss, predictions), grads = jax.value_and_grad(compute_loss, has_aux=True, )(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    return state, (loss, predictions)\n",
    "\n",
    "\n",
    "# 6. Define training loop.\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    state, (loss, _) = train_step(state, X_data, y_data)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "\n",
    "# 7. Test you model\n",
    "## Create test data.\n",
    "x_test = jnp.linspace(-5, 5, 10).reshape(-1, 1)\n",
    "X_test = jnp.concatenate([x_test, x_test**2], axis=1)\n",
    "noise = jax.random.normal(key=noise_key, shape=(10, 1))\n",
    "y_test = x_test ** 2 + 2 * x_test + 1 + noise\n",
    "\n",
    "## Make predictions on the test data\n",
    "y_pred_test = state.apply_fn({'params': state.params}, X_test)\n",
    "\n",
    "## Calculate test loss\n",
    "# train_loss = jnp.mean((y_pred_train - y_data) ** 2)\n",
    "# test_loss = jnp.mean((y_pred_test - y_test) ** 2)\n",
    "train_loss = calculate_loss(state.params, X_data, y_data)\n",
    "test_loss = calculate_loss(state.params, X_test, y_test)\n",
    "print(f'Train loss: {train_loss}')\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "## Visualize the predicted data\n",
    "plt.scatter(x_test, y_test, label='test data', color='blue')\n",
    "plt.plot(x_test, y_pred_test, label='predictions', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638b128",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369e0b6",
   "metadata": {},
   "source": [
    "## Machine Learning Tips\n",
    "\n",
    "This section provides a high-level roadmap of machine learning fundamentals, specifically tailored for the JAX/Flax ecosystem. It covers task types, architectural patterns, data management, and the nuances of neural network optimization.\n",
    "\n",
    "### 1. Types of Tasks and Architectures\n",
    "\n",
    "* **Task Categorization**:\n",
    "    * **Classification**: Binary (A/B), Multiclass (One of many), and Multilabel (Many of many).\n",
    "    * **Regression**: Predicting continuous values (e.g., binding strength).\n",
    "    * **Representation Learning**: Creating embeddings to capture data structure without direct supervision.\n",
    "* **Architectural Toolbox**:\n",
    "    * **(Multilayer Perceptrons) MLPs**: Fully connected layers for vector transformations.\n",
    "    * **(Convolutional Neural Networks) CNNs**: Spatial filters for images and sequences.\n",
    "    * **Transformers**: Attention-based models for long-range dependencies.\n",
    "    * **(Graph Neural Networks) GNNs**: Message-passing for relational/graph data.\n",
    "    * **Autoencoders**: Compression and reconstruction of input for latent space learning.\n",
    "\n",
    ">The current frontier of deep learning is less about inventing entirely new architectures and more about the strategic composition of existing building blocks, specifically by leveraging *inductive biases*. Here are key concepts:\n",
    ">* **Inductive Bias as a Shortcut**: An inductive bias is a set of assumptions about the data's structure that helps a model learn more efficiently. Instead of the model having to learn everything from scratch, these biases narrow the \"search space.\"\n",
    ">    * **Structural Assumptions**: Choosing an architecture should depend on the nature of the data:\n",
    ">    * **Spatial**: For related nearby points (images).\n",
    ">    * **Sequential**: For ordered data (DNA, text).\n",
    ">    * **Relational**: For interacting entities (graphs).\n",
    ">* **Invariance as a Bias**: A specific type of bias where the model is designed to ignore certain changes:\n",
    ">    * **Translation Invariance (CNNs)**: Moving an object within a frame shouldn't change its identity.\n",
    ">    * **Permutation Invariance (GNNs/Transformers)**: Changing the order of the input shouldn't change the result (unless specific sequence information is added back in).\n",
    "\n",
    "\n",
    "### 2. Dataset Management and Hyperparameters\n",
    "\n",
    "* **The Three-Way Split**:\n",
    "    * **Training Set**: Fits the model parameters.\n",
    "    * **Validation Set**: Used for hyperparameter tuning and preventing overfitting.\n",
    "    * **Test Set**: Held out to estimate real-world generalization.\n",
    "* **Hyperparameters**: Constants set before training (Learning rate, Batch size, Model size, Regularization). Evaluation on the validation set ensures the model learns patterns rather than memorizing noise.\n",
    "\n",
    "### 3. Activations Functions\n",
    "\n",
    "Introduce nonlinearity into neural networks, enabling then to model complex relationships between inputs and outputs. The choice of function depends on whether it is being used in a **hidden layer** (to help the model learn) or the **output layer** (to format the prediction). Here are the core activation functions:\n",
    "\n",
    "* **ReLU (Rectified Linear Unit)**:\n",
    "    * **Behavior**: $f(x) = max(0, x)$\n",
    "    * **Usage**: The \"workhorse\" for hidden layers in deep networks because it is computationally efficient and helps mitigate vanishing gradients.\n",
    "* **GELU (Gaussian Error Linear Unit)**:\n",
    "    * **Behavior**: A smoother version of ReLU that weights inputs by their magnitude based on a Gaussian distribution.\n",
    "    * **Usage**: Extremely common in modern *Transformer* architectures; often provides slight performance gains over ReLU.\n",
    "* **Sigmoid & Tanh**:\n",
    "    * **Behavior**: \"Squashing\" functions. Sigmoid maps inputs to $[0, 1]$, while Tanh maps to $[-1, 1]$.\n",
    "    * **Usage**: Primarily used in output layers (e.g., Sigmoid for binary classification). In deep hidden layers, they can cause gradients to \"saturate\" or vanish because of gradients approaches zero at high values.\n",
    "* **Softmax**:\n",
    "    * **Behavior**: Unlike the others, this is not element-wise. It normalizes a vector of values so they sum to 1.\n",
    "    * **Usage**: Exclusively for the final layer of *multiclass classification* to represent a probability distribution.\n",
    "\n",
    "<div style='display: flex; justify-content: center'>\n",
    "        <img src='images/activation_functions.png' width='600px'>\n",
    "</div>\n",
    "\n",
    "### 4.Optimization\n",
    "\n",
    "Optimizers are algorithms responsible for updating a model's weights and biases to minimize the loss function. Optimizers utilize *gradient descent*, a mathematical approach that calculates the direction and magnitude by which each parameter should be adjusted to reduce loss.\n",
    "\n",
    "**The Adam Optimizer** is the preferred choice for most modern deep learning tasks. It is an \"adaptive\" optimizer, meaning it doesn't use a single global learning rate but instead adjusts the rate for each individual parameter. Core Components of Adam are \n",
    "* **Momentum**: Helps the optimizer \"roll\" through flat areas of the loss landscape and speed up training.\n",
    "* **RMSProp**: Scales updates based on recent gradient magnitudes to handle noise.\n",
    "\n",
    "Adam is generally faster and more robust, particularly when dealing with noisy data or sparse gradients.\n",
    "\n",
    "### 5. Initialization Strategy\n",
    "\n",
    "Parameter Initialization in deep learning is critical. Setting the right starting values for weights is not just a formality; it is essential for a stable training process. Proper initialization prevents the \"Vanishing\" or \"Exploding\" gradient problems, where signals become too small to learn from or too large to compute as they pass through deep layers.\n",
    "\n",
    "**Xavier (Glorot) Initialization** is broadly acepted as the standard technique. It scales initial weights based on the number of input and output units to ensure that the variance of activations and gradients remains consistent throughout the network.\n",
    "\n",
    "Xavier Glorot's goal was to make the variance of the weights equal to the harmonic mean of the input and output dimensions:\n",
    "\n",
    "$$Var(W) = \\frac{2}{n_{in} + n_{out}}$$\n",
    "\n",
    "The intuition is that if you have a layer with many inputs, each individual weight needs to be smaller so that the sum of the inputs doesn't blow up. Conversely, if you have many outputs, the gradients coming back will be summed, so the weights must be scaled to prevent the gradient signal from becoming too large.\n",
    "\n",
    "Glorot's parameters are sampled either from a uniform distribution ($U(-a, a)$) or a normal distribution ($N(0, \\sigma^2)$). For uniform distribution we have:\n",
    "\n",
    "$$W \\sim U(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $n_{in}$: The number of input units (fan-in).\n",
    "* $n_{out}$: The number of output units (fan-out).\n",
    "\n",
    "By default, most frameworks (including Flax) use the uniform version.\n",
    "\n",
    "### 6. Model Checkpointing\n",
    "\n",
    "Checkpointing is the process of saving model parameters during training to prevent data loss and enable workflow flexibility.\n",
    "\n",
    "* **Core Purpose:** Enables pausing/resuming training and preserving the final model for future use.\n",
    "* **This Book's Utility:** A simplified, low-boilerplate tool that stores only the most recent state.\n",
    "* **Production Standard:** Recommends `Orbax` for complex needs like versioning or saving \"best-so-far\" models.\n",
    "\n",
    "### 7. Early Stopping\n",
    "\n",
    "Monitoring validation metrics to halt training when improvement plateaus, preventing unnecessary compute and overfitting.\n",
    "\n",
    "Flax provides a dedicated utility to manage training termination based on performance metrics.\n",
    "\n",
    "```python\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "# Initialize the EarlyStopping monitor\n",
    "# patience: Number of steps to wait for improvement before stopping\n",
    "# min_delta: Minimum change to qualify as an improvement\n",
    "early_stop = EarlyStopping(patience=5, min_delta=0.01)\n",
    "\n",
    "# Usage inside a training loop:\n",
    "# Update the monitor with the latest validation metric\n",
    "early_stop = early_stop.update(current_validation_loss)\n",
    "if early_stop.should_stop:\n",
    "    break\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a09a82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf97b7",
   "metadata": {},
   "source": [
    "## Selecting a Working Environment\n",
    "\n",
    "Training large networks effectively requires specialized hardware like GPUs or TPUs. Depending on your resources, there are three primary paths:\n",
    "\n",
    "* **Local Hardware**: Best for full control and avoiding recurring cloud fees; requires upfront investment in a gaming desktop or workstation.\n",
    "* **Cloud Providers (AWS, GCP, Azure)**: Best for scalability and large models; offers high flexibility but can become expensive over time.\n",
    "* **Google Colab**: The ideal starting point for beginners; provides free, zero-setup, cloud-based notebooks with GPU/TPU access.\n",
    "\n",
    "### Structuring Your Code for Reuse and Debugging\n",
    "\n",
    "Modular architecture and rigorous verification strategies are encouraged to transition from interactive notebooks to scalable projects.\n",
    "\n",
    "* **Modular Organization**: Separate code into distinct Python modules (e.g., `datasets.py`, `models.py`, `metrics.py`). This isolation simplifies debugging and enables easy component reuse in future work.\n",
    "* **Dataset Integrity**: Cleanly separated dataset classes allow you to rule out data-loading issues when troubleshooting model failures.\n",
    "* **Sanity Checks**: Validate logic with small-scale tests, such as:\n",
    "    * **Overfitting small data**: Ensuring the model can perfectly memorize  examples.\n",
    "    * **Label Shuffling**: Checking if loss and accuracy react logically to randomized data.\n",
    "* **Visual Debugging**: Use plotting to identify hidden failures—like empty tensors or target shifts—that final performance metrics might mask.\n",
    "\n",
    "#### Recommended Project Structure\n",
    "\n",
    "| Module | Responsibility |\n",
    "| --- | --- |\n",
    "| `data.py` | Loading, preprocessing, and batching logic. |\n",
    "| `model.py` | Architecture definitions (Flax `nn.Module`). |\n",
    "| `train.py` | The training step, loop, and optimizer setup. |\n",
    "| `utils.py` | Metrics calculation and visualization helpers. |\n",
    "\n",
    "### Setting Up a GPU Development Environment\n",
    "\n",
    "To move beyond notebooks for complex models, a robust setup enhances speed, debugging, and reproducibility.\n",
    "\n",
    "* **Local Development**:\n",
    "    * **Docker + NVIDIA Docker**: Create containerized, reproducible environments with seamless GPU access.\n",
    "    * **VSCode**: Ideal for integrating with Docker and remote development workflows.\n",
    "    * **Git**: Essential for version control and collaboration.\n",
    "* **Mac (Apple Silicon)**: Use `jax-metal` for acceleration via the Metal backend, though some features may still have compatibility issues.\n",
    "* **Cloud GPU Options**:\n",
    "    * **Major Providers**: AWS, GCP, and Azure for enterprise-scale, on-demand instances.\n",
    "    * **Specialized Providers**: Paperspace, Lambda Labs, or RunPod for better value and simpler setups for smaller projects.\n",
    "\n",
    "### Version Conflicts\n",
    "\n",
    "Scientific libraries (JAX, Flax, NumPy) evolve rapidly, often leading to version conflicts. Balancing stability with new features is critical for a functional environment.\n",
    "\n",
    "* **Version Strategy**: Aim for \"recent but stable\" versions. Use tools like **uv** (a faster `pip` alternative) to resolve difficult metadata conflicts and install incompatible packages when necessary.\n",
    "* **Environment Isolation**:\n",
    "    * **Virtual Envs**: Use `venv`, `conda`, or `uv venv` to isolate project dependencies.\n",
    "    * **Docker**: Use containers for absolute reproducibility, especially when moving from local machines to remote GPU instances.\n",
    "* **Notebook Management**: In Google Colab, pre-installed packages may conflict with your stack. Use `!pip install` to override them, but always *restart the runtime* afterward to ensure the changes are loaded.\n",
    "* **Troubleshooting**: Rely on GitHub Issues and community forums, as version mismatches are a universal challenge in the JAX ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae29eed",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b9a59",
   "metadata": {},
   "source": [
    "### Bonus: Difference between `linen.nn` and `nnx` by an example\n",
    "\n",
    "In the JAX ecosystem, **Linen** (the traditional Flax API) and **NNX** (the newer, experimental Flax API) represent two fundamentally different philosophies for building neural networks.\n",
    "\n",
    "The primary difference lies in how they handle *State* (the weights, biases, and batch stats).\n",
    "\n",
    "\n",
    "#### The Core Philosophical Difference\n",
    "\n",
    "* **Linen: Purely Functional:** Linen follows JAX’s functional purity strictly. A `Linen` module is just a set of instructions; it *does not store data*.\n",
    "    * **The Model:** A stateless blueprint.\n",
    "    * **The State:** A separate, immutable dictionary (`params`).\n",
    "    * **The Workflow:** You must pass the variables into the model every time you call it: `model.apply(variables, x)`.\n",
    "\n",
    "* **NNX: Object-Oriented JAX:** NNX introduces a \"Reference-based\" approach that feels much more like PyTorch or standard Python classes, while still being compatible with JAX transformations.\n",
    "    * **The Model:** An object that actually holds its own state.\n",
    "    * **The State:** Attributes of the class (e.g., `self.kernel`).\n",
    "    * **The Workflow:** You call the model directly: `model(x)`.\n",
    "\n",
    "#### Side-by-Side Code Comparison\n",
    "\n",
    "* **Flax Linen (Functional):**\n",
    "\n",
    "    ```python\n",
    "    import flax.linen as nn\n",
    "\n",
    "    class LinearLinen(nn.Module):\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            return nn.Dense(1)(x)\n",
    "\n",
    "    model = LinearLinen()\n",
    "    variables = model.init(rng, x)  # Returns a dict\n",
    "    out = model.apply(variables, x) # Must pass variables back in\n",
    "\n",
    "    ```\n",
    "\n",
    "* **Flax NNX (Stateful/Object-Oriented)**\n",
    "\n",
    "    ```python\n",
    "    from flax import nnx\n",
    "\n",
    "    class LinearNNX(nnx.Module):\n",
    "        def __init__(self, din, dout, rngs):\n",
    "            self.dense = nnx.Linear(din, dout, rngs=rngs)\n",
    "\n",
    "        def __call__(self, x):\n",
    "            return self.dense(x)\n",
    "\n",
    "    model = LinearNNX(1, 1, rngs=nnx.Rngs(0)) # Model owns its parameters\n",
    "    out = model(x)                             # Call like a normal function\n",
    "\n",
    "    ```\n",
    "\n",
    "#### Key Differences Table\n",
    "\n",
    "| Feature | Flax Linen | Flax NNX |\n",
    "| --- | --- | --- |\n",
    "| **State Management** | **Stateless**: Params are in a separate dict. | **Stateful**: Params are attributes of the object. |\n",
    "| **Transformations** | Use `jax.jit(model.apply)`. | Use `nnx.jit(model)`. |\n",
    "| **Shape Inference** | Excellent (via `@nn.compact`). | More explicit (usually defined in `__init__`). |\n",
    "| **Initialization** | Requires a \"dummy\" forward pass. | Standard Python initialization. |\n",
    "| **Complexity** | Higher \"boilerplate\" for state management. | Lower boilerplate; feels like PyTorch. |\n",
    "\n",
    "\n",
    "#### Why NNX is developed?\n",
    "\n",
    "Linen is mathematically beautiful but can be mentally taxing for complex architectures (like GANs or models with many moving parts) because you are constantly piping dictionaries through functions. NNX was designed to:\n",
    "\n",
    "* Make it easier for developers coming from PyTorch/Keras.\n",
    "* Allow objects to track their own state (like optimizer state or RNG counters) internally.\n",
    "* It uses a clever \"functionalization\" process under the hood so that it still plays perfectly with `jax.grad` and `jax.jit`.\n",
    "\n",
    "#### Which one should we use?\n",
    "\n",
    "* **Use Linen:** If you prefer the absolute clarity of \"data in, data out\" functional programming.\n",
    "* **Use NNX:** If you find the dictionary-passing in Linen tedious and want a more modern, object-oriented feel that is likely the future of the Flax ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0491e17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
